<!DOCTYPE html>
<!--
    Type on Strap jekyll theme v2.3.1
    Copyright 2016-2020 Sylhare
    Theme free for personal and commercial use under the MIT license
    https://github.com/sylhare/Type-on-Strap/blob/master/LICENSE
-->
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer src="/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="More Than One Turn" href="https://derekchen14.github.io/feed.xml"/>
    
    

    <!-- KaTeX 0.12.0 -->
    
    <script defer src="/assets/js/vendor/katex.min.js"></script>
    <script defer src="/assets/js/vendor/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    

    <!-- Mermaid 8.8.2 -->
    
    <!-- <script src=”https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" onload="mermaid.initialize({startOnLoad:true});"></script> -->
    <script defer src="/assets/js/vendor/mermaid.min.js" onload="mermaid.initialize({startOnLoad:true});"></script>
    

    <!-- Simple-Jekyll-Search 1.17.12 -->
    <script src="/assets/js/vendor/simple-jekyll-search.min.js" type="text/javascript"></script>

    <!-- Google Analytics / Cookie Consent -->
    <script>
      const cookieName = 'cookie-notice-dismissed-https://derekchen14.github.io';
      const isCookieConsent = 'false';
      const analyticsName = 'G-KGN8PKZ5K7';
    </script>

    
    
        <!-- Global site tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-KGN8PKZ5K7"></script>
        <!-- Page analysis (analytics.js) -->
        <script async src='https://www.google-analytics.com/analytics.js'></script>
    

    <!-- seo tags -->
    <meta property="og:image" content="https://derekchen14.github.io/assets/img/feature-img/triangular.jpeg">
    
    <meta property="og:type" content="website" />
    
    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>EMNLP 2020 Highlights | More Than One Turn</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="EMNLP 2020 Highlights" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review. As this (crazy) year comes to a close, we take a moment to reflect on what has happened in the world of NLP and specifically in relation to Dialogue Modeling and Natural Language Understanding. From my (limited) perspective, there are four trends which I noticed, including increased research into (1) data methods rather than models and (2) data efficiency techniques. In the follow-up, we discuss observations around (3) limitations in dialogue state tracking and (4) convergence around similar ideas. Let’s dive into each one in more detail:" />
<meta property="og:description" content="This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review. As this (crazy) year comes to a close, we take a moment to reflect on what has happened in the world of NLP and specifically in relation to Dialogue Modeling and Natural Language Understanding. From my (limited) perspective, there are four trends which I noticed, including increased research into (1) data methods rather than models and (2) data efficiency techniques. In the follow-up, we discuss observations around (3) limitations in dialogue state tracking and (4) convergence around similar ideas. Let’s dive into each one in more detail:" />
<link rel="canonical" href="https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html" />
<meta property="og:url" content="https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html" />
<meta property="og:site_name" content="More Than One Turn" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-28T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="EMNLP 2020 Highlights" />
<script type="application/ld+json">
{"headline":"EMNLP 2020 Highlights","dateModified":"2020-12-28T00:00:00-05:00","datePublished":"2020-12-28T00:00:00-05:00","@type":"BlogPosting","url":"https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html"},"description":"This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review. As this (crazy) year comes to a close, we take a moment to reflect on what has happened in the world of NLP and specifically in relation to Dialogue Modeling and Natural Language Understanding. From my (limited) perspective, there are four trends which I noticed, including increased research into (1) data methods rather than models and (2) data efficiency techniques. In the follow-up, we discuss observations around (3) limitations in dialogue state tracking and (4) convergence around similar ideas. Let’s dive into each one in more detail:","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <!-- RSS -->
    <link type="application/atom+xml" rel="alternate" href="https://derekchen14.github.io/feed.xml" title="More Than One Turn" />

    <!-- Twitter Cards -->
    <meta name="twitter:title" content="EMNLP 2020 Highlights">
    <meta name="twitter:description" content="This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review.  As this (crazy) year comes to a close, we take a moment...">
    
    <meta name="twitter:creator" content="@derekchen14">
    <meta name="twitter:site" content="@derekchen14">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="https://derekchen14.github.io/assets/img/feature-img/triangular.jpeg">
    <meta name="twitter:image:alt" content="EMNLP 2020 Highlights">
</head>

  <body>
    <header class="site-header">

    <!-- Logo and title -->
	<div class="branding">
        
		<a href="/">
			<img alt="logo img" class="avatar" src="/assets/img/triangle.png" />
		</a>
        
        <a class="site-title" aria-label="More Than One Turn" href="/">
        More Than One Turn
		</a>
	</div>

    <!-- Toggle menu -->
    <nav class="clear">
    <a aria-label="pull" id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>

    <!-- Menu -->
    <ul class="hide">
        

        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="About Me" title="About Me" href="/about/">
                     About Me 
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Contact" title="Contact" href="/contact/">
                     Contact 
                </a>
            </li>
            
            
        
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Publications" title="Publications" href="/publication/">
                     Publications 
                </a>
            </li>
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Search" title="Search" href="/search/">
                     <i class="fa fa-search" aria-hidden="true"></i>
                    
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Tags" title="Tags" href="/tags/">
                     <i class="fa fa-tags" aria-hidden="true"></i>
                    
                </a>
            </li>
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
    </ul>

	</nav>
</header>

    <div class="content">
      <article >
  <header id="main" style="">
    <div class="title-padder">
      
      <h1 id="EMNLP+2020+Highlights" class="title">EMNLP 2020 Highlights</h1>
      


<div class="post-info">
    <p class="meta">
      December 28, 2020
    </p></div>

      
    </div>
  </header>

  <section class="post-content">
  
      <p>This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review.  As this (crazy) year comes to a close, we take a moment to reflect on what has happened in the world of NLP and specifically in relation to Dialogue Modeling and Natural Language Understanding.  From my (limited) perspective, there are four trends which I noticed, including increased research into (1) data methods rather than models and (2) data efficiency techniques.  In the follow-up, we discuss observations around (3) limitations in dialogue state tracking and (4) convergence around similar ideas.</p>

<p>Let’s dive into each one in more detail:</p>

<!--more-->

<h2 id="1-data-methods-over-models">(1) Data Methods over Models</h2>

<p>From the review of ACL earlier this year, we have already seen that the world belongs to Transformers and we’re just living in it.  For most tasks, some version of BERT (including RoBERTa), BART, GPT or T5 have become the de facto baselines, taking over BiLSTMs w/ Attention and RNNs before that.  As evidenced on their performance on <a href="https://super.gluebenchmark.com/">SuperGLUE</a>, these models work suprisingly well across a wide range of tasks, and many traditional datasets are arguably “solved” in a loose sense of the word.  This has led to at least two major trends which highlight the importance of data and de-emphasize the efforts of designing a new model.</p>

<p>When current models outperform their benchmarks, the typical order of the ecosystem naturally tilts the balance in the other direction.  Researchers start producing datasets that are increasingly harder until we are able to once again fool the models, reaching a new equilibrium.  However this time around, for certain areas of study, we have started to hit on a fundamental limitation where naively collecting data from crowdsource workers using traditional methods yields a task that is immediately solvable by the strong baselines.  In other words, we can’t make the tasks any harder because then the task becomes too hard for humans to reliably annotate correctly.</p>

<p>So does this mean that ML models have achieved human level performance in understanding language?  Certainly not, since all models (neural or otherwise), still lack even rudimentary common sense or other basic reasoning skills that we expect from toddlers.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> The issue lies not with humans or models, but rather in how we design the task and the associated data collection techniques.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup> Consequently, we have started to see more folks thinking carefully about the issues we introduce during the data collection process.</p>

<p>As a number of papers have noted, one of the issues plaguing modern models is their ability to perform so well out-of-the-box by exploiting human annotator biases.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup>  For example, when writing mulitple choice answers, humans will typically choose a positive example as the correct answer.  As such, models are sometimes able to choose the correct answer, without even having read the corresponding question.  Accordingly, new techniques, such as adversarial filtering, have been developed to combat this.  In EMNLP, the authors take this a step further by offering <a href="https://arxiv.org/abs/2009.10795">Dataset Cartography</a> (Swayamdipta et al.) as a model-based tool for characterizing and diagnosing these types of issues.</p>

<p>In my experience, spending one or two days cleaning the data often has much better return than the same one or two days spent designing a new model architecture.  While this does not necessarily lead to novel advancements in the academic sense, it does yield notable gains in predictive performance that are quite meaningful in the practical sense.  Overall, it’s an exciting time to be building tools for automating data pre-processing and analyzing data quality.</p>

<h2 id="2-data-efficiency">(2) Data Efficiency</h2>

<p>The second data-related trend comes from recognizing that while large transformer-based models perform
shocking well in zero-shot settings, real-world scenarios have no qualms around using further data to continue to boost performance.  As a result, data collection for the end task remains relevant, and perhaps even more so as we realize the degree of impact that simply adding more data can have.  To this extent, there has been a noticeable growth in the amount of papers which care about data efficiency, which I will define as the desire to make the most efficient use of the limited amount of available, annotated data.</p>

<p>In regards to data efficiency, large-scale LMs arguably fall under the category of self-supervision.  For dialogue in particular, authors Wu, Hoi, and Xiong suggest methods for <a href="https://arxiv.org/abs/2010.13920">Improving Limited Labeled DST with Self-Supervision</a> by preserving latent consistency and modeling conversational behavior.  Whereas self-supervised models are often useful for fine-tuning on downstream tasks, meta-learned models perform this preparation explicitly.  <a href="https://arxiv.org/abs/2010.02500">Efficient Meta Lifelong-Learning with Limited Memory</a> (Wang, Mehta, Póczos, Carbonell) tackles meta-learning of models to prevent catastrophic forgetting and negative transfer by designing a more efficient episodic memory component (MbPA++).<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">4</a></sup></p>

<p>Interestingly, one of the ways the authors make the memory component more efficient is by being more selective about which prior examples are placed into memory, thus maximizing the chance of matching with an item during inference.  In particular, they aim to maximize the diversity of stored examples (rather than uncertainty) in a way that is reminiscent of methods in active learning.  On the topic of active learning, <a href="https://arxiv.org/abs/2010.09535">Cold-start Active Learning</a> (Yuan, Lin, Boyd-Graber) attempts to take the best of both pre-training (with BERT) and active learning to produce samples most conducive to learning on the end task of text classification.  Similar to BADGE<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">5</a></sup>, the authors propose to embed all examples into a shared vector space in order to choose the next sample to label, however they argue the embeddings should instead be selected based on their surprisal factor rather than their gradients.</p>

<p>One last method of using annotated data efficiently is to increase its impact through data augmentation methods.  Whereas there are some fairly simple techniques that can be applied in computer vision, naive data augmentation can often fail to perform well since swapping out even a single word can dramatically change the semantics of an utterance. Back translation is better since it is less likely to produce incoherent sentences, but does produce generic responses which are not as useful. Instead, <a href="https://www.aclweb.org/anthology/2020.emnlp-main.726/">Data Boost</a> (Liu et al.) trains a generator (based off GPT-2) to come up with new examples. Condition the generation of each token on the class, but train the model through Reinforcement Learning, where the RL reward is the salience (i.e. relevance) of each token to the class plus a KL-penalty to discourage movement outside the trust-region.</p>

<p>The common insight from all data augmentation papers is rather than augment directly, instead generate new fake examples and include a filtering step to ensure quality.  To see how this theme recurs in dialogue, move onto the second half of the observations found in <a href="https://morethanoneturn.com/2020/12/30/year-end-review-2020.html">Part Two</a>.</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1612.06890">CLEVR dataset</a> (Johnson et al.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://www.aclweb.org/anthology/D19-1107/">Are we modeling the task or the annotator?</a> (Geva, Goldberg, Berant) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/2002.04108">Adversarial Filters of Dataset Biases</a> (Le Bras et al.) <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1906.01076">Episodic Memory in Lifelong Language Learning</a> (d’Autume, Ruder, Kong, Yogatama) <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1906.03671">Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds</a> (Ash et al.) <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    
  </section>

  <!-- Social media shares -->
  <!-- 

<div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fderekchen14.github.io%2F2020%2F12%2F28%2Femnlp-2020-highlights.html" target="_blank" title=" Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?text=EMNLP+2020+Highlights%20https%3A%2F%2Fderekchen14.github.io%2F2020%2F12%2F28%2Femnlp-2020-highlights.html" target="_blank" title="">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="http://www.reddit.com/submit?url=https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html&title=EMNLP+2020+Highlights%20%7C%20More+Than+One+Turn" target="_blank" title=" Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=EMNLP+2020+Highlights%20%7C%20More+Than+One+Turn&body=:%20https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html" target="_blank" title="">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>


 -->

   <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/tags#conference">
      <p><i class="fa fa-tag fa-fw"></i> conference</p>
    </a>
    
    <a class="button" href="/tags#trends">
      <p><i class="fa fa-tag fa-fw"></i> trends</p>
    </a>
    
  </div>
</footer>


</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
    
    <div id="previous-post">
        <a alt="2020 Year End Review (Part 2)" href="/2020/12/30/year-end-review-2020.html">
            <p>Previous post</p>
            2020 Year End Review (Part 2)
        </a>
    </div>
    

    
    <div id="next-post">
        <a alt="Measuring Uncertainty" href="/2020/07/29/measuring-uncertainty.html">
            <p>Next post</p>
            Measuring Uncertainty
        </a>
    </div>
    
</div>



<!-- To change color of links in the page -->
<style>
  

  header#main {
    background-repeat:no-repeat;
  
  }
</style>

    </div>
    <footer class="site-footer">
    <p class="text">
        Aut viam inveniam aut faciam.</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                

<li>
    <a href="mailto:derekchen14@gmail.com" title="Email">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>













<li>
    <a href="https://github.com/derekchen14" title="Follow on GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#333333"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>







<li>
    <a href="https://www.linkedin.com/in/derekchen14/" title="Follow on LinkedIn">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#2867B2"></i>
            <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>









<li>
    <a feed.xml href="/feed.xml"
       title="Follow RSS feed">
        <span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:orange"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>














<li>
    <a class="type" href="https://twitter.com/derekchen14"
       title="Follow on Twitter">
        <span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#1DA1F2"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>








                </ul>
            </div>
</footer>



  </body>
</html>
