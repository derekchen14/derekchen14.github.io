<!DOCTYPE html>
<!--
    Type on Strap jekyll theme v2.3.1
    Copyright 2016-2020 Sylhare
    Theme free for personal and commercial use under the MIT license
    https://github.com/sylhare/Type-on-Strap/blob/master/LICENSE
-->
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer src="/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="More Than One Turn" href="https://derekchen14.github.io/feed.xml"/>
    
    

    <!-- KaTeX 0.12.0 -->
    
    <script defer src="/assets/js/vendor/katex.min.js"></script>
    <script defer src="/assets/js/vendor/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    

    <!-- Mermaid 8.8.2 -->
    
    <!-- <script src=”https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" onload="mermaid.initialize({startOnLoad:true});"></script> -->
    <script defer src="/assets/js/vendor/mermaid.min.js" onload="mermaid.initialize({startOnLoad:true});"></script>
    

    <!-- Simple-Jekyll-Search 1.17.12 -->
    <script src="/assets/js/vendor/simple-jekyll-search.min.js" type="text/javascript"></script>

    <!-- Google Analytics / Cookie Consent -->
    <script>
      const cookieName = 'cookie-notice-dismissed-https://derekchen14.github.io';
      const isCookieConsent = 'false';
      const analyticsName = 'G-KGN8PKZ5K7';
    </script>

    
    
        <!-- Global site tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-KGN8PKZ5K7"></script>
        <!-- Page analysis (analytics.js) -->
        <script async src='https://www.google-analytics.com/analytics.js'></script>
    

    <!-- seo tags -->
    <meta property="og:image" content="https://derekchen14.github.io/assets/img/feature-img/triangular.jpeg">
    
    <meta property="og:type" content="website" />
    
    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>2020 Year End Review (Part 2) | More Than One Turn</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="2020 Year End Review (Part 2)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Continuing on the thoughts described in Part 1, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data strategy is playing an increasingly important role in the development of modern machine learning models. How that plays out in more detail will be discussed in the future, but today we will look further into the development of dialogue in the past few months as well as other observations." />
<meta property="og:description" content="Continuing on the thoughts described in Part 1, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data strategy is playing an increasingly important role in the development of modern machine learning models. How that plays out in more detail will be discussed in the future, but today we will look further into the development of dialogue in the past few months as well as other observations." />
<link rel="canonical" href="https://derekchen14.github.io/2020/12/30/year-end-review-2020.html" />
<meta property="og:url" content="https://derekchen14.github.io/2020/12/30/year-end-review-2020.html" />
<meta property="og:site_name" content="More Than One Turn" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-30T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="2020 Year End Review (Part 2)" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"https://derekchen14.github.io/2020/12/30/year-end-review-2020.html","headline":"2020 Year End Review (Part 2)","dateModified":"2020-12-30T00:00:00-05:00","datePublished":"2020-12-30T00:00:00-05:00","description":"Continuing on the thoughts described in Part 1, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data strategy is playing an increasingly important role in the development of modern machine learning models. How that plays out in more detail will be discussed in the future, but today we will look further into the development of dialogue in the past few months as well as other observations.","mainEntityOfPage":{"@type":"WebPage","@id":"https://derekchen14.github.io/2020/12/30/year-end-review-2020.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <!-- RSS -->
    <link type="application/atom+xml" rel="alternate" href="https://derekchen14.github.io/feed.xml" title="More Than One Turn" />

    <!-- Twitter Cards -->
    <meta name="twitter:title" content="2020 Year End Review (Part 2)">
    <meta name="twitter:description" content="Continuing on the thoughts described in Part 1, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data stra...">
    
    <meta name="twitter:creator" content="@derekchen14">
    <meta name="twitter:site" content="@derekchen14">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="https://derekchen14.github.io/assets/img/feature-img/triangular.jpeg">
    <meta name="twitter:image:alt" content="2020 Year End Review (Part 2)">
</head>

  <body>
    <header class="site-header">

    <!-- Logo and title -->
	<div class="branding">
        
		<a href="/">
			<img alt="logo img" class="avatar" src="/assets/img/triangle.png" />
		</a>
        
        <a class="site-title" aria-label="More Than One Turn" href="/">
        More Than One Turn
		</a>
	</div>

    <!-- Toggle menu -->
    <nav class="clear">
    <a aria-label="pull" id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>

    <!-- Menu -->
    <ul class="hide">
        

        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="About Me" title="About Me" href="/about/">
                     About Me 
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Contact" title="Contact" href="/contact/">
                     Contact 
                </a>
            </li>
            
            
        
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Publications" title="Publications" href="/publication/">
                     Publications 
                </a>
            </li>
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Search" title="Search" href="/search/">
                     <i class="fa fa-search" aria-hidden="true"></i>
                    
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Tags" title="Tags" href="/tags/">
                     <i class="fa fa-tags" aria-hidden="true"></i>
                    
                </a>
            </li>
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
    </ul>

	</nav>
</header>

    <div class="content">
      <article >
  <header id="main" style="">
    <div class="title-padder">
      
      <h1 id="2020+Year+End+Review+%28Part+2%29" class="title">2020 Year End Review (Part 2)</h1>
      


<div class="post-info">
    <p class="meta">
      December 30, 2020
    </p></div>

      
    </div>
  </header>

  <section class="post-content">
  
      <p>Continuing on the thoughts described in <a href="https://morethanoneturn.com/2020/12/28/emnlp-2020-highlights.html">Part 1</a>, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data strategy is playing an increasingly important role in the development of modern machine learning models.  How that plays out in more detail will be discussed in the future, but today we will look further into the development of dialogue in the past few months as well as other observations.</p>

<!--more-->

<p>As a quick disclaimer, the sections titles are meant to be provocative, so any percieved slights are misguided since all papers mentioned represent significant progress in their own right.  Accordingly, no comments should be construed as minimizing any of the authors’ great work in any manner.</p>

<h2 id="3-stagnation-of-dialogue">(3) Stagnation of Dialogue</h2>

<p>Like all NLP conferences in the past few years, EMNLP 2020 contained its fair share of papers around Dialogue State Tracking. <a href="https://arxiv.org/abs/2004.03386">Schema Fusion Networks</a> (Zhu, Li, Chen and Yu) aimed to improve DST by encoding past predictions into a schema graph and carrying over past knowledge into further predictions.  Some other ideas include <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.75/">Actor-Double-Critic</a> (Wu, Tseng, and Gasic), <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.95/">GCDST</a> (Wu, Zou, Jiang, and Aw), <a href="https://arxiv.org/abs/2009.07615">Temporally Expressive Networks</a> (Chen, Zhang, Mao, Xu) and <a href="aclweb.org/anthology/2020.emnlp-main.243/">Slot Attention with Value Normalization</a> (Wang, Guo, Zhu).  What all these papers have in common is the development of complex models to boost the final accuracy score.</p>

<p>Thus, arguably the most impressive was DST paper was <a href="https://arxiv.org/abs/2005.00796">SimpleTOD</a> (Hosseini-Asl et al.) which used the straightforward, but powerful idea of taking GPT-2 as the core model to sequentially generate the output dialogue states.  Properly designing the prompts to feed into the model seems trivial, but is actually quite insightful once you realize the exponential number of combinations possible to design a model input.  Furthermore, this simple setup yielded the best results we have seen on MultiWOZ<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> so far with 55.76 joint goal accuracy.</p>

<p>While these methods are all impressive, it is starting to seem like the current MultiWOZ benchmark is starting to hit its limits since qualitative analysis will reveal that most model errors are actually due to errors in annotation.  As model performance starts to saturate, the natural progression is to move onto further benchmarks.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup><sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup><sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">4</a></sup>  Thus, the short-comings of the original dataset have spurred on the creation of MultiWOZ 2.1<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">5</a></sup>, MultiWOZ 2.2<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">6</a></sup> and MultiWOZ 2.3<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote">7</a></sup>.</p>

<p>These revisions fix a number of labeling issues, but there is also an underlying issue where it seems the community might be overfitting to a single dataset, which means only making incremental gains. To this extent, dialogue research has perhaps stagnated since we are so highly focused on the single task of multi-domain slot-filling.  Thus, it is perhaps time to have a new standard benchmark which retains the core components of semantic understanding, but also offers other aspects of conversation to study.  As some possible examples, we could explore task-oriented dialogues where the user is unsure of the options available to them, so the job of the agent is to elicit their preferences before recording them.  Another avenue is to look at dialogues where the agent is constrained in their actions by company policies, which reflect real-world scenarios.</p>

<p>To be clear, other elements of dialogue, such as natural language generation are still seeing exciting movement, and this section will be wrapped up by highlighting three papers in this realm.  Building on the theme from the previous section, <a href="https://arxiv.org/abs/2004.07462">Paraphrase Augmented Task-Oriented Dialog Generation</a> (Gao, Zhang, Ou, Yu) uses Data Augmentation techniques rather than model changes to improve the performance of Dialgoue State Tracking.  In particular, the authors devise a method for augmenting data through paraphrasing with templates.  They do so by taking advantage of the structure of the dialogue labels in the training set as opposed to just paraphrasing the raw text.</p>

<p>Within <a href="https://www.aclweb.org/anthology/2020.emnlp-main.230/">Make Neural Natural Language Generation as Reliable as Templates</a> Elder, O’Connor, and Foster also use generation as a method for data augmentation.  Their idea is to find the balance between template systems (which may be too rigid) and neural systems (which are often uncontrollable) that allows for diversity while also maintaining reliability.  The key idea is to use regex and heuristics to identify desired surface forms that we would want a model to generate.  Then, we restrict the softmax output and generation capabilities (ie. beam search) to only be able to produce these surface forms and other minor function words.</p>

<p>If we were to push the augmentation idea to the extreme, we would be training entire dialogue systems using generated data, which is precisely what a user simulator would provide.  In fact, <a href="https://arxiv.org/abs/2011.08243">Dialog Simulation with Realistic Variations</a> from the Alexa Conversations at Amazon have done exactly that.  Although still not quite a simulator with realistic actions, the authors do add some interesting variations.  Starting with a M2M paraphrasing technique<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote">8</a></sup>, the group puts in extra effort to generate novel template flows.  Specifically, they allow for users to adjust their preferences during the chat and have some element of pushing the conversation along by pro-actively requesting the slot-values of API calls that have not yet been made.</p>

<h2 id="4-convergence-of-ideas">(4) Convergence of Ideas</h2>

<p>The more optimistic view of this point is that “great people think alike”.  The more pessimistic view of this point is that the lack of interesting datasets means there are only a few fertile areas left to look.  Perhaps a more realistic view is that similar ideas commonly arise from independent sources when many people are exploring the same frontier at once, and this is simply the first time I’ve noticed.  The bottom line is that I started to notice a number of duplicate papers stating basically the same core idea, both published at the same time at EMNLP.</p>

<p>The first example of this is maybe the most obvious given the similarity of their titles. <a href="https://arxiv.org/abs/2010.12770">Conversational Semantic Parsing for DST</a> from Apple and <a href="https://www.aclweb.org/anthology/2020.emnlp-main.408/">Conversational Semantic Parsing</a> from Facebook both examine their specific style of parsing dialogues.  I imagine that Amazon, Tencent and Google all have something similar as well.  For reference, Microsoft’s version of this is to view <a href="https://arxiv.org/abs/2009.11423">Task-Oriented Dialogue as Dataflow Synthesis</a> (Semantic Machines et al.).</p>

<p>More specifically, Apple’s version presents dialogues as Trees with segments that are chained together with periods.  They adopt a stack to represent multiple tasks in dialog history with pointers from certain branches to others.  Facebook’s version is quite similar in that it also contains a hierarchical structure where certain slot values are expanded into further detail.  However, as someone who has designed a dialogue meaning representation for production purposes as well, I simply cannot fathom how these representations can be quickly annotated by crowdsource workers.  The structure of these labels make many of them hard to even verify, much less annotate, with any reliable level of accuracy.  There are many very smart people who work there though, so it’s possible they are privy to some insight that I am not.</p>

<p>The second set of similar papers share the high level goal of generating novel training data for semantic parsing in an semi-automated fashion.  The shared core idea is to generate the training data by conditioning query-based templates on the known database schema.  Then, the templates are filled and transformed into natural langauge through the use of deep learning models.  To avoid using augmented data that changes the semantics, they also apply the same pipeline for verification.  Namely, they start by parsing the natural language output back into the logical form.  Then, this SQL query is executed against the engine to return a result.  Finally, results failing to match the original, dynamically-selected KB attribute are filtered out – leaving only high quality generations for training.</p>

<p><a href="https://arxiv.org/abs/2010.04806">AutoQA</a> (Xu, Semnani, Campagna, Lam) uses BART for paraphrasing the templates into natural language, whereas <a href="https://arxiv.org/abs/2009.07396">GAZP</a> (Zhong, Lewis, Wang, and Zettlemoyer) uses BERT with BiLSTM components.  If we allow for changing the task to NER, then <a href="https://www.aclweb.org/anthology/2020.emnlp-main.590/">Counterfactual Generator</a> (Zeng, Li, Zhai, Zhang) can also be added to this set, since it also follows the same format of generating imperfect examples and then using the originally trained model to filter for the best data augmentations.  In all three cases, the training set that includes the augmentations achieves the best final performance.</p>

<p>The final set of similar papers touches on the topic of improving model performance through the use of soft perturbations to generate new data samples.  More specifically, <a href="https://arxiv.org/abs/2009.10195">Self-Supervised Manifold Based Data Augmentation</a> (Ng, Cho, and Ghassemi) perform data augmentation by masking out various tokens and then having BERT fill those tokens in, keeping the original label.  This may be error prone since you could drop key words that change the label, but for the tasks they chose, this was not a substantial concern.  In particular, they experiment with Sentiment Analysis, Natural Language Inference and Machine Translation.  During their experiments, they found that, using a BERT model to predict hard (aka. one-hot) labels led to roughly equal results.  Intuitively, this should be expected since it is simply feeding in more examples of things the model already knows.  The insight then was feeding in soft labels and training with KL-divergence, which then led to improved Out-of-domain performance.</p>

<p>To see why this helps, we turn to <a href="https://www.aclweb.org/anthology/2020.emnlp-main.671/">Towards More Accurate Uncertainty Estimation</a> (He et al.) which employs a very similar insight.  In particular, the authors create perturbed inputs and labels using MixUp and also change their loss function to KL-divergence.  This creates labels are now also “soft” in that the new label is a distribution between the two mixed classes rather than discrete.  Similar to the original MixUp paper then<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote">9</a></sup>, the idea is that using the soft labels prevents the model from being too overly confident since during training it has seen outputs where the target was not necessarily completely towards a single label, but rather shared across multiple labels.  Overall, the idea that soft targets may be more robust than hard targets is something worth further exploration!</p>

<hr />

<p>In total, there are arguably fewer mind-blowing model changes or huge performance gains within NLP as there might have been in years past.  In it’s place, we observe important and more nuanced advancements that highlight the maturity of the field.  I would argue that the shift towards more pragmatic data management concerns is actually good since it implies a move towards bringing these great NLP advancements into the real world.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1810.00278">MultiWOZ</a> (Budzianowski, et al.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1804.07461">GLUE</a> (Wang et al.) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1606.05250">SQuAD</a> (Rajpurkar et al.) <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1502.05698">bAbI</a> (Weston at al.) <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1907.01669">MultiWOZ 2.1</a> (Eric et al.) <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/2007.12720">MultiWOZ 2.2</a> (Zang et al.) <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/2010.05594">MultiWOZ 2.3</a> (Han and Liu et al.) <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1801.04871">Building a Conversational Agent Overnight</a> (Shah et al.) <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/1710.09412">mixup: Beyond Empircal Risk Minimzation</a> (Zhang et al.) <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    
  </section>

  <!-- Social media shares -->
  <!-- 

<div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fderekchen14.github.io%2F2020%2F12%2F30%2Fyear-end-review-2020.html" target="_blank" title=" Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?text=2020+Year+End+Review+%28Part+2%29%20https%3A%2F%2Fderekchen14.github.io%2F2020%2F12%2F30%2Fyear-end-review-2020.html" target="_blank" title="">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="http://www.reddit.com/submit?url=https://derekchen14.github.io/2020/12/30/year-end-review-2020.html&title=2020+Year+End+Review+%28Part+2%29%20%7C%20More+Than+One+Turn" target="_blank" title=" Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=2020+Year+End+Review+%28Part+2%29%20%7C%20More+Than+One+Turn&body=:%20https://derekchen14.github.io/2020/12/30/year-end-review-2020.html" target="_blank" title="">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>


 -->

   <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/tags#conference">
      <p><i class="fa fa-tag fa-fw"></i> conference</p>
    </a>
    
    <a class="button" href="/tags#trends">
      <p><i class="fa fa-tag fa-fw"></i> trends</p>
    </a>
    
  </div>
</footer>


</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
    
    <div id="previous-post">
        <a alt="Embracing the AI-First Paradigm" href="/2021/05/23/embracing-ai-first.html">
            <p>Previous post</p>
            Embracing the AI-First Paradigm
        </a>
    </div>
    

    
    <div id="next-post">
        <a alt="EMNLP 2020 Highlights" href="/2020/12/28/emnlp-2020-highlights.html">
            <p>Next post</p>
            EMNLP 2020 Highlights
        </a>
    </div>
    
</div>



<!-- To change color of links in the page -->
<style>
  

  header#main {
    background-repeat:no-repeat;
  
  }
</style>

    </div>
    <footer class="site-footer">
    <p class="text">
        Aut viam inveniam aut faciam.</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                

<li>
    <a href="mailto:derekchen14@gmail.com" title="Email">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>













<li>
    <a href="https://github.com/derekchen14" title="Follow on GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#333333"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>







<li>
    <a href="https://www.linkedin.com/in/derekchen14/" title="Follow on LinkedIn">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#2867B2"></i>
            <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>









<li>
    <a feed.xml href="/feed.xml"
       title="Follow RSS feed">
        <span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:orange"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>














<li>
    <a class="type" href="https://twitter.com/derekchen14"
       title="Follow on Twitter">
        <span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#1DA1F2"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>








                </ul>
            </div>
</footer>



  </body>
</html>
