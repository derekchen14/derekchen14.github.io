<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://derekchen14.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://derekchen14.github.io/" rel="alternate" type="text/html" /><updated>2022-05-24T18:45:15-04:00</updated><id>https://derekchen14.github.io/feed.xml</id><title type="html">More Than One Turn</title><subtitle>Blog about dialogue modeling and data efficiency</subtitle><entry><title type="html">Building User Simulators for Scalable Dialogue Systems</title><link href="https://derekchen14.github.io/2022/05/24/building-user-simulators.html" rel="alternate" type="text/html" title="Building User Simulators for Scalable Dialogue Systems" /><published>2022-05-24T00:00:00-04:00</published><updated>2022-05-24T00:00:00-04:00</updated><id>https://derekchen14.github.io/2022/05/24/building-user-simulators</id><content type="html" xml:base="https://derekchen14.github.io/2022/05/24/building-user-simulators.html">&lt;p&gt;Training dialogue agents for real-life use cases is immensely difficult since manual data annotation quickly hits scaling issues.  One way around this is to build a user simulator which can theoretically then generate tons of examples for the agent to learn from.  However, to build a system representing the user, you would need a model that understands how to react and respond to agents.  But to train such a user model you would then need some dialogue system that acts as an agent.  So we have a chicken-and-egg problem, right?  Well, not quite.  There is at least one key distinction between a user simulator and a dialogue agent.
 &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Note that while a dialogue agent takes in a natural language, follows some policy (often to make API calls), and then generates a natural language response, the simulator does not have to complete all those steps.  In particular, the user simulator can avoid the NLU component and go straight into policy management.  Furthermore, the number of user actions is inherently limited by the number of APIs that the product or service can provide.  To be clear, a real life user engages in a countless number of actions, but only a small subset of those actions are expected to be handled by the agent anyway — in all other cases, the agent can simply abstain.&lt;/p&gt;

&lt;p&gt;Armed with these two facts, how can we go about designing a practical user simulator? Our first insight tells us that unlike a dialogue agent, what we want to build is a system that can handle reasonably complex actions followed by extremely sophisticated NLG component for producing responses.  Framed in this way, we see that such a system already exists: open-domain chat bots.  In fact, a large part of building task-oriented chatbots is that users often break out of script and engage in chit-chat.  If our user simulator already produced such outputs, in addition to directed information, then our dialogue agent would naturally be more robust to such variations.&lt;/p&gt;

&lt;p&gt;Our second insight is that our user simulator does not need a terribly complex policy manager.  Instead, we can enumerate them with a set of rules to produce all combinations of user intents.  We can then sample from this set to produce plausible scenarios.  To see how this works, let’s look at an example.  Suppose we are dealing with the a flight domain.  A user may want to book a flight, cancel a flight, check on an existing flight or ask about some FAQs.  In reality, there are maybe a dozen more actions that user would want to take, but not that much more.&lt;/p&gt;

&lt;p&gt;What makes emulating and/or understanding the user difficult is the number of values they provide and the way in which they provide them.  For example, when booking a flight, the user may choose from thousands of (departure/destination) locations and times.  This can lead to millions of values, but there are only four possible slot combinations.  Taking into account edge cases such as price requirements, seating requests and airline preferences, we might expand this into a total of 100 slots for a given domain.  Ultimately though, the number of slots is relatively small compared to the number of values.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;  We separately mentioned that the way the user’s express a preference have lots of variation: there are a hundred ways to say that you want to “Fly from San Diego to Miami on Saturday morning”.  What all these permutations have in common though is that all the complexity is in how to express the exact request.&lt;/p&gt;

&lt;p&gt;In other words, with a limited number of domains, slots and intents it is not particularly hard to design a policy which can scale to as many combinations as you want since the heavy lifting is actually within the text generation component.  And what existing models are great at generating text given a context?  Open-domain chatbots.  Such models can already directly query the internet and read through millions of Wikipedia pages.  Trying to parse a few dozen parameters before generating a natural language response should be a walk in the park.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Further notice that the slots in a given domain are often re-used across intents which dramatically simplifies the task. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="modeling" /><category term="rl" /><category term="dialogue" /><summary type="html">Training dialogue agents for real-life use cases is immensely difficult since manual data annotation quickly hits scaling issues. One way around this is to build a user simulator which can theoretically then generate tons of examples for the agent to learn from. However, to build a system representing the user, you would need a model that understands how to react and respond to agents. But to train such a user model you would then need some dialogue system that acts as an agent. So we have a chicken-and-egg problem, right? Well, not quite. There is at least one key distinction between a user simulator and a dialogue agent.</summary></entry><entry><title type="html">Data Augmentation for Scalable Product Development</title><link href="https://derekchen14.github.io/2022/03/11/data-augmentation-for-scalable-product.html" rel="alternate" type="text/html" title="Data Augmentation for Scalable Product Development" /><published>2022-03-11T00:00:00-05:00</published><updated>2022-03-11T00:00:00-05:00</updated><id>https://derekchen14.github.io/2022/03/11/data-augmentation-for-scalable-product</id><content type="html" xml:base="https://derekchen14.github.io/2022/03/11/data-augmentation-for-scalable-product.html">&lt;p&gt;Data augmentation methods are a staple when training computer vision models, with methods like flipping, resizing, cropping and blurring used so ubiquitously that they are a foregone conclusion in most systems.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; These methods help improve model robustness such that anyway you change the image of a cat, the model still recognizes the item in the picture as a cat.  This is relatively straight forward since all aforementioned techniques keep the main object the same such that a cat remains a cat, and does not somehow magically morph into a dog.  But does this work for NLP as well?&lt;/p&gt;

&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;naive-data-augmentation&quot;&gt;Naive Data Augmentation&lt;/h2&gt;

&lt;p&gt;Text is composed of discrete tokens rather than continous vectors, so what does blurring or flipping even mean in this setting?  It turns out that cropping and rotation can be approximated by first converting a sentence into a dependency parse and then pruning branches or shifting them around.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; If we define blurring as finding nearby pixels for mixing, then we can view swapping tokens within a sentence as an analogous operation.  Swapping is part of Easy Data Augmentation (EDA) which also includes randomly inserting or deleting tokens, as well as synonym replacement.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Auto-augment for images automatically learns an optimal mixture of augmentations&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, which has inspired similar forms of automatically designing augmentation policies for dialogue.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;  These all roughly fall under the umbrella of &lt;em&gt;surface form alteration&lt;/em&gt; which changes the actual tokens of text to create new examples.&lt;/p&gt;

&lt;p&gt;Another popular family of augmentation techniques is &lt;em&gt;latent perturbation&lt;/em&gt; where the raw text is first mapped into some hidden state before being transformed back to natural langauge.  Variational Auto-encoders (VAEs) fit nicely into this paradigm since you can perturb the latent state before decoding to theoretically generate an arbitrary number of augmentations.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; Posterier collapse aside, such a generative model can be hard to train since it requires learning the entire distribution of outcomes.  Oftentimes, it is much easier to learn a direct mapping from one piece of raw text to another, where the hidden state is untouched.  This is basically paraphrasing.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; If we jump to a pivot langauge in the middle of encoding and decoding, this is Round Trip Translation (RTT).&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;  There are countless other techniques in the realm (see &lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; for one survey), but a last one worth mentioning is Mix-match which interpolates the hidden states of two examples and also the hidden states of their labels to produce new examples.&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; Temperature sharpening may also be applied.&lt;/p&gt;

&lt;p&gt;While all these methods have seen some success in NLP, naively applying them to intent classification and other dialogue tasks may not be so straightforward because they can easily alter the semantic meaning of an utterance. To see why, take for example the sentence “I would prefer not to eat Indian food, it’s too spicy”.  Suppose we were to perform word replacement, and switch Indian to American: “I would prefer not to eat American food, it’s too spicy”.  Then the utterance doesn’t make a lot of sense, and the user’s intent has also been altered.  Suppose token dropout were used instead: “I would prefer to eat Indian food”.  Now the user’s preference has been completely inverted!  Unlike in the image setting, tiny perturbations can cause large semantic shifts in text.&lt;/p&gt;

&lt;h2 id=&quot;label-preserving-data-augmentation&quot;&gt;Label Preserving Data Augmentation&lt;/h2&gt;

&lt;p&gt;To preserve semantics as we generate new examples, the augmentation technique ought to have an awareness of the importance of certain entities or phrases before performing its transformation.  For example, rather than dropping random tokens, this can be accomplished more intelligently by only focusing on stopwords or other low order segments as determined by a POS tagger.  When operating in a latent space, &lt;em&gt;consistency training&lt;/em&gt; induces robustness applying general perturbations around a region of a label, while encouraging the model to still maintain the same prediction.&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; A typical loss function to enforce matching predictions can be a cross entropy loss.  Others have also tried MSE, KL-divergence, JSD or any other distance metric.&lt;sup id=&quot;fnref:17&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Taking this a step further, rather than operating at the token-level, we can expand this to encompass entire segments where different phrase fragments are shuffled or recombined to form new utterances as part of compositional augmentation.&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; Typically, the focus is on the unimportant spans in order to avoid accidentally changing the label, but if we have access to an ontology, we can actually target the label by switching known values for slot-filling with other values belonging to that same slot.&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;If the goal is to make sure the label is preserved, we can actually work on the problem in reverse as well!  Concretely, we can consider creating new examples from scratch starting from the labels themselves using &lt;em&gt;text generation&lt;/em&gt; methods.  Language model (LM) decoding pre-trains a language model to auto-regressively predict a text span composed of a label followed by a corresponding utterance. During inference, the model is fed the label alone and asked to produce a novel example.&lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; Alternatively, we can use the label set to write out templates which are then paraphrased by humans into natural language.&lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; This can be applied to the dialogue setting by having entire conversations written out in template format before paraphrasing into natural text.&lt;sup id=&quot;fnref:15&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Even though the methods now preserve labels, they still face a host of other issues. To start, there is no single method that is reliably useful.&lt;sup id=&quot;fnref:16&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;  This lack of consistency holds not only when dealing with a wide range of NLP tasks, but even when optimizing just for specific tasks such as intent classification.&lt;sup id=&quot;fnref:17:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;  Moreover, most methods have only been studied on classification, rather than on complicated tasks such as dialogue state tracking. Consequently, there is no consensus to how well these augmentation methods work in practice. But what if the goal isn’t even model robustness on in-domain examples?  What if what we really want is to build scalable dialogue systems that can survive the complexity of real-life conversations?&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation-for-diversity&quot;&gt;Data Augmentation for Diversity&lt;/h2&gt;

&lt;p&gt;Using data augmentation to achieve robustness on out-of-domain (OOD) examples requires a shift in mindset from targeting label quality towards focusing on label diversity.  This mirrors a similar shift in computer vision in optimizing for both Affinity and Diversity.&lt;sup id=&quot;fnref:21&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; Template paraphrasing may generate high quality annotations since we know the labels unfront, but the breadth of coverage is limited by the number of manually crafted templates.  LM decoding fine-tuned on your dataset may start to overfit to the distribution it has encountered. In order to produce novel examples then, we must inject some alternate prior into the data distribution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Auxilliary supervision&lt;/em&gt; techniques obtain data following a different distribution by literally using a separate dataset. Essentially, some small seed set can be used to query a large pool of unlabeled utterances to find examples that are close enough to maintain the label, but far away enough to expand the coverage.&lt;sup id=&quot;fnref:18&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;  In order to maintain the label preserving properties, an additional filtering process can be applied.  An alternative to moving towards a different distribution is to add noise to the data manifold, which has also been shown to improve out-of-domain robustness.&lt;sup id=&quot;fnref:19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Text generation based on large-scale pretrained LMs may be especially well-suited for promote diversity since it was (likely) trained on data unrelated to the downstream task, yet can still output coherent examples.  To extract the information stored in its billions of parameters, we can lean on tricks such as beam search, nucleus sampling, or top-k sampling when applied to LM decoding.&lt;sup id=&quot;fnref:10:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;  Text in-filling with a masked language model (MLM) can also be utilized in a similar manner.&lt;sup id=&quot;fnref:16:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; And although it hasn’t happened yet, using prompt-based methods to generate new examples is likely right around the corner.  These method resolve OOD issues to some extent, but are unsatisfying because they fail to cover all possible conversational scenarios. But since the space of possible topics is infinite, does that mean data augmentation is fundamentally flawed?&lt;/p&gt;

&lt;p&gt;To see how this may work, we first have to recognize that the limitation in coverage is not a flaw of data augmentation, but rather a general difficulty of building dialogue systems. So to make things practical, we will not aim to build systems that can discuss any topic.  Instead, we limit the conversations to pertinent aspects helpful for solving specific use cases (ie. buy a chair, find an email, book a flight).  Additionally, we set expectations by letting users know that the chatbot has its limitations.  However, this leads to a new issue since the current techniques expand coverage of the solution space in an arbitrary manner, and while we no longer need to expand to an infinitely large horizon, we still have to expand to new domains in a targeted and controllable manner.&lt;/p&gt;

&lt;h2 id=&quot;augmentation-as-conditional-generation&quot;&gt;Augmentation as Conditional Generation&lt;/h2&gt;

&lt;p&gt;Controllable data augmentation allows for scaling to new domains and use cases in a reliable manner, which is absolutely essential for building products on top of this technology.  To achieve this, we must first clearly define what we mean by use cases.  Specifically, use cases can be considered individual skills (ie. find a restaurant, make a reservation, update a reservation to add more peoeple, cancel a reservation) that all belong to some domain (ie. restaurant reservation management). Suppose we are able to build a dialogue system that can handle three use cases, but we would now like to expand into a fourth.  More than just haphazard data augmentation, we would like to synthesize new training examples in a scalable fashion targeted to help the model learn this fourth skill.&lt;/p&gt;

&lt;p&gt;By conditioning on the new use case we care about when performing generation, we should be able to create new examples for model training that specifically fit our criteria.  Concretely, suppose the fourth skill we want to add is the ability for the dialogue system to converse about restaurant cancellations.  Then, we outline the actions agents must take to handle cancellations.&lt;sup id=&quot;fnref:20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt;  The discrete actions can be translated into templates using automated means, which are then paraphrased into natural language.  In contrast to direct paraphrasing, the goal here would be to produce examples in totally new directions rather than re-hashing the existing data.&lt;/p&gt;

&lt;p&gt;Once we have some starting examples, we can then use data augmentation in a traditional manner to produce many more.  In conclusion, in order to make data augmenation useful for scalable product development, we moved away from using it as a means of promoting robustness, and instead as an alternate form of data collection.  The only step left is making this entire process operate efficiently, which is admittedly easier said than done.&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation-taxonomy&quot;&gt;Data Augmentation Taxonomy&lt;/h2&gt;

&lt;p&gt;To recap, we have explored a whirlwind of data augmentation options, starting with ideas inspired from computer vision and gradually moved closer to techniques specialized for scaling development of dialogue systems.  What we discovered along the way is that key application of data augmentation is not necessarily for improving model performance, but rather for dealing with long-tail out-of-domain use cases.  To wrap up, we now organize the smorgasboard of augmentation techniques into a more formal structure for future study.&lt;/p&gt;

&lt;p&gt;One way to organize all the different types of augmentation methods is to think of the end goal in mind —- namely the newly formed training example.  A key benefit is this allows us to formulate the solution space with a directed acyclic graph (DAG) which ensures that our taxonomy is collectively exhaustive.  In doing so we end up with the diagram above which outlines five key categories of data augmentation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/data-aug.png&quot; alt=&quot;Data Augmentation Taxonomy&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Surface Form Alteration (A –&amp;gt; B): EDA, Character Swapping, Synonym Replacement, Rules-based Systems&lt;/li&gt;
  &lt;li&gt;Latent Perturbation (A –&amp;gt; C –&amp;gt; B): Pivot languages, VAEs, GANs, Direct paraphrasing&lt;/li&gt;
  &lt;li&gt;Text Generation (C –&amp;gt; B): LM Decoding, Text In-filling, User Simulation&lt;/li&gt;
  &lt;li&gt;Auxiliary Supervision (D –&amp;gt; B): External data pool, kNN retrieval on unlabeled utterances, Weak supervision&lt;/li&gt;
  &lt;li&gt;Template Paraphrasing (D –&amp;gt; C –&amp;gt; B): M2M, Overnight&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An interesting observation is that while template paraphrasing seems like a subgroup, according to the DAG it is a parent level method.  Ultimately it seems that while all methods are useful, but some are more useful than others.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2020), &lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j/chen20j.pdf&quot;&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Louvan and Magnini, 2020), &lt;a href=&quot;https://aclanthology.org/2020.paclic-1.20/&quot;&gt;Lightweight Data Augmentation for Low Resource Slot Filling and Intent Classification&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wei and Zou (2019), &lt;a href=&quot;https://aclanthology.org/D19-1670/&quot;&gt;Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Cubuk et al. (2020), &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.pdf&quot;&gt;AutoAugment: Learning Augmentation Policies from Data&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Niu and Bansal (2019), &lt;a href=&quot;https://aclanthology.org/D19-1132/&quot;&gt;Automatically Learning Data Augmentation Policies for Dialogue Tasks&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Zhao et al. (2017), &lt;a href=&quot;https://aclanthology.org/P17-1061/&quot;&gt;Learning Discourse-level Diversity for Neural Dialog Models using Conditional VAEs&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Prakash et al. (2016), &lt;a href=&quot;https://aclanthology.org/C16-1275/&quot;&gt;Neural Paraphrase Generation with Stacked Residual LSTM Networks&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sennrich et al. (2016), &lt;a href=&quot;https://aclanthology.org/P16-1009/&quot;&gt;Improving Neural Machine Translation Models with Monolingual Data&lt;/a&gt; &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Feng et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.findings-acl.84/&quot;&gt;A Survey of Data Augmentation Approaches for NLP&lt;/a&gt; &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:10:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Berthelot et al. (2020), &lt;a href=&quot;http://papers.neurips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf&quot;&gt;MixMatch: A Holistic Approach to Semi-Supervised Learning&lt;/a&gt; &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Xie et al. (2020), &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf&quot;&gt;Unsupervised data augmentation for consistency training&lt;/a&gt; &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021), &lt;a href=&quot;https://openreview.net/forum?id=4DXQP8laTU2&quot;&gt;An Empirical Survey of Data Augmentation for Limited Data Learning in NLP&lt;/a&gt; &lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:17:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jacob Andreas (2020), &lt;a href=&quot;https://aclanthology.org/2020.acl-main.676/&quot;&gt;Good-Enough Compositional Data Augmentation&lt;/a&gt; &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Anaby-Tavor et al. (2020), &lt;a href=&quot;https://aaai.org/ojs/index.php/AAAI/article/view/6233&quot;&gt;Do not have enough data? Deep learning to the rescue!&lt;/a&gt; &lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wang et al. (2015), &lt;a href=&quot;https://aclanthology.org/P15-1129/&quot;&gt;Building a Semantic Parser Overnight&lt;/a&gt; &lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shah et al. (2018), &lt;a href=&quot;https://aclanthology.org/N18-3006/&quot;&gt;Bootstrapping a Neural Conversational Agent with Dialogue Self-Play&lt;/a&gt; &lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen and Yin (2021), &lt;a href=&quot;https://datacentricai.org/neurips21/papers/138_CameraReady_Data_Aug_v5.pdf&quot;&gt;Data Augmentation for Intent Classification&lt;/a&gt; &lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:16:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:21&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lopes et al. (2021), &lt;a href=&quot;https://openreview.net/forum?id=ZcKPWuhG6wy&quot;&gt;Tradeoffs in Data Augmentation: An Empirical Study&lt;/a&gt; &lt;a href=&quot;#fnref:21&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:18&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen and Yu (2021), &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.35/&quot;&gt;GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation&lt;/a&gt; &lt;a href=&quot;#fnref:18&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ng et al. (2020), &lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.97/&quot;&gt;SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving OOD Robustness&lt;/a&gt; &lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.naacl-main.239/&quot;&gt;ABCD: Action-Based Conversations Dataset&lt;/a&gt; &lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="data-strategy" /><category term="product-strategy" /><category term="startups" /><summary type="html">Data augmentation methods are a staple when training computer vision models, with methods like flipping, resizing, cropping and blurring used so ubiquitously that they are a foregone conclusion in most systems.1 These methods help improve model robustness such that anyway you change the image of a cat, the model still recognizes the item in the picture as a cat. This is relatively straight forward since all aforementioned techniques keep the main object the same such that a cat remains a cat, and does not somehow magically morph into a dog. But does this work for NLP as well? Chen et al. (2020), A Simple Framework for Contrastive Learning of Visual Representations &amp;#8617;</summary></entry><entry><title type="html">Designing Meaning Representations for Dialogue Systems</title><link href="https://derekchen14.github.io/2021/11/26/designing-meaning-representation.html" rel="alternate" type="text/html" title="Designing Meaning Representations for Dialogue Systems" /><published>2021-11-26T00:00:00-05:00</published><updated>2021-11-26T00:00:00-05:00</updated><id>https://derekchen14.github.io/2021/11/26/designing-meaning-representation</id><content type="html" xml:base="https://derekchen14.github.io/2021/11/26/designing-meaning-representation.html">&lt;p&gt;In order for a virtual assistant to be useful, the agent should do more than just information retrieval and basic chit-chat.  Rather than pattern recognition on the response level, the agent should be able to perform pattern recognition on the discourse level so it can mimic human-reasoning (even as true understanding remains an elusive goal).  If a model were to reason about an utterance, it must have been trained to do so.  Furthermore, we argue that such training must be explicitly performed through (weakly) supervised learning, rather than implicitly extracted from a large pre-trained LM (eg. through careful prompting).
&lt;!--more--&gt;
This article aims to show explicit training is indeed necessary and that such training is possible through well-designed meaning representations of dialogue.&lt;/p&gt;

&lt;h1 id=&quot;necessity-of-fine-tuning&quot;&gt;Necessity of Fine-tuning&lt;/h1&gt;

&lt;p&gt;The explosion of Foundation Models such as BART, GPT, DeBERTa, and others may make it seem like the solutions to all our NLP problems are right around the corner.  However, any serious investigation into the matter will quickly dissuade us of any such illusions.  These gigantic models are hard to train, hard to control and hard to deploy into real-world environments.  Moreover, while large language models generate realistic sounding text, even the creators of such models would likely hesitate to claim that those same models &lt;em&gt;comprehend&lt;/em&gt; the text they are generating.  Indeed, the standard practice is to pre-train a large LM and then fine-tune it on the task of your choosing.&lt;/p&gt;

&lt;p&gt;Given our core task is dialogue systems, how far can we go using only a PLM?&lt;/p&gt;

&lt;h3 id=&quot;lemma-1-dialogue-semantics-is-not-sufficiently-modeled-by-language-modeling&quot;&gt;Lemma 1: Dialogue semantics is not sufficiently modeled by Language modeling.&lt;/h3&gt;
&lt;p&gt;Language models predict next tokens to minimize perplexity.  Dialogue also contains an element of this skill since knowing what the speaker will say next is indicative of understanding, implying you are following along with the conversation.  However, we believe that dialogues are not captured by language modeling in a proof by induction:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Starting with individual tokens, note that specific values are impossible to predict because they vary for each new interaction (ie. aleatoric uncertainty).  If a customer mentions “My name is X” or “You can call me at Y” that name or phone number is different for each person.  Some names may be more likely than others (ie. John), but for the most part, names are not shared.&lt;/li&gt;
  &lt;li&gt;Sequences of tokens also follow this general pattern.  Certain phrases have higher probability than others, but conversations overall do not follow a predictable flow.  For example, there exist numerous generic phrases such as “there are plenty of fish in the sea”, “dancing in the moonlight”, or “distance makes the heart grow fonder”.  Given the first few tokens, it’s not hard to fill in the rest.  However, dialogues are not always so predictable.  Not only is it possible to tweak these phrases, but folks will occasionally do so precisely for the impact of the unexpected twist: “I would visit, but distance makes the heart grow tired since I don’t like running.”
Expanding this from phrases to sentence, from sentences to multi-turn utterances, and so on, we see that semantics are not so easy to capture by language modeling alone.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For further proof, consider the Nucleus Sampling paper (Holtzmann, 2019) which showed that true natural language has much higher perplexity than machine generated output.  We also don’t want to forget about co-reference, ellipsis and anaphora.  Finally, consider that most common sense reasoning is left unstated.  Overall, if so much of communication is never explicitly stated, then merely being good at predicting what will be said is not enough to understand what is being said.&lt;/p&gt;

&lt;h3 id=&quot;lemma-2-conversational-outcomes-are-subject-to-interpretation&quot;&gt;Lemma 2: Conversational outcomes are subject to interpretation&lt;/h3&gt;
&lt;p&gt;Even when something is explicitly stated, the meaning of such utterances may be ambiguous.  “Yea, we’ll get there around 8.”  Is that 8 AM or 8 PM?  You didn’t mention if your arrival was later today or tomorrow.  Is 8:30 still “around 8” or would that be considered too late?  All this makes dialogue complicated.  As another example, consider “That sounds sooo bad.”  Does that mean the activity is actually bad, or did the speaker mean that ironically the activity is actually quite good. Is the activity wrong to some folks but debatably acceptable to others?&lt;/p&gt;

&lt;p&gt;Language is too complicated to understand without full context.  And even with that, only training on target labels can help guide a model towards meaningful (ie. practically useful) understanding.  Encoding an utterance into a embedding through a language model or auto-encoder misses out on the critical human-in-the-loop validation. Foundation Models only perform language modeling, and thus are not sufficient for capturing the complexities for dialogue semantics.  In order to capture dialogue, extra supervision is necessary.&lt;/p&gt;

&lt;p&gt;The proof up to this point is not fully satisfying because we have only really shown that pre-training is insufficient with high probability.  Alternate forms of dialogue pre-training are still theoretically possible.  To tackle this in a more complete manner, we also offer some theoretical evidence.  We argue that language exists for communication and such communication is for achieving shared goals.  Since unsupervised pre-training does not have a labeled target, it does not have a clear goal, and therefore can never mimic this aim.  We are confident additional linguistic theory can be added to further support this claim.  In any case, despite the incomplete logic, the overall intuition should be clear: language models alone are not enough to build powerful dialogue models, and explicit training targets are needed.  We hope the reader finds this point convincing enough, so we now move onto discussing how such targets can be formed.&lt;/p&gt;

&lt;h1 id=&quot;meaning-representations-as-supervision&quot;&gt;Meaning Representations as Supervision&lt;/h1&gt;

&lt;p&gt;Since dialogues are so complex, one could convincingly argue that even defining a comprehensive schema is impossible since enumerating all options is beyond the capabilities of any organization.  We actually agree with this claim, and yet we still believe that a useful meaning representation (MR) of dialogue exists and can be obtained in cost effective manner.&lt;/p&gt;

&lt;p&gt;To see how, let us first define what we mean by a “meaning representation”.  A meaning representation is a way of defining the conversational semantics that can be concretely expressed and therefore can be used as a training signal for a model to learn. Many such expressions already exist, most commonly through the dialogue states as seen in Dialogue State Tracking benchmarks or policy states in dialogue-based reinforcement learning.  They have also been referred to as abstract meaning representations (AMRs) when used in the context of structured prediction.  Traditionally, these representations are also the target programs of semantic parsing.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-desiderata-for-a-good-mr&quot;&gt;What are the desiderata for a good MR?&lt;/h3&gt;
&lt;p&gt;Given that many meaning representations are possible, what would a “good” meaning representation look like?  For our purposes, the ideal label set would strike a balance between being:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;easy enough to annotate at scale&lt;/li&gt;
  &lt;li&gt;complex enough to faithfully capture the details of a conversation
Let’s study these trade-offs in more detail.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What does it mean to be easy to annotate? Scalable annotation means that a “commodity” annotator can perform the job without excessive training and at reasonable cost. This can be achieved by making the annotation task simple.  For example, you can make the process a matter of verifying a model’s predictions, rather than requiring the annotators to come up with the labels from scratch.  (How to bootstrap this process from a cold start is the topic for future discussion.)  Separately, we can also limit the scope of annotation to a single domain when starting out. Finally, we can limit the size of the ontology so not every variation is covered, which greatly simplifies the annotation task.  Suffice to say, there are a number of ways to design the annotation task such that annotators can move quickly, yet still produce accurate labels.&lt;/p&gt;

&lt;p&gt;The simplifications introduced in the previous paragraph beg the question: if the ontology is so restricted, how can we hope for a model to learn all the desired details within a dialogue.  Well, what do we want from a complex ontology?  Which details are actually necessary to be captured?  We refer back to Lemma 1, which implies that all we want to capture are specific slot-values (ie. tokens) as well as the semantics of longer sequences (ie. phrases).  Training large PLMs towards these two targets is entirely possible given sufficient training data, which itself is possible since the annotation task has been simplified tremendously.&lt;/p&gt;

&lt;p&gt;Critically, note what we do &lt;em&gt;not&lt;/em&gt; capture within our ontology.  (1) Beyond semantics, we can safely assume that a Foundation model can capture the syntax of dialogue, meaning this is not something we need to annotate.  Along those lines, (2) we assume our problem is scoped such that we do not require capturing pragmatics either.  Finally, while we care about slot-values, (3) we assume that slots themselves are pre-defined by an API, not something a model needs to predict.  Overall, we rely heavily on the capabilities of pre-trained LMs so the amount of explicit supervision required is actually quite minimal.&lt;/p&gt;

&lt;h3 id=&quot;corollary-manfacturing-cars-required-standardization&quot;&gt;Corollary: Manfacturing cars required standardization&lt;/h3&gt;
&lt;p&gt;As a slight tangent, let us draw a parallel from designing MRs to producing cars during the Industrial Revolution.
In order to manufacture a large number of cars, you first had to simplify and standardize the process of building a car.
Similarly, in order to label a large number of conversations, you have to first reduce and standardize the meaning representation of an utterance.  All car parts were joined together within an assembly line.  All conversations are labeled in succession by crowdworkers.  Automation helps turn each individual’s task into something extremely simple.  You transform each job from “can you build this car part” into “can you verify this car part was added correctly”.  Alternatively, you transform each annotation from “can you annotate this example” into “can you verify this proposed label is correct.”  In the end, you complete many labels/cars in a quick, yet high quality manner.&lt;/p&gt;

&lt;p&gt;In conclusion, we are able to capture the important details of a conversation by considering the key entities and semantics of a conversation.  We are able to annotate this quickly by concurrently not considering anything else, as well as making the process itself quite efficient.  Finally, we spend time designing the form of the meaning representation to be easy to manage.  This allows us to train powerful dialogue systems at scale.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Authors Note: This post is clearly not a formal proof, just having a bit of fun with the idea :)&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="data-strategy" /><category term="explainer" /><category term="dialogue" /><summary type="html">In order for a virtual assistant to be useful, the agent should do more than just information retrieval and basic chit-chat. Rather than pattern recognition on the response level, the agent should be able to perform pattern recognition on the discourse level so it can mimic human-reasoning (even as true understanding remains an elusive goal). If a model were to reason about an utterance, it must have been trained to do so. Furthermore, we argue that such training must be explicitly performed through (weakly) supervised learning, rather than implicitly extracted from a large pre-trained LM (eg. through careful prompting).</summary></entry><entry><title type="html">Data Collection Best Practices</title><link href="https://derekchen14.github.io/2021/09/11/data-collection-best-practices.html" rel="alternate" type="text/html" title="Data Collection Best Practices" /><published>2021-09-11T00:00:00-04:00</published><updated>2021-09-11T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/09/11/data-collection-best-practices</id><content type="html" xml:base="https://derekchen14.github.io/2021/09/11/data-collection-best-practices.html">&lt;p&gt;The first step to truly becoming an &lt;a href=&quot;https://morethanoneturn.com/2021/05/23/embracing-ai-first.html&quot;&gt;AI-first&lt;/a&gt; company is to adopt a &lt;a href=&quot;http://datacentricai.org/&quot;&gt;data-centric&lt;/a&gt; view, which naturally implies taking data collection seriously as a core competency of the business.  Even before involving any sophistcated algorithms to improve data quality, there are already many best practices to consider when performing manual data collection.  At a high level, this can be broken down into improvements 
&lt;!--more--&gt;
on the people, the tools and the process itself.  In total, they constitute a basic checklist of items to consider when performing data labeling for NLP tasks.&lt;/p&gt;

&lt;h2 id=&quot;annotator-training&quot;&gt;Annotator Training&lt;/h2&gt;

&lt;h3 id=&quot;recruiting&quot;&gt;Recruiting&lt;/h3&gt;
&lt;p&gt;Gathering quality data begins with reliable annotators who understand the problem you’re working on.  Your first line of defense are the tools built directly into MTurk including filters and quals.  Filters should be set to 90-95% acceptance rate and specific English-speaking locations.  A pro-tip is to consider places beyond the United States, such as to include Canada, Britain and Singapore.  Of course, if your NLP task requires multi-language or code-switched labels, then you should branch out even further.  Qualifications are mini-exams that you can require annotators pass before starting your HIT.  The benefit of using these quals to both (a) prevent spammers and (b) ensure that the worker understands the task cannot be overstated.&lt;/p&gt;

&lt;p&gt;Of course, you should also pay the crowd-workers a reasonable rate for their work.  It should be commonsense, but there is also a clear correlation between offering higher pay and attracting higher quality workers.  If you happen to be doing repeated work, it might make sense to keep a running list of known experts on your task such that you can invite them back rather than recruiting new folks every time.&lt;/p&gt;

&lt;h3 id=&quot;onboarding&quot;&gt;Onboarding&lt;/h3&gt;
&lt;p&gt;As new workers start your data collection task, they will need a well-written training manual to explain the task and how to go about labeling the text for any given situation.  Natural language utterances are extremely nuanced and thus require clear guidelines for picking out these detailed differences.  Dialogue in particular is heavily context dependent.  As you work more with crowdworkers, you will find almost all of them acting in good faith to provide the best annotations possible.  When something is mislabeled, it is much more likely the cause is due to lackluster guidelines or truly tricky conversations rather than workers behaving with ill-intent.  Thus, putting in the time to really write clear instructions is well worth the effort.&lt;/p&gt;

&lt;p&gt;Tips for writing great guidelines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;give an explanation of your task in clear, simple writing – without the jargon&lt;/li&gt;
  &lt;li&gt;remember that your reader is not an expert in linguistics or NLP&lt;/li&gt;
  &lt;li&gt;explain what is happening from the start, not from what you think they should already know&lt;/li&gt;
  &lt;li&gt;highlight the most important details in &lt;em&gt;italics&lt;/em&gt;, &lt;strong&gt;bold&lt;/strong&gt; or &lt;span style=&quot;color:green;&quot;&gt;color&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;offer examples of good annotations and examples of common mistakes when annotating&lt;/li&gt;
  &lt;li&gt;repeat yourself on the most critical parts if necessary&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to go above and beyond, consider creating tutorial videos and/or offering feedback through email.  When collecting data for ABCD,&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; we found that offering live feedback through a Slack/Discord channel yielded tremendous gains.  Other research has corroborated the findings that expert feedback can be quite helpful.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;  Intutively, it also makes sense that superficial interventions, such as asking the workers to provide a justification for their labels does not help much.  An interesting trick is to have the highest rated crowdworkers provide feedback to other crowdworkers which can help to naturally scale the process.&lt;/p&gt;

&lt;h3 id=&quot;retention&quot;&gt;Retention&lt;/h3&gt;
&lt;p&gt;For long-running tasks, you can also add in certain aspects to make sure the workers keep on their toes.  In particular, it is common practice to include occassional gold labels that you know are correct.  Then, any workers who mess up too many of these may have their qualifications revoked. Additionally, ideas such as time limits or other thresholds can help to prevent folks getting lazy.  More specifically, suppose you are collecting a dialogue chat, then a token minimum can be automatically checked to make sure the utterances they have generated match some minimum length.  Finally, you should perform occasional spot checks to ensure quality has not dropped.&lt;/p&gt;

&lt;p&gt;Just like running any company though, employee satisfaction becomes paramount.  It makes much more sense to think about how to retain your best workers than to waste extra time worrying about a few bad apples.  One of the unmentioned benefits of feedback and iteration is that it keeps the workers engaged.  Along those lines any sort of reward system, such as bonuses for particularly good labels, warrant a bit of discussion.  Bonuses can be offered per HIT that is done well, or as a one-off system for people who perform well in aggregate.  If there are folks managing and supporting other crowdsource workers then this would certainly warrant some sort of bonus. Be creative in your rewards! Utimately, anything that can help the workers do their job better will end up helping you.&lt;/p&gt;

&lt;h2 id=&quot;annotation-experience&quot;&gt;Annotation Experience&lt;/h2&gt;

&lt;h3 id=&quot;general-principles&quot;&gt;General principles&lt;/h3&gt;
&lt;p&gt;Making the data collection process as realistic as possible will drastically improve the data being collected because workers no longer need to conciously think about all the rules and regulations in the guidelines.  Instead, they can just focus on acting normally and letting their natural tendencies take over, which is what we want to train a model to do anyway.  This insight compelled us to develop Expert Live Chat,&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; which differs from typical Wizard-of-Oz data collection by three aspects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Conversations are conducted continuously in real-time.
    - no set number of turns
    - no templated responses
    - interlocutors can speak for multiple turns in a row&lt;/li&gt;
  &lt;li&gt;Users involved are not interchangeable.
    - there is an explicit agent and customer relationship, which mimics real-life
    - since people have distinct roles, the typical customs of how to behave naturally arise&lt;/li&gt;
  &lt;li&gt;Players are informed that all participants are human.
    – there is no wizard behind the scenes
    - encourages people to act like they would with humans rather than with machines
While these aspects make data collection more difficult (methods for resolving this discussed in the paper), they dramatically increase the verisimilitude of the generated conversations.  More generally, even when dealing with non-dialogue related data tasks, any way to make the task itself feel natural should improve results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;human-computer-interaction&quot;&gt;Human-computer interaction&lt;/h3&gt;
&lt;p&gt;There exists a large body of work in &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;&gt;HCI&lt;/a&gt; and &lt;a href=&quot;https://www.interaction-design.org/literature/article/what-is-design-thinking-and-why-is-it-so-popular&quot;&gt;Design Thinking&lt;/a&gt; around how to make better software, much of which can be carried over to the &lt;a href=&quot;https://aclanthology.org/W12-3613/&quot;&gt;design of annotation tools&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic idea behind designing great user experiences is to make the task as simple and intuitive as possible.  This encompasses making the labeling task itself straightforward (more on this in the next section), but also includes keeping the tool simple and easy to use as well.  If there are ways to minimize, or even eliminate, clicking around on a mouse, then you should do so.  For example, rather than having to click [Next Item] or [Submit], allow these to be transformed into keyboard shortcuts. Rather than a long dropdown menu the user must scroll through, allow for fuzzy search on the list of labels.  Even when restricted to just typing, find ways to minimize the number of keystrokes needed.  For example, offer a text autocomplete function for common phrases or inputs.&lt;/p&gt;

&lt;p&gt;Prevent users from making mistakes, but if they do, then allow those users to fix them easily.  We can instantiate these principles through distinct elements in the user interface.  Buttons meant to be clicked should seem clickable (e.g. bolded, shaded), whereas buttons meant to be avoided should not (e.g. grayed out).  If a user clicks on a button that causes irreverisible changes or destructive actions (e.g. delete all annotations), show a warning dialog to check if this action is something they actually wanted to perform.  If a user &lt;em&gt;does&lt;/em&gt; click on something accidentally, provide an option to easily recover from the error; for example, a back button to return to the previous example.&lt;/p&gt;

&lt;h3 id=&quot;system-architecture&quot;&gt;System architecture&lt;/h3&gt;
&lt;p&gt;Real world data collection tasks don’t just end when the text has been annotated.  We should be desiging for the end-to-end data collection pipeline from ideation to shipping into production.  Is there anything that allows researchers or engineers to quickly spin up new experiments?  Perhaps it is very common for you to perform human evaluation on new iterations of your dialogue model.  Templates can be put into place to easily spin up new evaluation rounds.  This can even be automated such that whenever a new model is fully trained, automatically kick off qualitative evaluation (e.g. coherency, fluency) the same way you might kick off quantitative evaluation (e.g. BLEU, perplexity).&lt;/p&gt;

&lt;p&gt;Automation can work in reverse as well.  Suppose the task is sentiment analysis where given an input utterance, you want to classify as either positive, negative or neutral.  After finishing a round of data collection, the system will automatically pass the labels to an in-house annotation team to QA.  High confidence labels can be directly integrated into data store to immediately start-up a new cycle of model training.&lt;/p&gt;

&lt;h2 id=&quot;annotation-process&quot;&gt;Annotation Process&lt;/h2&gt;

&lt;h3 id=&quot;labeling-platforms&quot;&gt;Labeling platforms&lt;/h3&gt;
&lt;p&gt;When performing data collection, realize that a wide spectrum of services are available to do the actual annotation.  Starting with the broadest audience, serices such as &lt;a href=&quot;https://toloka.yandex.com/&quot;&gt;Toloka&lt;/a&gt; and &lt;a href=&quot;https://www.mturk.com/&quot;&gt;Amazon Mechnical Turk&lt;/a&gt; allow you to tap into a much wider pool for NLP annotation tasks.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;  Large commerical vendors are also an option, but frankly I would only recommend them when there are strict PII requirements, such as dealing with medical or enterprise data, since otherwise their high price points aren’t justified.  The next level involves working with contractors such as those found through &lt;a href=&quot;https://www.upwork.com/&quot;&gt;upwork&lt;/a&gt; or &lt;a href=&quot;https://www.fiverr.com/&quot;&gt;fiverr&lt;/a&gt;.  The benefit is the ability to retain knowledge over time, at the cost of a higher upfront investment to vet for quality workers.&lt;/p&gt;

&lt;p&gt;If you have the budget, then moving the entire process in-house would obviously be preferred.  An in-house annotation team can be guided to the exact task you prefer and will always be available.  The internal communication can also allow for side-by-side iteration on the task design and data collection, since the ontology can often shift due to feedback from the raw data.  Finally, if you can afford to hire a handful of PhD linguists or a team of in-house experts to perform annotation, then you are probably a FAANG company.  In all seriousness, expert annotation was the traditional method of obtaining labels, but fails to scale to the size needed to train modern ML models.&lt;/p&gt;

&lt;h3 id=&quot;task-design&quot;&gt;Task Design&lt;/h3&gt;
&lt;p&gt;If you can formulate your problem in a simple manner, then you can obviate the need for subject matter experts and take full advantage of crowd-source workers to scale the effort.  For example, rather than offering a long list of options to choose from for a label, offer just a few multiple choice options.  Even with a limited ontology, annotation can still be difficult when dealing with multi-class and multi-label tasks (ie. many intents can be present in a single dialogue utterance). One ingenious method is to break down the labeling task into a series of simple choices, effectively turning the annotator into a human decision tree where the number of choices for a single HIT is equivalent to the branching factor.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;  Another technique is to minimize the cognitive load so you can build a semantic parser&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; or dialogue agent overnight.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; The authors transformed the task by reversing the process.  Rather than providing an utterance and asking for a label, the task starts with a known label and asking the worker to paraphrase a template associated with the known label into a natural language utterance.&lt;/p&gt;

&lt;p&gt;If you can transform the problem into binary selection, perhaps with a contrastive learning algorithm, the task becomes that much easier to label and review.  But even more than the time savings, there is a certain level of simplicity where the task can rely on System I rather than System II processing.  Whereas human eyes and brains are naturally pre-disposed to perform image classification, concious effort must be made to parse speech and perform contextual reasoning.  Labeling then becomes a task the annotator can simply react to rather than think about conciously, which more closely matches how a neural network operates. In other words, effort spent on task simplication may offer exponential gains on model improvement.&lt;/p&gt;

&lt;h3 id=&quot;active-learning&quot;&gt;Active learning&lt;/h3&gt;
&lt;p&gt;A common idea to appears when trying to improve the annotation process is to be more intelligent about how we select what to label.  While active learning does seem to provide noticeable benefits,&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; much of the research suggests that the gains are inconsistent&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; or limited&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.  More recent work has shown that the cause might be due to the fact that active learning often picks up on outliers which are hard or even impossible to learn from.&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Given the difficulties of getting things to work well in practice, it is my opinion that active learning might not be a great avenue to explore.  While you may see gains of a few weeks in speed-up for example selection, it will cost you a few weeks to set up the process as well.  Assuming there are problems along the way, you might even end up in a net-negative position in terms of annotation speed.  In conclusion, if there are obvious cases where certain examples need extra labels (eg. a brand new class was added to the ontology), then some focused annotation might be worthwhile. But in general, active learning might not be worth the effort.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Effective data collection is a skill and an art.  Depending on the nature of the task, certain methods may help speed up the process, but we want to careful that efficiency gains aren’t occuring at local maxima.  Ultimately, investments in data collection will pay great dividends and should be taken seriously by any company desiring to become AI-first.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.naacl-main.239/&quot;&gt;Action-Based Conversations Dataset&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Nangia et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.acl-long.98/&quot;&gt;Effective Crowdsourcing Protocol for Data Collection&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Snow et al. (2008), &lt;a href=&quot;https://aclanthology.org/D08-1027/&quot;&gt;Evaluating Non-Expert Annotations for NLP Tasks&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dian Yu and Zhou Yu (2020), &lt;a href=&quot;https://aclanthology.org/2021.eacl-main.94/&quot;&gt;MIDAS: A Dialog Act Annotation Scheme&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wang et al. (2015), &lt;a href=&quot;https://aclanthology.org/P15-1129/&quot;&gt;Building a Semantic Parser Overnight&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shah et al. (2018), &lt;a href=&quot;https://aclanthology.org/N18-3006/&quot;&gt;Building a Conversational Agent Overnight&lt;/a&gt; &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ash et al. (2020), &lt;a href=&quot;https://openreview.net/forum?id=ryghZJBKPS&quot;&gt;BADGE: Deep Batch Active Learning&lt;/a&gt; &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lowell et al. (2019), &lt;a href=&quot;https://aclanthology.org/D19-1003/&quot;&gt;Practical Obstacles to Deploying Active Learning&lt;/a&gt; &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ein-Dor et al. (2021), &lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.638/&quot;&gt;Active Learning for BERT&lt;/a&gt; &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Karamcheti et al. (2021),  &lt;a href=&quot;https://aclanthology.org/2021.acl-long.564/&quot;&gt;Investigating the Negative Impact of Outliers for VQA&lt;/a&gt; &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="ai" /><category term="data-strategy" /><category term="explainer" /><category term="lists" /><summary type="html">The first step to truly becoming an AI-first company is to adopt a data-centric view, which naturally implies taking data collection seriously as a core competency of the business. Even before involving any sophistcated algorithms to improve data quality, there are already many best practices to consider when performing manual data collection. At a high level, this can be broken down into improvements</summary></entry><entry><title type="html">Proper Assessment of Data Value</title><link href="https://derekchen14.github.io/2021/07/24/proper-assessment-data.html" rel="alternate" type="text/html" title="Proper Assessment of Data Value" /><published>2021-07-24T00:00:00-04:00</published><updated>2021-07-24T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/07/24/proper-assessment-data</id><content type="html" xml:base="https://derekchen14.github.io/2021/07/24/proper-assessment-data.html">&lt;p&gt;Data is the new oil.  It underpins an undeniable aspect of growth in the popularity and dominance of deep learning.  But is all data created equal? What makes some data points worth more than others?  How could we go about calculating what each data point is worth?  If data is so important, we should certainly want to have a proper way to assess its value.  To do so, we should begin by recognizing that
 &lt;!--more--&gt;
some data is worth more than others.&lt;/p&gt;

&lt;p&gt;For starters, labeled data should be worth more than unlabeled data.  Even with the most advanced pretraining, we would need a thousand to a million times more unlabeled data to rival the performance of valid labeled data for a given task.  As a next step, we might notice that even within annotated data, certain labels are more likely to be noisy as a natural consequence of crowdsourcing large datasets.  Thus, we could conclude that clean data is worth more than noisy data.  Going further, within the subset of correctly labeled data, certain examples will help the model learn more, perhaps because they are data points near the decision boundary or perhaps they are more closely aligned with test distribution.&lt;/p&gt;

&lt;p&gt;With all that said, how could we go about calculating such a thing?  Is there a systematic and principled method for determining the proper value of data?  That’s what this post will dive into.  Although there are no perfect answer, we chart three major directions worth exploring.  First, we consider data as more valuable when it maximizes information gain.  Next, we measure a datapoint’s value by observing its impact on the model’s training dynamics.  Lastly, we consider a training example’s value as how much improvement it provides over not having that example.&lt;/p&gt;

&lt;h2 id=&quot;1-maximizing-information-gain&quot;&gt;1) Maximizing Information Gain&lt;/h2&gt;

&lt;p&gt;Data that contains more information should be considered more valuable.  We can quantify the level of information within the data as examples which provide more diversity or examples that lowers uncertainty.&lt;/p&gt;

&lt;h3 id=&quot;diversity&quot;&gt;Diversity&lt;/h3&gt;

&lt;p&gt;Intuitively, diversity is helpful because data that adds something different from what I’ve seen before is more informative.  We can measure through various forms of novelty&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or mutual information.  From the novelty perspective, data points that are embedded in a space further away from anything we’ve seen so far is considered diverse.  Variants here include how we embed the data and how to calculate distance.  A notable case, as proposed by BADGE&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, is to use the gradient of the training example as its embedding, which encodes not only diverse but also high magnitude.&lt;/p&gt;

&lt;p&gt;From the information perspective, we can measure the mutual information of the training example compared to a subset of the training data. Mathematically, we measure mutual information as:&lt;/p&gt;

\[MI(X, Y) = D_{KL}  [ P(X,Y) || P(X) \cdot P(Y) ]\]

&lt;p&gt;where X can be our given data so far and Y is our current data point. Diverse data should be less predictive of current training data, and thus have lower mutual information.&lt;/p&gt;

&lt;h3 id=&quot;uncertainty&quot;&gt;Uncertainty&lt;/h3&gt;

&lt;p&gt;We also prefer data that allows us to learn more information regarding areas that I’m not so sure about. Mathematically, this can be formalized as lowering the entropy of a model’s outputs.  Alternatively, we can use uncertainty as a tool by measuring a model’s uncertain about that data point.  Then getting a correct label for that datapoint would be highly informative.  Since a single datapoint is less likely to have a large impact, we can measure the impact of a batch of data.  Then, to get the value of each data point, we average over all batches that the datapoint participated in.  Furthermore, recall that model uncertainty can be measured in multiple ways including variance approximation with dropout&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; or explicit Bayesian networks&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2-observing-training-dynamics&quot;&gt;2) Observing Training Dynamics&lt;/h2&gt;

&lt;p&gt;Since modern neural networks are over-parameterized, they can completely memorize the training data&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.  However, those same models typically learn cleanly labeled examples before the noisy examples.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.  Therefore, we can determine an example is being more valuable by observing how it fluctuates during training.&lt;/p&gt;

&lt;h3 id=&quot;observing-the-softmax&quot;&gt;Observing the Softmax&lt;/h3&gt;

&lt;p&gt;As suggested within Dataset Cartography&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, examples can be divided into those that are easy-to-learn, ambiguous and hard-to-learn.  The ambiguous ones supposedly help the most with learning, as long as some easy-to-learn examples are also included to stabilize training.  These ambiguous examples are those likely to be near the decision boundary, and thus help the model generalize.  Most interesting for our examination are the hard-to-learn examples, which can often be considered mislabeled.&lt;/p&gt;

&lt;p&gt;To find hard-to-learn examples, we track the model’s confidence across time to calculate a score.  More specifically, we track the output of the model for each training example as measured by:&lt;/p&gt;

\[\mu_i = \frac{1}{E} \sum_{e=1}^E p_{\theta^e} (y_i^{\star} | x_i)\]

&lt;p&gt;where \(\theta^e\) are the model’s parameters at epoch \(e\). Then, these scores are averaged over all training epochs where the examples that have a lower score are those most likely to be noisy.  We can then discard those examples or relabel them.  The general idea is that when a model is not confident in it’s output (ie. places low probability mass), this suggests that the label assigned to that example is wrong.&lt;/p&gt;

&lt;h3 id=&quot;observing-logits&quot;&gt;Observing Logits&lt;/h3&gt;

&lt;p&gt;Alternatively, rather than tracking the model’s final outputs, we can also consider looking at it’s logits. Specifically, Pleiss et al. suggest looking at the Area Under the Margin (AUM) of an example’s logit compared to the largest other logit&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.  In formula form, we measure the score as:&lt;/p&gt;

\[AUM(x,y) = \frac{1}{T} \sum_{t=1}^T z_y^{(t)} (x)  - max_{i \neq y} \left( z_i^{(t)} (x) \right)\]

&lt;p&gt;where \(z_y\) represents the assigned logit and \(z_{i \neq y}\) is the largest other logit.&lt;/p&gt;

&lt;p&gt;To understand what is going on, suppose we are classifying some images as either dogs, cats, horses, frogs, cows, etc.  Additionally, suppose that the given example is a dog, then the assigned logit is also dog.  The model’s next largest logit is, say, a cat but the the gap between them is large since the model has done a good job differentiating between the two.  This large gap translates to a large AUM score.  Now suppose we are given a fish example, labeled as a dog.  Then the model’s assigned logit may be quite low, whereas the next largest logit of fish is likely to be quite high since the model (rightly) believes that the image is a fish. Thus, the AUM will be low and once again we have identified a low value data point.&lt;/p&gt;

&lt;h3 id=&quot;observing-hidden-state&quot;&gt;Observing Hidden State&lt;/h3&gt;

&lt;p&gt;As a final direction, we might also consider how the hidden state of the model changes over time as training progresses.  In particular, I would assume that correctly labeled examples need smaller gradients and have more steady hidden states, whereas incorrectly labeled examples will exhibit more fluctuation.  The additional benefit of looking at the hidden state is that we can potentially have multiple output heads, representing multiple tasks.  Since each task would have a different set of logits and softmaxes, we instead look at the latest common hidden state of all tasks to measure the training dynamics.  This is a novel idea that hasn’t been tested yet, so it could be totally wrong, but I think it’s worth a shot.&lt;/p&gt;

&lt;h2 id=&quot;3-measuring-marginal-improvement&quot;&gt;3) Measuring Marginal Improvement&lt;/h2&gt;

&lt;h3 id=&quot;naive-difference&quot;&gt;Naive Difference&lt;/h3&gt;

&lt;p&gt;The basic ideas is that if a data point offers high marginal improvement, then it should be considered valuable. So as a baseline, we could measure the accuracy of the model trained on a subset of the data \(p\) and then measure the accuracy again after trained with the extra data point \(q\).  The marginal gain in accuracy \(p - q\) can then be attributed to the data point’s influence.  This is quite reminiscent of how LIME operates for the purposes of ML explainability&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The clear drawback is that this requires re-training the model for every single data point, which quickly becomes intractable.  The other issue is that a single datapoint might not make a much of a difference depending on the data that came before it, so the impact would be negligible.  As a degenerate example, suppose we had a datapoint \(x_1\) that was duplicated in the dataset as \(x_2\).  If the model encountered \(x_1\) first, then it would have some value, but if the model had encountered \(x_2\) in a prior round, then \(x_1\) would be deemed to have no value despite not actually changing.  To mitigate the issues described above, we first describe Shapley Values as a formal method for dealing with the ordering and then an algorithmic update to deal with the tractability.&lt;/p&gt;

&lt;h3 id=&quot;shapley-values&quot;&gt;Shapley Values&lt;/h3&gt;

&lt;p&gt;Originally developed for economic purposes, the Shapely Value (SV) calculates the marginal benefit offered by each person within a coalition&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;.  People with high values are important, while those with especially low values are not helpful, and perhaps even harmful to the team.  In the case of data collection, each person represents an annotated label.  Accordingly, the SV of each datapoint tells us whether it is clean or noisy. and thus should be relabeled and/or eliminated.&lt;/p&gt;

&lt;p&gt;This is done by calculating the marginal utility of each datapoint, averaged over all permutations of the subsets of collected data.  The permutation of a subset represents shuffling the order of the data when training the neural network. Thus, not only would we need to retrain a network for all subsets of data, we would need to take into account all permutations of drawing the subset.&lt;/p&gt;

\[s_i = \frac{1}{N} \sum_{S \subseteq D \\ z_i} \binom{N-1}{|S|}^{-1} [v(S \cup {z_i}) - v(S)]\]

&lt;p&gt;In the equation above \(s_i\) stands for the Shapley Value of the \(i^{th}\) example out of &lt;em&gt;N&lt;/em&gt; training examples.  \(S \cup {z_i}\) represents the subset with the extra datapoint, whereas \(S\) by itself represents the subset without it.  The function \(v(\cdot)\) is the standalone point value of the data, which can be approximated by the accuracy of the model trained with that data.  Due to the special properties of this value, the Shapley Value actually provides a unique, ideal solution for calculating data values for a given model&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.  We still need to deal with the issue of calculating the value in a reasonable time frame.&lt;/p&gt;

&lt;h3 id=&quot;knn-approximation-with-lsh&quot;&gt;KNN approximation with LSH&lt;/h3&gt;

&lt;p&gt;Authors within Jia et al. ingeniously get around the problem of re-training the model for every data by choosing a model that requires no training to operate&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;.  In particular, the authors cast the model as a KNN rather than a neural network, so training for each new datapoint is trivial (there isn’t any), with only memory limitations to consider.  First, recall that for each training input, the KNN gets that example correct if the test input matched with the training input has the correct label.  The test label becomes the prediction without any extra effort beyond a lookup call.&lt;/p&gt;

&lt;p&gt;With this in mind, the algorithm proceeds as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For each test example \(x_j\):
    &lt;ul&gt;
      &lt;li&gt;Sort the distance towards all training data to that test example&lt;/li&gt;
      &lt;li&gt;Calculate the value of each test example as the normalized marginal improvement it provides&lt;/li&gt;
      &lt;li&gt;Marginal improvement means:
        &lt;ul&gt;
          &lt;li&gt;Case 1: training example \(x_i\) and \(x_{i+1}\) are both still wrong, then the improvement is zero since 0 - 0 = 0&lt;/li&gt;
          &lt;li&gt;Case 2: training example \(x_i\) is right, while the point further away is wrong, then improvement is 1 - 0 = 1&lt;/li&gt;
          &lt;li&gt;Case 3: training example \(x_i\) is wrong while the point further away \(x_{i+1}\) is actually correct, then the improvement is negative 0 - 1 = -1&lt;/li&gt;
          &lt;li&gt;Case 4: both training examples are correct since we are dealing with nearby datapoints, so improvement is nullified since 1 - 1 = 0&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Normalized means to divide by the number of the remaining train examples being considered to keep things fair&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;For each train example \(x_i\):
    &lt;ul&gt;
      &lt;li&gt;Average over the contributions that \(x_i\) made to the test examples calculated earlier&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The secret is that calculating the value of each test example is much simpler compared to previous methods because it is simply a deciding which of the four cases the training label falls under.  Thus going through the loops is quite fast, where the bottleneck is the sorting of training data.&lt;/p&gt;

&lt;p&gt;As another speed-up, the authors turn to LSH (locality sensitive hashing) to minimize the amount of searching needed. To see how this works, first note that training examples far away from the test example likely contribute no marginal improvement to the \(s_j\).  So we choose some threshold T, where training points further than T are just assumed to have value = 0 and those items are all eliminated from calculation. To be even smarter, an extension could somehow eliminate the super close training examples as well, since they also offer no marginal value.  The authors do not explore this.&lt;/p&gt;

&lt;p&gt;The other part is to obtain the \(M - T\) candidates using truncated LSH, rather than sorting by exact distance.  Concretely, note that the LSH process will (a) embed all the documents into some fixed multi-dimensional space that has some semantic meaning.  Typically, minHash is used, but for this case we use a specially designed hash function that approximates a L2 norm. (b) Suppose each document is now a 80-dim vector.  Break apart all the documents into bands (eg. b = 8, which has 10-dim vector) and compare only the partial vectors within each bucket using a hash function. (c) The partial vectors which are hashed to the same bucket collide because they are supposedly similar.  The corresponding full documents become candidate pairs. (d) Compare the limited number of candidate pairs on an exact match basis.  So for our case, we just switch up Step D into calculating Shapely Values rather than searching for duplicate pairs.  Overall, this means that even with 1 million training examples, we can calculate approximate Shapley Values in under an hour.&lt;/p&gt;

&lt;h2 id=&quot;key-use-cases&quot;&gt;Key Use Cases&lt;/h2&gt;

&lt;p&gt;Since data collected from crowdsourcing is often noisy, redundant labels are collected for each datapoint so that we can determine a final label through a majority vote.  Given these methods of assessing data quality, we can now consider an alternative formulation whereby we do not need spend extra effort upfront for all training examples.  Instead, we can label once, identify low value / mislabled data and then throw that data away or put it up for relabeling.  Since most annotators act in good faith and provide the correct label, this direction should decrease costs by at least half.&lt;/p&gt;

&lt;p&gt;Additionally, some of the methods described above are able to assign value to unlabeled data.  In those cases, we can perform active learning to further maximize our limited resources.  If we go the other way, after all the data is collected, we could also consider paying more to workers who annotated better labels.  More generally, we can pay workers at a rate commensurate with the value of the labels they provide.&lt;/p&gt;

&lt;p&gt;Overall, having a measure of data quality is useful to understand not only the data but ultimately the strength of the model.  Higher quality data naturally leads to better performing models.  This means real-world impact for whatever we designed our models to do.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2018/file/b1301141feffabac455e1f90a7de2054-Paper.pdf&quot;&gt;Novelty Seeking Agents&lt;/a&gt; (Conti et al., 2018) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.03671&quot;&gt;Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds&lt;/a&gt; (Ash et al., 2019) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v48/gal16.html&quot;&gt;Dropout as a Bayesian Approximation&lt;/a&gt; (Gal and Ghahramani, 2016) &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/blundell15.html&quot;&gt;Weight Uncertainty in Neural Networks&lt;/a&gt; (Blundell et al., 2015) &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=Sy8gdB9xx&quot;&gt;Understanding Deep Learning Requires Rethinking Generalization &lt;/a&gt; (Zhang et al., 2017) &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.5555/3305381.3305406&quot;&gt;A Closer Look at Memorization in Deep Networks&lt;/a&gt;  (Arpit et al., 2017) &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.746/&quot;&gt;Dataset Cartography&lt;/a&gt; (Swayamdipta et al., 2020) &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.10528&quot;&gt;Identifying Mislabeled Data using AUM Ranking&lt;/a&gt; (Pleiss et al., 2020) &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot;&gt;Local Interpretable Model-Agnostic Explanations (LIME)&lt;/a&gt; (Ribeiro et al., 2016) &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.rand.org/pubs/research_memoranda/RM0656.html&quot;&gt;Notes on the N-Person Game&lt;/a&gt; (Shapley, 1951) &lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf&quot;&gt;Data Shapley: Equitable Valuation of Data&lt;/a&gt; (Ghorbani and Zou, 2019) &lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.08619&quot;&gt;Efficient Data Valuation for Nearest Neighbor Algorithms&lt;/a&gt; (Jia et al., 2019) &lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="ai" /><category term="data-strategy" /><category term="explainer" /><summary type="html">Data is the new oil. It underpins an undeniable aspect of growth in the popularity and dominance of deep learning. But is all data created equal? What makes some data points worth more than others? How could we go about calculating what each data point is worth? If data is so important, we should certainly want to have a proper way to assess its value. To do so, we should begin by recognizing that</summary></entry><entry><title type="html">Meta Reinforcement Learning</title><link href="https://derekchen14.github.io/2021/06/01/meta-rl.html" rel="alternate" type="text/html" title="Meta Reinforcement Learning" /><published>2021-06-01T00:00:00-04:00</published><updated>2021-06-01T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/06/01/meta-rl</id><content type="html" xml:base="https://derekchen14.github.io/2021/06/01/meta-rl.html">&lt;p&gt;This post will explore meta reinforcement learning with only minimal math.  We attempt to dive into the core concepts without making it too complicated.&lt;/p&gt;

&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Start by recalling that the RL problem can be defined by the attributes of a Markov Decision Process (MDP) with components of the set of states, action space, transition matrix, reward function and initial starting state:  \(\{ \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, s_0 \}\).  We solve an RL problem by interacting with the environment through some trajectory \(\tau\) to maximize rewards.  More specifically, an agent starts at some initial state \(s_0\), takes actions \(a \in \mathcal{A}\) which will take it to some new state \(s\) and recieve some reward \(r_t\).  By maximizing the sum of discounted rewards, we have optimized the Bellman equation to arrive at a good agent.&lt;/p&gt;

&lt;h2 id=&quot;meta-learning&quot;&gt;Meta-learning&lt;/h2&gt;

&lt;p&gt;Traditional supervised learning aims to find an optimal model that is great at performing some target task during test time.  In contrast, meta-learning tries to develop a model \(\theta^{\star}\) that is not necessarily good to start with, but can become great at the target task using only a few update steps.  In other words, we develop models that have good performance in the few-shot learning setup.  To do so, a model ought to learn how to learn.&lt;/p&gt;

&lt;p&gt;To understand how this works, we take a brief detour into multi-task learning.  Multi-task learning wants a model that can perform well on many tasks at once. A key difficulty is to learn each incremental task without catastrophic forgetting of previously learned tasks.  The meta-RL setup also contains many tasks, which we refer to as support tasks, which can be used to aid the learning of the target task.  While multi-task learning aims to do well at all the task, classic meta-learning only cares about doing well on the one target task.  However, meta-learning can be viewed as being more difficult since it tries to learn the target task with substantially fewer datapoints during test time.&lt;/p&gt;

&lt;p&gt;One way of tackling meta-learning is to use metric based methods, many of which boil down to smarter versions of kNNs.  Suppose we have ten classes of images from CIFAR10. During training, we have to embed all the images into a shared embedding space, but during training we simply match our test image to the nearest one we’ve seen before. Note that two areas to tweak include (1) how we embed these examples and (2) how we calculate the distance between two examples. A Matching Network (&lt;a href=&quot;https://arxiv.org/abs/1606.04080&quot;&gt;Vinyals et al., 2016&lt;/a&gt;) makes this a bit smarter by matching to a weighted combination of the nearest neighbors rather than to just the single nearest image.  Their embedding is a BiLSTM or CNN and the distance is cosine similarity.  A Relation Network (&lt;a href=&quot;https://arxiv.org/abs/1711.06025&quot;&gt;Sung et al., 2018&lt;/a&gt;) learn the a distance metric instead using a CNN that outputs a similarity score.  A Prototypical Network (&lt;a href=&quot;https://arxiv.org/abs/1703.05175&quot;&gt;Snell, Swersky &amp;amp; Zemel, 2017&lt;/a&gt;) change the embeddings during training to be the average of the support sets.  Given nine support classes, a “prototype” vector is calculated for each class which is the weighted average of the embedded examples in that class.  The distance used is a squared Euclidean distance.  We can imagine many other versions based on other embedding functions and learned distance metrics.&lt;/p&gt;

&lt;p&gt;The other way of tackling meta-learning is through gradient based methods, popularized by Model-Agnostic Meta-Learning (MAML) from Finn et al. in 2017.  What we ultimately want is a model parameterized by \(\theta^{\star}\) that can learn with just a few datapoints (let’s say 20).  So we simply mimic this setup during training.  Start with some initial model \(\theta_t\) and pass in a bunch of training examples from the support set.  Take some gradient update steps to achieve an updated model \(\hat{\theta_t}\), which we refer to as the inner-loop.  Now, pass in 20 training examples for the target set (to match what we expect during test time) through \(\hat{\theta_t}\) to obtain new gradients.  Use these gradients to update \(\theta_t\), which results in \(\theta_{t+1}\).  Next, repeat the process with new training data, reaching \(\theta_{t+2}\).  We call this training the outer loop.  After some &lt;em&gt;n&lt;/em&gt; rounds of outer loop training we have model \(\theta_{t+n}\) which we use as \(\theta^{\star}\) during test time.  Model \(\theta^{\star}\) should work reasonably well since it should have learned basic concepts like edges and shapes that help it learn quickly in new settings.&lt;/p&gt;

&lt;h2 id=&quot;meta-reinforcement-learning&quot;&gt;Meta Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;One way to view meta-learning is that rather than training on a bunch of datapoints, we instead train on a bunch of support tasks.  This gives another distinction with multi-task learning, namely that meta-learning &lt;em&gt;improves&lt;/em&gt; as the number of tasks increase, whereas multi-task performance will typically &lt;em&gt;decrease&lt;/em&gt; as the number of tasks grow.   To transfer into meta-RL, we just recognize that reinforcement learning tasks as defined by their MDPs.  In other words, rather than training on a bunch of support tasks, we train on a bunch of MDPs and test on the target MDP during test time.&lt;/p&gt;

&lt;p&gt;Meta-learning in a supervised setting tries to learn something useful of the problem space shared across different support tasks.  As an example, if our support tasks include question answering about celebrity gossip, news articles and tech blogs, we could hope that the model meta-learns about reading comprehension.  Then during test time, when our target task is question answering about financial updates, the model only needs to learn finance jargon since it already knows how to read.  For reinforcement learning, if our support MDPs include unique reward functions for a humanoid learning to run left, forward and backward, we could hope that the model has learned to balance and move.  Then, during test time, when our target MDP offers rewards for running to the right, the humanoid can learn this quickly.&lt;/p&gt;

&lt;h2 id=&quot;optimization-methods&quot;&gt;Optimization Methods&lt;/h2&gt;

&lt;p&gt;At a high-level, there are at least three methods for optimizing a meta-RL model (&lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa19/static/slides/lec-20.pdf&quot;&gt;Rakelly 2019&lt;/a&gt;).  These forms are all semantically equivalent, but can be seen as different ways of solving the same problem.&lt;/p&gt;

&lt;h4 id=&quot;1-optimization&quot;&gt;1. Optimization&lt;/h4&gt;
&lt;p&gt;We can train our reinforcement learning method using the MAML style gradients as we have already encountered.  If we optimize our model using policy gradients algorithm, then we are pushing up the likelihood of states that lead to higher rewards and pushing down states that lead to lower reward, which is similar to how we react to losses in supervised learning.  So the transition from regular meta-learning to RL-based meta-learning is to learn from reward signals rather than loss signals.  Just like regular meta-learning needed to develop first-order MAML (&lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;Finn et al., 2017&lt;/a&gt;) to get around the double-gradient issue, RL-based meta-learning also needs to make some simplifications along the way.  The major benefit of viewing meta-RL as optimization is that it works just like regular deep learning and will converge to the optimal solution given enough data.  We say that this method is “consistent”.  The downside is that if we face sparse rewards, the model will suffer even worse than in regular RL, which is already not so great.&lt;/p&gt;

&lt;h4 id=&quot;2-recurrence&quot;&gt;2. Recurrence&lt;/h4&gt;
&lt;p&gt;We would like a model to meta-learn something about the environment even if we didn’t get a reward for that episode, such as how to move around in the simulated world and the fact that the current trajectory was bad (thus leading to no reward).  Once way to keep track of such data is to feed each trajectory through an RNN instead.  Then the hidden states &lt;em&gt;h&lt;/em&gt; of the RNN can keep track of such meta-data.  A single RNN will run multiple episodes to produce multiple inner-gradient updates, which are then used to produce a single outer-gradient updates.  The hidden state of the RNN is reset to its initial \(h_0\) after each outer-loop gradient update, and the process can repeat again.  While recurrence is powerful at capturing more detail in the hidden state, this information is latent and may not end up capturing the details we care about (aka. “not expressive”).&lt;/p&gt;

&lt;h4 id=&quot;3-contextual&quot;&gt;3. Contextual&lt;/h4&gt;
&lt;p&gt;As an improvement on the recurrence method, we can bias the details we want to capture.  Suppose the model knows the MDP is it operating based on the context (ie. the rewards it has recieved so far). This will help the model produce temporally coherent trajectories when accomplishing the task, as opposed to taking random actions as it starts to explore the state space.  Consequently, the contextual method aims to predict the latent MDP &lt;em&gt;z&lt;/em&gt; before predicting the state, which produces more structured exploration during the inner loop  (&lt;a href=&quot;https://arxiv.org/abs/1802.07245&quot;&gt;Gupta et al., 2018&lt;/a&gt;) .  Concretely, we would aim to learn a model that first learns \(q(z|c)\), where &lt;em&gt;c&lt;/em&gt; is the context and then updates the \(\pi_{\theta}(a|s,z)\).  We learn &lt;em&gt;z&lt;/em&gt; with variational inference where we optimize the ELBO with \(D_{KL} [q_{\phi}(z|c_i) || p(z)]\) where \(p(z)\) is a normal Gaussian.  During inference, the model will choose actions conditioned on its best estimate of the environment parameters based on the context.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Meta-RL is a powerful tool for training reinforcement learning algorithms with limited data.  When applied to dialogue policy management, we can imagine learning a policy that learns to react to its environment with limited experience.  Given that marketing teams will often segment customers into different user types (eg. power users, core users, casual users), we can perhaps train agents that can adapt to different user segments in short time frames.&lt;/p&gt;</content><author><name></name></author><category term="rl" /><category term="explainer" /><summary type="html">This post will explore meta reinforcement learning with only minimal math. We attempt to dive into the core concepts without making it too complicated.</summary></entry><entry><title type="html">Embracing the AI-First Paradigm</title><link href="https://derekchen14.github.io/2021/05/23/embracing-ai-first.html" rel="alternate" type="text/html" title="Embracing the AI-First Paradigm" /><published>2021-05-23T00:00:00-04:00</published><updated>2021-05-23T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/05/23/embracing-ai-first</id><content type="html" xml:base="https://derekchen14.github.io/2021/05/23/embracing-ai-first.html">&lt;p&gt;When the web first appeared, many folks adopted the new technology by porting over existing physical assets (ie. magazines and newspapers) to appear on a website with almost no changes.  When smartphones began to take hold, those same people ported over their existing websites onto a smaller screen, again with no discernable changes.  Rather than embracing the new medium, people simply applied the content of the old onto the new.  In hindsight, it seems obvious that the builders and makers should have been developing for the new medium, but perhaps paradigm shifts aren’t so obvious when they are happening.  What lessons does that teach us about the shift into building AI-first companies?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;To figure out what we can learn from this, we start by really understanding what went wrong.  Then, we can identify opportunities to amend a possibly misguided mindset.  Next, we apply this framework onto exisiting projects which exhibit spurious objectives.  Finally, we conclude by highlighting the takeaway lessons for building an AI-first company.&lt;/p&gt;

&lt;h2 id=&quot;1-what-went-wrong&quot;&gt;1. What Went Wrong?&lt;/h2&gt;

&lt;p&gt;Early webmasters recognized that the Internet was going to be a big thing, but often failed to take advantage of the new medium.  Rather than creating new experiences, they created online versions of offline artifacts.  With today’s view, we know that rather than static pages of content, a website can be interactive with links and buttons.  We can make spreadsheets, dashboards, and scrolling menus.  Rather than focusing on what the web couldn’t do, we should have been focusing on what it &lt;em&gt;could&lt;/em&gt; do.&lt;/p&gt;

&lt;p&gt;Mobile developers faced the same hurdles.  Rather than opting for pinch and zoom, most took content on the web and simplified it for smaller screens.  There was a lack of imagination in using the GPS tracking, the real-time updates and built-in camera to build mobile first experiences.  Looking back, we see that the most successful companies of that era (AirBnB, Uber, Pinterest, Instagra, etc.) are precisely those that learned to take advantage of these new features.  It seems then that embracing the new is the clear solution.&lt;/p&gt;

&lt;h2 id=&quot;2-lessons-from-the-past&quot;&gt;2. Lessons from the Past&lt;/h2&gt;

&lt;p&gt;What makes it so hard to see things “inevitable” changes coming?  How could a developer be so right in betting on the web, yet be so mistaken in what they built?  A reasonable solution shouldn’t just recommend building with the advantages of the new medium, but should also acknowledge why the transition might be turbulent.  For many new technologies, there are people who jump onto the bandwagon simply because it is new and rising.  These people wrote blogs and put up largely static websites because it was the cool, new thing to do.  Does that explain the majority of the population though?  What about the brick and mortar stores moving their retail interfaces online?  What about the educators, evangelists and entertainment moguls building their empires?&lt;/p&gt;

&lt;p&gt;Most people building an online presence regarded the web as simply a new medium rather than a paradigm shift because they weren’t interested in paradigm shifts.  They were interested in selling whatever it was they already provided and so the web or mobile web was just another way to do what they had already been doing.  Even after it became clear that the new tech needed new ways of thinking, many people simply learned the best practices and left it at that.  These folks are not concerned with ushering in new wave of technology.  And who can blame them, they’ve got their day jobs to worry about!&lt;/p&gt;

&lt;p&gt;The early majority and late majority need to be convinced of the new wave and also taught the appropriate methods of using the new technology.  Google had to build SEO, PPC, and web analytics platforms for people to learn how to build in their garden.  Apple and Android had to offer SDKs and other online training for mobile developers to build in their walled garden.  Both had to showcase examples of good websites and good mobile apps, respectively.&lt;/p&gt;

&lt;p&gt;Additionally, we’ve also noted that the innovators will use new technology for the sake of new tech.  These folks are not so much ahead of the curve as they are just interested in playing with the latest shiny object.  This is why looking at their behaviors will lead to many false prophets (AR/VR, early tablets, autonomous driving, etc.)  The shift must occur then with the early adopters, who must have demanded that the builders create something that takes advantage of the technology or risk becoming another over-hyped flop.&lt;/p&gt;

&lt;h2 id=&quot;3-applications-to-the-present&quot;&gt;3. Applications to the Present&lt;/h2&gt;

&lt;p&gt;Chatbots today are still in the early stages where there are certainly some commercial successes (Alexa, Siri, Google Now), but certainly not the world-changing revolution that was promised around 2016.  The innovators have participated, but it will take a change in how we build these dialogue systems to attract the early adopter crowd.  Luckily, the errors in applying Conversationl AI seem to follow same patterns of omission, so the issue should be straightforward to diagnose.&lt;/p&gt;

&lt;p&gt;The first versions of task-oriented chatbots will likely operate with existing user interaction paradigms until we learn how to take advantage of the unique aspects of chatbot technology.  Consequently, what we see today are agent-assisted systems that help users navigate websites.   We see dialogue systems that take a mobile experience and modify it for chat by changing each mobile API call into a voice-based API call.  Everything is coded up as a bunch of if/then statements because that’s how we build our applications today.&lt;/p&gt;

&lt;p&gt;It doesn’t have to be this way of course.&lt;/p&gt;

&lt;p&gt;Intelligent virtual assistants should be a wholly &lt;em&gt;new&lt;/em&gt; method for interacting with technology and information.  We should be able to speak with mostly natural language and get back a reasonable response rather than spitting out directions one line at a time.  We should be able to ask for what we actually want (ie. book a hotel, shop for a pair of dress shoes) rather than using the virtual agent as a voice-activated navigation tool for a mobile app that offers travel reservations or sells shoes.  We should be able to recover from misunderstandings by clarifying what we want in a way that pushes the conversation forward, rather than screaming at a bot that can’t seem to comprehend that it just made a mistake.  In short, we should be engaging in conversations over commands.&lt;/p&gt;

&lt;h2 id=&quot;4-ai-first-companies&quot;&gt;4. AI-First Companies&lt;/h2&gt;

&lt;p&gt;An AI-first company (and really we’re referring to converstional AI companies) should build dialogue agents that understand the new medium that they represent.  It should be reliable enough to handle the variations found in real-life conversation, which means the agent should be trained in a way that can adapt to changing circumstances.  Training a flexible agent means being more innovative about where the training data comes from, how the training data is collected and what kind of data is annotated.  How the data aspect should be managed though is a subject for another day …&lt;/p&gt;</content><author><name></name></author><category term="ai" /><category term="startups" /><category term="trends" /><summary type="html">When the web first appeared, many folks adopted the new technology by porting over existing physical assets (ie. magazines and newspapers) to appear on a website with almost no changes. When smartphones began to take hold, those same people ported over their existing websites onto a smaller screen, again with no discernable changes. Rather than embracing the new medium, people simply applied the content of the old onto the new. In hindsight, it seems obvious that the builders and makers should have been developing for the new medium, but perhaps paradigm shifts aren’t so obvious when they are happening. What lessons does that teach us about the shift into building AI-first companies?</summary></entry><entry><title type="html">2020 Year End Review (Part 2)</title><link href="https://derekchen14.github.io/2020/12/30/year-end-review-2020.html" rel="alternate" type="text/html" title="2020 Year End Review (Part 2)" /><published>2020-12-30T00:00:00-05:00</published><updated>2020-12-30T00:00:00-05:00</updated><id>https://derekchen14.github.io/2020/12/30/year-end-review-2020</id><content type="html" xml:base="https://derekchen14.github.io/2020/12/30/year-end-review-2020.html">&lt;p&gt;Continuing on the thoughts described in &lt;a href=&quot;https://morethanoneturn.com/2020/12/28/emnlp-2020-highlights.html&quot;&gt;Part 1&lt;/a&gt;, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data strategy is playing an increasingly important role in the development of modern machine learning models.  How that plays out in more detail will be discussed in the future, but today we will look further into the development of dialogue in the past few months as well as other observations.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;As a quick disclaimer, the sections titles are meant to be provocative, so any percieved slights are misguided since all papers mentioned represent significant progress in their own right.  Accordingly, no comments should be construed as minimizing any of the authors’ great work in any manner.&lt;/p&gt;

&lt;h2 id=&quot;3-stagnation-of-dialogue&quot;&gt;(3) Stagnation of Dialogue&lt;/h2&gt;

&lt;p&gt;Like all NLP conferences in the past few years, EMNLP 2020 contained its fair share of papers around Dialogue State Tracking. &lt;a href=&quot;https://arxiv.org/abs/2004.03386&quot;&gt;Schema Fusion Networks&lt;/a&gt; (Zhu, Li, Chen and Yu) aimed to improve DST by encoding past predictions into a schema graph and carrying over past knowledge into further predictions.  Some other ideas include &lt;a href=&quot;https://www.aclweb.org/anthology/2020.findings-emnlp.75/&quot;&gt;Actor-Double-Critic&lt;/a&gt; (Wu, Tseng, and Gasic), &lt;a href=&quot;https://www.aclweb.org/anthology/2020.findings-emnlp.95/&quot;&gt;GCDST&lt;/a&gt; (Wu, Zou, Jiang, and Aw), &lt;a href=&quot;https://arxiv.org/abs/2009.07615&quot;&gt;Temporally Expressive Networks&lt;/a&gt; (Chen, Zhang, Mao, Xu) and &lt;a href=&quot;aclweb.org/anthology/2020.emnlp-main.243/&quot;&gt;Slot Attention with Value Normalization&lt;/a&gt; (Wang, Guo, Zhu).  What all these papers have in common is the development of complex models to boost the final accuracy score.&lt;/p&gt;

&lt;p&gt;Thus, arguably the most impressive was DST paper was &lt;a href=&quot;https://arxiv.org/abs/2005.00796&quot;&gt;SimpleTOD&lt;/a&gt; (Hosseini-Asl et al.) which used the straightforward, but powerful idea of taking GPT-2 as the core model to sequentially generate the output dialogue states.  Properly designing the prompts to feed into the model seems trivial, but is actually quite insightful once you realize the exponential number of combinations possible to design a model input.  Furthermore, this simple setup yielded the best results we have seen on MultiWOZ&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; so far with 55.76 joint goal accuracy.&lt;/p&gt;

&lt;p&gt;While these methods are all impressive, it is starting to seem like the current MultiWOZ benchmark is starting to hit its limits since qualitative analysis will reveal that most model errors are actually due to errors in annotation.  As model performance starts to saturate, the natural progression is to move onto further benchmarks.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;  Thus, the short-comings of the original dataset have spurred on the creation of MultiWOZ 2.1&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, MultiWOZ 2.2&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; and MultiWOZ 2.3&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;These revisions fix a number of labeling issues, but there is also an underlying issue where it seems the community might be overfitting to a single dataset, which means only making incremental gains. To this extent, dialogue research has perhaps stagnated since we are so highly focused on the single task of multi-domain slot-filling.  Thus, it is perhaps time to have a new standard benchmark which retains the core components of semantic understanding, but also offers other aspects of conversation to study.  As some possible examples, we could explore task-oriented dialogues where the user is unsure of the options available to them, so the job of the agent is to elicit their preferences before recording them.  Another avenue is to look at dialogues where the agent is constrained in their actions by company policies, which reflect real-world scenarios.&lt;/p&gt;

&lt;p&gt;To be clear, other elements of dialogue, such as natural language generation are still seeing exciting movement, and this section will be wrapped up by highlighting three papers in this realm.  Building on the theme from the previous section, &lt;a href=&quot;https://arxiv.org/abs/2004.07462&quot;&gt;Paraphrase Augmented Task-Oriented Dialog Generation&lt;/a&gt; (Gao, Zhang, Ou, Yu) uses Data Augmentation techniques rather than model changes to improve the performance of Dialgoue State Tracking.  In particular, the authors devise a method for augmenting data through paraphrasing with templates.  They do so by taking advantage of the structure of the dialogue labels in the training set as opposed to just paraphrasing the raw text.&lt;/p&gt;

&lt;p&gt;Within &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.230/&quot;&gt;Make Neural Natural Language Generation as Reliable as Templates&lt;/a&gt; Elder, O’Connor, and Foster also use generation as a method for data augmentation.  Their idea is to find the balance between template systems (which may be too rigid) and neural systems (which are often uncontrollable) that allows for diversity while also maintaining reliability.  The key idea is to use regex and heuristics to identify desired surface forms that we would want a model to generate.  Then, we restrict the softmax output and generation capabilities (ie. beam search) to only be able to produce these surface forms and other minor function words.&lt;/p&gt;

&lt;p&gt;If we were to push the augmentation idea to the extreme, we would be training entire dialogue systems using generated data, which is precisely what a user simulator would provide.  In fact, &lt;a href=&quot;https://arxiv.org/abs/2011.08243&quot;&gt;Dialog Simulation with Realistic Variations&lt;/a&gt; from the Alexa Conversations at Amazon have done exactly that.  Although still not quite a simulator with realistic actions, the authors do add some interesting variations.  Starting with a M2M paraphrasing technique&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;, the group puts in extra effort to generate novel template flows.  Specifically, they allow for users to adjust their preferences during the chat and have some element of pushing the conversation along by pro-actively requesting the slot-values of API calls that have not yet been made.&lt;/p&gt;

&lt;h2 id=&quot;4-convergence-of-ideas&quot;&gt;(4) Convergence of Ideas&lt;/h2&gt;

&lt;p&gt;The more optimistic view of this point is that “great people think alike”.  The more pessimistic view of this point is that the lack of interesting datasets means there are only a few fertile areas left to look.  Perhaps a more realistic view is that similar ideas commonly arise from independent sources when many people are exploring the same frontier at once, and this is simply the first time I’ve noticed.  The bottom line is that I started to notice a number of duplicate papers stating basically the same core idea, both published at the same time at EMNLP.&lt;/p&gt;

&lt;p&gt;The first example of this is maybe the most obvious given the similarity of their titles. &lt;a href=&quot;https://arxiv.org/abs/2010.12770&quot;&gt;Conversational Semantic Parsing for DST&lt;/a&gt; from Apple and &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.408/&quot;&gt;Conversational Semantic Parsing&lt;/a&gt; from Facebook both examine their specific style of parsing dialogues.  I imagine that Amazon, Tencent and Google all have something similar as well.  For reference, Microsoft’s version of this is to view &lt;a href=&quot;https://arxiv.org/abs/2009.11423&quot;&gt;Task-Oriented Dialogue as Dataflow Synthesis&lt;/a&gt; (Semantic Machines et al.).&lt;/p&gt;

&lt;p&gt;More specifically, Apple’s version presents dialogues as Trees with segments that are chained together with periods.  They adopt a stack to represent multiple tasks in dialog history with pointers from certain branches to others.  Facebook’s version is quite similar in that it also contains a hierarchical structure where certain slot values are expanded into further detail.  However, as someone who has designed a dialogue meaning representation for production purposes as well, I simply cannot fathom how these representations can be quickly annotated by crowdsource workers.  The structure of these labels make many of them hard to even verify, much less annotate, with any reliable level of accuracy.  There are many very smart people who work there though, so it’s possible they are privy to some insight that I am not.&lt;/p&gt;

&lt;p&gt;The second set of similar papers share the high level goal of generating novel training data for semantic parsing in an semi-automated fashion.  The shared core idea is to generate the training data by conditioning query-based templates on the known database schema.  Then, the templates are filled and transformed into natural langauge through the use of deep learning models.  To avoid using augmented data that changes the semantics, they also apply the same pipeline for verification.  Namely, they start by parsing the natural language output back into the logical form.  Then, this SQL query is executed against the engine to return a result.  Finally, results failing to match the original, dynamically-selected KB attribute are filtered out – leaving only high quality generations for training.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.04806&quot;&gt;AutoQA&lt;/a&gt; (Xu, Semnani, Campagna, Lam) uses BART for paraphrasing the templates into natural language, whereas &lt;a href=&quot;https://arxiv.org/abs/2009.07396&quot;&gt;GAZP&lt;/a&gt; (Zhong, Lewis, Wang, and Zettlemoyer) uses BERT with BiLSTM components.  If we allow for changing the task to NER, then &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.590/&quot;&gt;Counterfactual Generator&lt;/a&gt; (Zeng, Li, Zhai, Zhang) can also be added to this set, since it also follows the same format of generating imperfect examples and then using the originally trained model to filter for the best data augmentations.  In all three cases, the training set that includes the augmentations achieves the best final performance.&lt;/p&gt;

&lt;p&gt;The final set of similar papers touches on the topic of improving model performance through the use of soft perturbations to generate new data samples.  More specifically, &lt;a href=&quot;https://arxiv.org/abs/2009.10195&quot;&gt;Self-Supervised Manifold Based Data Augmentation&lt;/a&gt; (Ng, Cho, and Ghassemi) perform data augmentation by masking out various tokens and then having BERT fill those tokens in, keeping the original label.  This may be error prone since you could drop key words that change the label, but for the tasks they chose, this was not a substantial concern.  In particular, they experiment with Sentiment Analysis, Natural Language Inference and Machine Translation.  During their experiments, they found that, using a BERT model to predict hard (aka. one-hot) labels led to roughly equal results.  Intuitively, this should be expected since it is simply feeding in more examples of things the model already knows.  The insight then was feeding in soft labels and training with KL-divergence, which then led to improved Out-of-domain performance.&lt;/p&gt;

&lt;p&gt;To see why this helps, we turn to &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.671/&quot;&gt;Towards More Accurate Uncertainty Estimation&lt;/a&gt; (He et al.) which employs a very similar insight.  In particular, the authors create perturbed inputs and labels using MixUp and also change their loss function to KL-divergence.  This creates labels are now also “soft” in that the new label is a distribution between the two mixed classes rather than discrete.  Similar to the original MixUp paper then&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;, the idea is that using the soft labels prevents the model from being too overly confident since during training it has seen outputs where the target was not necessarily completely towards a single label, but rather shared across multiple labels.  Overall, the idea that soft targets may be more robust than hard targets is something worth further exploration!&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;In total, there are arguably fewer mind-blowing model changes or huge performance gains within NLP as there might have been in years past.  In it’s place, we observe important and more nuanced advancements that highlight the maturity of the field.  I would argue that the shift towards more pragmatic data management concerns is actually good since it implies a move towards bringing these great NLP advancements into the real world.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1810.00278&quot;&gt;MultiWOZ&lt;/a&gt; (Budzianowski, et al.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1804.07461&quot;&gt;GLUE&lt;/a&gt; (Wang et al.) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1606.05250&quot;&gt;SQuAD&lt;/a&gt; (Rajpurkar et al.) &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1502.05698&quot;&gt;bAbI&lt;/a&gt; (Weston at al.) &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1907.01669&quot;&gt;MultiWOZ 2.1&lt;/a&gt; (Eric et al.) &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2007.12720&quot;&gt;MultiWOZ 2.2&lt;/a&gt; (Zang et al.) &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2010.05594&quot;&gt;MultiWOZ 2.3&lt;/a&gt; (Han and Liu et al.) &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1801.04871&quot;&gt;Building a Conversational Agent Overnight&lt;/a&gt; (Shah et al.) &lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1710.09412&quot;&gt;mixup: Beyond Empircal Risk Minimzation&lt;/a&gt; (Zhang et al.) &lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="conference" /><category term="trends" /><summary type="html">Continuing on the thoughts described in Part 1, we now describe two trends around dialogue and NLP research. In the previous post, we discussed how data strategy is playing an increasingly important role in the development of modern machine learning models. How that plays out in more detail will be discussed in the future, but today we will look further into the development of dialogue in the past few months as well as other observations.</summary></entry><entry><title type="html">EMNLP 2020 Highlights</title><link href="https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html" rel="alternate" type="text/html" title="EMNLP 2020 Highlights" /><published>2020-12-28T00:00:00-05:00</published><updated>2020-12-28T00:00:00-05:00</updated><id>https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights</id><content type="html" xml:base="https://derekchen14.github.io/2020/12/28/emnlp-2020-highlights.html">&lt;p&gt;This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review.  As this (crazy) year comes to a close, we take a moment to reflect on what has happened in the world of NLP and specifically in relation to Dialogue Modeling and Natural Language Understanding.  From my (limited) perspective, there are four trends which I noticed, including increased research into (1) data methods rather than models and (2) data efficiency techniques.  In the follow-up, we discuss observations around (3) limitations in dialogue state tracking and (4) convergence around similar ideas.&lt;/p&gt;

&lt;p&gt;Let’s dive into each one in more detail:&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;1-data-methods-over-models&quot;&gt;(1) Data Methods over Models&lt;/h2&gt;

&lt;p&gt;From the review of ACL earlier this year, we have already seen that the world belongs to Transformers and we’re just living in it.  For most tasks, some version of BERT (including RoBERTa), BART, GPT or T5 have become the de facto baselines, taking over BiLSTMs w/ Attention and RNNs before that.  As evidenced on their performance on &lt;a href=&quot;https://super.gluebenchmark.com/&quot;&gt;SuperGLUE&lt;/a&gt;, these models work suprisingly well across a wide range of tasks, and many traditional datasets are arguably “solved” in a loose sense of the word.  This has led to at least two major trends which highlight the importance of data and de-emphasize the efforts of designing a new model.&lt;/p&gt;

&lt;p&gt;When current models outperform their benchmarks, the typical order of the ecosystem naturally tilts the balance in the other direction.  Researchers start producing datasets that are increasingly harder until we are able to once again fool the models, reaching a new equilibrium.  However this time around, for certain areas of study, we have started to hit on a fundamental limitation where naively collecting data from crowdsource workers using traditional methods yields a task that is immediately solvable by the strong baselines.  In other words, we can’t make the tasks any harder because then the task becomes too hard for humans to reliably annotate correctly.&lt;/p&gt;

&lt;p&gt;So does this mean that ML models have achieved human level performance in understanding language?  Certainly not, since all models (neural or otherwise), still lack even rudimentary common sense or other basic reasoning skills that we expect from toddlers.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; The issue lies not with humans or models, but rather in how we design the task and the associated data collection techniques.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Consequently, we have started to see more folks thinking carefully about the issues we introduce during the data collection process.&lt;/p&gt;

&lt;p&gt;As a number of papers have noted, one of the issues plaguing modern models is their ability to perform so well out-of-the-box by exploiting human annotator biases.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;  For example, when writing mulitple choice answers, humans will typically choose a positive example as the correct answer.  As such, models are sometimes able to choose the correct answer, without even having read the corresponding question.  Accordingly, new techniques, such as adversarial filtering, have been developed to combat this.  In EMNLP, the authors take this a step further by offering &lt;a href=&quot;https://arxiv.org/abs/2009.10795&quot;&gt;Dataset Cartography&lt;/a&gt; (Swayamdipta et al.) as a model-based tool for characterizing and diagnosing these types of issues.&lt;/p&gt;

&lt;p&gt;In my experience, spending one or two days cleaning the data often has much better return than the same one or two days spent designing a new model architecture.  While this does not necessarily lead to novel advancements in the academic sense, it does yield notable gains in predictive performance that are quite meaningful in the practical sense.  Overall, it’s an exciting time to be building tools for automating data pre-processing and analyzing data quality.&lt;/p&gt;

&lt;h2 id=&quot;2-data-efficiency&quot;&gt;(2) Data Efficiency&lt;/h2&gt;

&lt;p&gt;The second data-related trend comes from recognizing that while large transformer-based models perform
shocking well in zero-shot settings, real-world scenarios have no qualms around using further data to continue to boost performance.  As a result, data collection for the end task remains relevant, and perhaps even more so as we realize the degree of impact that simply adding more data can have.  To this extent, there has been a noticeable growth in the amount of papers which care about data efficiency, which I will define as the desire to make the most efficient use of the limited amount of available, annotated data.&lt;/p&gt;

&lt;p&gt;In regards to data efficiency, large-scale LMs arguably fall under the category of self-supervision.  For dialogue in particular, authors Wu, Hoi, and Xiong suggest methods for &lt;a href=&quot;https://arxiv.org/abs/2010.13920&quot;&gt;Improving Limited Labeled DST with Self-Supervision&lt;/a&gt; by preserving latent consistency and modeling conversational behavior.  Whereas self-supervised models are often useful for fine-tuning on downstream tasks, meta-learned models perform this preparation explicitly.  &lt;a href=&quot;https://arxiv.org/abs/2010.02500&quot;&gt;Efficient Meta Lifelong-Learning with Limited Memory&lt;/a&gt; (Wang, Mehta, Póczos, Carbonell) tackles meta-learning of models to prevent catastrophic forgetting and negative transfer by designing a more efficient episodic memory component (MbPA++).&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Interestingly, one of the ways the authors make the memory component more efficient is by being more selective about which prior examples are placed into memory, thus maximizing the chance of matching with an item during inference.  In particular, they aim to maximize the diversity of stored examples (rather than uncertainty) in a way that is reminiscent of methods in active learning.  On the topic of active learning, &lt;a href=&quot;https://arxiv.org/abs/2010.09535&quot;&gt;Cold-start Active Learning&lt;/a&gt; (Yuan, Lin, Boyd-Graber) attempts to take the best of both pre-training (with BERT) and active learning to produce samples most conducive to learning on the end task of text classification.  Similar to BADGE&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;, the authors propose to embed all examples into a shared vector space in order to choose the next sample to label, however they argue the embeddings should instead be selected based on their surprisal factor rather than their gradients.&lt;/p&gt;

&lt;p&gt;One last method of using annotated data efficiently is to increase its impact through data augmentation methods.  Whereas there are some fairly simple techniques that can be applied in computer vision, naive data augmentation can often fail to perform well since swapping out even a single word can dramatically change the semantics of an utterance. Back translation is better since it is less likely to produce incoherent sentences, but does produce generic responses which are not as useful. Instead, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.emnlp-main.726/&quot;&gt;Data Boost&lt;/a&gt; (Liu et al.) trains a generator (based off GPT-2) to come up with new examples. Condition the generation of each token on the class, but train the model through Reinforcement Learning, where the RL reward is the salience (i.e. relevance) of each token to the class plus a KL-penalty to discourage movement outside the trust-region.&lt;/p&gt;

&lt;p&gt;The common insight from all data augmentation papers is rather than augment directly, instead generate new fake examples and include a filtering step to ensure quality.  To see how this theme recurs in dialogue, move onto the second half of the observations found in &lt;a href=&quot;https://morethanoneturn.com/2020/12/30/year-end-review-2020.html&quot;&gt;Part Two&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1612.06890&quot;&gt;CLEVR dataset&lt;/a&gt; (Johnson et al.) &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.aclweb.org/anthology/D19-1107/&quot;&gt;Are we modeling the task or the annotator?&lt;/a&gt; (Geva, Goldberg, Berant) &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2002.04108&quot;&gt;Adversarial Filters of Dataset Biases&lt;/a&gt; (Le Bras et al.) &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.01076&quot;&gt;Episodic Memory in Lifelong Language Learning&lt;/a&gt; (d’Autume, Ruder, Kong, Yogatama) &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.03671&quot;&gt;Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds&lt;/a&gt; (Ash et al.) &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="conference" /><category term="trends" /><summary type="html">This is nominally a review of trends from the EMNLP 2020, but also serves as the end of year review. As this (crazy) year comes to a close, we take a moment to reflect on what has happened in the world of NLP and specifically in relation to Dialogue Modeling and Natural Language Understanding. From my (limited) perspective, there are four trends which I noticed, including increased research into (1) data methods rather than models and (2) data efficiency techniques. In the follow-up, we discuss observations around (3) limitations in dialogue state tracking and (4) convergence around similar ideas. Let’s dive into each one in more detail:</summary></entry><entry><title type="html">Measuring Uncertainty</title><link href="https://derekchen14.github.io/2020/07/29/measuring-uncertainty.html" rel="alternate" type="text/html" title="Measuring Uncertainty" /><published>2020-07-29T20:06:00-04:00</published><updated>2020-07-29T20:06:00-04:00</updated><id>https://derekchen14.github.io/2020/07/29/measuring-uncertainty</id><content type="html" xml:base="https://derekchen14.github.io/2020/07/29/measuring-uncertainty.html">&lt;p&gt;Compared to typical goal-oriented dialogue systems, interactive dialogue agents not only aim to solve the task at hand, but also engage with the user by asking questions.  Questions can be used to push the conversation forward or to spark some new ideas, but for now we will focus on the use of questions to clarify understanding.  &lt;a href=&quot;https://www.researchgate.net/profile/Matthew_Purver/publication/236273309_The_Theory_and_Use_of_Clarification_Requests_in_Dialogue/links/00b7d5313817a20f30000000/The-Theory-and-Use-of-Clarification-Requests-in-Dialogue.pdf&quot;&gt;Clarification requests&lt;/a&gt;, as they are referred to in the academic literature, come in many forms, but the key issues to solve are when to ask such questions and in what format.  Asking questions too often, or at inappropriate times, causes the conversation to feel disjointed or annoying.  Asking the wrong type of question causes the dialogue agent to seem incoherent or useless.&lt;/p&gt;

&lt;p&gt;Recognizing &lt;a href=&quot;/deciding-when-to-ask-questions-for-dialogue/&quot;&gt;when to ask questions&lt;/a&gt; and what questions to ask can be tackled by having a NLU module which has a interpretable and &lt;a href=&quot;https://papers.nips.cc/paper/5658-calibrated-structured-prediction.pdf&quot;&gt;well-calibrated&lt;/a&gt; measure of uncertainty, often expressed as a confidence score.  If the score is low, then the model is uncertain and should ask a clarification question (even if model is unable to generate questions, it should at least abstain from offering any solutions).  If the score is higher, then the model is more certain and can ask a different set of questions.  Once the score is past a certain threshold, we can deem the model to be confident enough to formulate a reasonable recommendation.&lt;/p&gt;

&lt;p&gt;As we study the landscape of options for measuring uncertainty, there seem to be four broad methods of generating confidence scores.  Let’s examine each one in detail.&lt;/p&gt;

&lt;h3 id=&quot;1-posterior-probabilities&quot;&gt;(1) Posterior Probabilities&lt;/h3&gt;

&lt;p&gt;The most straightforward manner of measuring the model’s uncertainty is to &lt;a href=&quot;https://arxiv.org/abs/1805.04604&quot;&gt;ask the model itself&lt;/a&gt;. Namely, a typical classification or ranking problem will have a softmax at the end which represents the \(p(y \mid x)\) .&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;1A) &lt;a href=&quot;https://arxiv.org/pdf/2006.09462&quot;&gt;Max Item&lt;/a&gt;&lt;br /&gt;
If the max output of &lt;a href=&quot;https://arxiv.org/abs/1610.02136&quot;&gt;softmax is below some threshold&lt;/a&gt;, then mark the model as uncertain. However, numerous papers have noted that the pure softmax is &lt;a href=&quot;https://arxiv.org/abs/1701.06548&quot;&gt;largely uncalibrated&lt;/a&gt; and tends to make the model overconfident due to the exponeniating factor. Thus, the uncalibrated logits tend to work better. There are various tricks, &lt;a href=&quot;https://arxiv.org/abs/1706.04599&quot;&gt;like working with temperature&lt;/a&gt; to improve the calibration, but ultimately, you will still be looking at the likelihood of the model itself.&lt;/p&gt;

&lt;p&gt;1B) Top-K Rank&lt;br /&gt;
Rather than depending on just the top item, we can possibly glean some information from the other predictions. For example, we can look at the gap between the confidence score of top two items. If the gap is below some threshold, this indicates low confidence, so we mark the model as uncertain. We can also think of the ratio between the top items instead. If we generalize this further, we can look at the gap between the second and third ranked item or the first and third item. In total, &lt;a href=&quot;https://arxiv.org/abs/1805.04604&quot;&gt;we can expand this to arbitrary K&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;1C) Full distribution&lt;br /&gt;
If we look at the gap between certain items, the signal here is probably diminishing at a certain point. However, looking at the &lt;a href=&quot;https://arxiv.org/abs/2005.07174&quot;&gt;overall entropy&lt;/a&gt; of the entire distribution can tell us something. Along those lines, we might also want to check the total variance of the softmax/logits.&lt;/p&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h3 id=&quot;2-ensembles&quot;&gt;(2) Ensembles&lt;/h3&gt;

&lt;p&gt;On the topic of variance, ensembles are a method for inducing variance upon our model.  Essentially, we perturb the model such that it produces different outputs, and if there is high variance in the outputs, then our model is considered less certain.  The intuition is that a more confident model will still make the same prediction even if the input has been slightly shifted since the latent label has not changed.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;2A) Post-Training&lt;br /&gt;
The most common form of ensemble is one created by &lt;a href=&quot;http://www.jmlr.org/proceedings/papers/v48/gal16.pdf&quot;&gt;Gal and Ghahramani&lt;/a&gt;. In this paper, they propose MC-dropout where random masks are placed on model to simulate dropout at &lt;a href=&quot;https://arxiv.org/pdf/2006.09462&quot;&gt;&lt;em&gt;test&lt;/em&gt; time&lt;/a&gt;. This brilliant insight allows us to gain a measure of uncertainty without any extra heavy lifting to come up with a new model. Alternatively, any sort of perturbation, such as gaussian noise or brown noise can be added to the model to see its reaction.&lt;/p&gt;

&lt;p&gt;2B) Pre-Training&lt;br /&gt;
Of course, &lt;a href=&quot;http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf&quot;&gt;straightforward ensembles&lt;/a&gt; can also give a sense of the variance of the outputs. This turns out to work better, but at the cost of having to train &lt;em&gt;M&lt;/em&gt; models for a better measure of uncertainty. If we are doing all the extra work of training extra models, we don’t even have to really restrict ourselves to the same architecture. We could theoretically try M=5 different models and go off the assumption that confident prediction should hold true across all the architectures.&lt;/p&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h3 id=&quot;3-outlier-detection&quot;&gt;(3) Outlier Detection&lt;/h3&gt;

&lt;p&gt;Another method of predicting uncertainty is to look at other sources of uncertainty.  The previous methods roughly falls under the bucket of epistemic uncertainty since they examine how the model performs.  Alternatively, we can also study the data for aleatoric uncertainty.  This falls under the assumption that the data varies due to natural variation in the system, but should not vary beyond some reasonable bounds.  (If this interpretation is totally wrong, feel free to comment below.)&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;3A) Input Outliers&lt;br /&gt;
Although this is a false dichotomy, we can somewhat split up the data based on inputs and outputs. By inputs, we mean looking at the distribution of the data before it is passed into the model. For example, we might look at the n-gram distribution of a sentence and compare that to the n-grams of the average sentence in the training corpus. Additionally, we could &lt;a href=&quot;https://arxiv.org/abs/1805.04604&quot;&gt;pass an utterance into a Language Model&lt;/a&gt; to get a sense of its perplexity. In both cases, utterances that are “not typical” could be considered more likely to be uncertain, and dealt with accordingly.&lt;/p&gt;

&lt;p&gt;Perhaps looking at a datapoint before it is processed is too biased. So as a practical matter, we might say that instead, the pre-computed statistic simply gives us a &lt;a href=&quot;https://arxiv.org/abs/2002.07965&quot;&gt;Bayesian prior&lt;/a&gt;, after which we can use any of the other methods described to get us a better sense of the posterior likelihood of uncertainty. Assuming we are working with sentences, we can examine the prior uncertainty of either the tokens or the sentence as a whole.&lt;/p&gt;

&lt;p&gt;3B) Output Outliers&lt;br /&gt;
Moving on to processed data, we can imagine passing the datapoint through our main model to make predictions. If the predicted class itself is rare, this can be a warning signal to &lt;a href=&quot;https://arxiv.org/abs/2006.09462&quot;&gt;possibly abstain&lt;/a&gt;. Of course rare classes do occassionally appear, or otherwise they shouldn’t even be considered, so this method shouldn’t be taken too far. We could also imagine embedding the data using any embedding algorithm and then clustering the results. Any &lt;a href=&quot;https://arxiv.org/abs/1803.04765&quot;&gt;embeddings that are not near any known centroids&lt;/a&gt; can then be flagged for review. In this sense, any tool that gives a sense of outlier detection can be used as a measure of uncertainty.&lt;/p&gt;

&lt;!--kg-card-end: markdown--&gt;
&lt;h3 id=&quot;4-second-order-methods&quot;&gt;(4) Second-order Methods&lt;/h3&gt;

&lt;p&gt;Finally, we can consider second-order methods where a separate model makes a prediction of the uncertainty.   Training a model to do the heavy lifting for us is actually quite ingenious, but perhaps the papers don’t think of it this way, so they ironically don’t aggrandize this direction as much as some of the papers which proposed the other methods.&lt;/p&gt;

&lt;!--kg-card-begin: markdown--&gt;

&lt;p&gt;4A) Reinforcement Learning&lt;br /&gt;
Suppose we want to train a dialogue agent or semantic parser to ask a clarification question when it is uncertain of the situation. Using an RL system, we can instead &lt;a href=&quot;https://arxiv.org/abs/1911.03598&quot;&gt;have a policy manager decide&lt;/a&gt; when to take such an action, and simply train it with a reward signal. The &lt;a href=&quot;https://www.aaai.org/ojs/index.php/AAAI/article/download/4101/3979&quot;&gt;REINFORCE algorithm can be used&lt;/a&gt;, but certainly the whole back of tricks with RL can be used now as long as we can design some intermediate rewards for the model to follow.&lt;/p&gt;

&lt;p&gt;This ends up working quite well for certain problems, but my instinct says that it doesn’t work on real-world use cases for all the same reasons why reinforcement learning often fails outside of the simulated scenarios. We could have a model that tries to recognize the domain shift from the simulated enviroment to the real world, but then we start back at square one with some inception style behavior.&lt;/p&gt;

&lt;p&gt;4B) Direct Prediction&lt;br /&gt;
If the training data included a not-applicable or &lt;a href=&quot;https://arxiv.org/abs/2004.01926&quot;&gt;none-of-the-above&lt;/a&gt; option, then we can possibly get a model to choose that when none of the given options fit nicely. The hard part here is that we would need to know the ground truth of when to choose this option, which pretty much never happens unless we perform data augmentation. This gives the original model a chance to decide for itself that it is uncertain, but often does not work that well, especially on out-of-domain predictions.&lt;/p&gt;

&lt;p&gt;An interesting line of work might be to handcraft options that are known to be unclear and to also have training examples that can clearly be answered. Efforts such as &lt;a href=&quot;https://arxiv.org/abs/2004.10645&quot;&gt;AmbigQA&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1907.06554&quot;&gt;Qulac&lt;/a&gt; are in the right direction, but fail to cover the nuance needed to really understand that a topic is unclear. Oftentimes, what might be obvious to one person requires clarification from another person, so the entire dataset is a bit subjective and thus hard to generate at scale. Given the current techniques around crowdsourcing, this is a limitation without any clear solution at the moment.&lt;/p&gt;

&lt;p&gt;4C) Predictions on Statistics&lt;br /&gt;
Saving the best for last, one of the most promising methods for generating a confidence score is to develop an &lt;a href=&quot;https://arxiv.org/abs/1610.02136&quot;&gt;uncertainty predictor&lt;/a&gt; based on dev set. Concretely, we can pre-train some model using the training data (D-train) to see how well it performs. Then because we know the ground truth (either because it was part of the original dataset or because we augmented it), we can say that whenever the original model made a mistake, &lt;a href=&quot;https://arxiv.org/abs/2004.01926&quot;&gt;the model should have been more uncertain on that training example&lt;/a&gt;. To prevent overfitting, we freeze the D-train model and pass in D-dev data into the model for training the predictor. Thus, we train a confidence estimator that can hopefully generalize to example from the D-test distribution.&lt;/p&gt;

&lt;!--kg-card-end: markdown--&gt;

&lt;p&gt;&lt;u&gt;Takeaways&lt;/u&gt;&lt;/p&gt;

&lt;p&gt;As we analyze all these methods of measuring uncertainty, one final thought is to consider how humans know to ask clarification questions.   How do or I know that something is unclear?  Is it just a feeling? My general thought is that we have a sense of outlier detection when asking questions, but this is only triggered when something is far outside the what we expect.  What we expect is measured likely by some external function, which implies some support for the methods under Category Four.  &lt;/p&gt;

&lt;p&gt;However, the key is that humans have this expectation, a &lt;a href=&quot;https://arxiv.org/abs/1902.08355&quot;&gt;theory of mind&lt;/a&gt; of the other individual and a general view of the world at large.   Building a comprehensive opinion of the world at large is intractable in the near future, but I remain optimistic that perhaps we can get a model to “fake it” well enough that general users either won’t notice or won’t care.&lt;/p&gt;</content><author><name></name></author><summary type="html">Compared to typical goal-oriented dialogue systems, interactive dialogue agents not only aim to solve the task at hand, but also engage with the user by asking questions.  Questions can be used to push the conversation forward or to spark some new ideas, but for now we will focus on the use of questions to clarify understanding.  Clarification requests, as they are referred to in the academic literature, come in many forms, but the key issues to solve are when to ask such questions and in what format.  Asking questions too often, or at inappropriate times, causes the conversation to feel disjointed or annoying.  Asking the wrong type of question causes the dialogue agent to seem incoherent or useless.</summary></entry></feed>