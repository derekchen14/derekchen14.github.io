<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>The Hype of Machine Learning</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css%3Fv=245ecc38c2.css" />

    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/index.html" />
    
    <meta property="og:site_name" content="More Than One Turn" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="The Hype of Machine Learning" />
    <meta property="og:description" content="Many folks in the tech industry are skeptical about the hype of machine learning, since the world has been promised to them before, but the reality hasn&#x27;t really panned out.  In fact this has happened so many times, there is a term for this in the machine learning community called" />
    <meta property="og:url" content="http://localhost:2368/the-hype-of-machine-learning/" />
    <meta property="article:published_time" content="2015-11-22T02:01:00.000Z" />
    <meta property="article:modified_time" content="2015-12-14T02:16:23.000Z" />
    <meta property="article:tag" content="machine-learning" />
    
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="The Hype of Machine Learning" />
    <meta name="twitter:description" content="Many folks in the tech industry are skeptical about the hype of machine learning, since the world has been promised to them before, but the reality hasn&#x27;t really panned out.  In fact this has happened so many times, there is a term for this in the machine learning community called" />
    <meta name="twitter:url" content="http://localhost:2368/the-hype-of-machine-learning/" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Derek Chen" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="machine-learning" />
    <meta name="twitter:site" content="@derekchen14" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "More Than One Turn",
        "url": "http://localhost:2368/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Derek Chen",
        "image": {
            "@type": "ImageObject",
            "url": "//www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&d=mm&r=x",
            "width": 250,
            "height": 250
        },
        "url": "http://localhost:2368/author/derek/",
        "sameAs": []
    },
    "headline": "The Hype of Machine Learning",
    "url": "http://localhost:2368/the-hype-of-machine-learning/",
    "datePublished": "2015-11-22T02:01:00.000Z",
    "dateModified": "2015-12-14T02:16:23.000Z",
    "keywords": "machine-learning",
    "description": "Many folks in the tech industry are skeptical\n[http://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html] \nabout the hype of machine learning, since the world has been promised to them\nbefore, but the reality hasn&#x27;t really panned out. In fact this has happened so\nmany times, there is a term for this in the machine learning community called an \nAI Winter [https://en.wikipedia.org/wiki/AI_winter]. A number of fundamental\nfactors are different though in 2015 that warrant ",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.29" />
    <link rel="alternate" type="application/rss+xml" title="More Than One Turn" href="../rss/index.html" />
    <style>
.page-title,
.page-description {
   color:#3D0F90!important;
   text-shadow: #F7FAEB 0px 0px 0.3em;
}
a { color: #3399ff; }  
.nav li a:after { display: none; }
.nav li:before { display: none; }
li { margin: 0.2em 0; }
ol, ul {
	margin: 0;
    position: relative;
}
ol > li, ul > li { bottom: 0; }
p { margin: 0 0 0.7em 0; }
.main-header { height: 90vh; }

article.post { width: 100%; }
section.post-full-content {
    padding: 0;
    line-height: 1em;
    font-size: 1.4em;
}
h1.site-title, h2.site-description { 
    text-shadow: 2px 2px 3px #232522;
} 
    
</style>

</head>
<body class="post-template tag-machine-learning">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="../index.html">More Than One Turn</a>
            <div class="site-nav-content">
                    <ul class="nav">
    <li class="nav-home"><a href="../index.html">Home</a></li>
    <li class="nav-about"><a href="http://feature.engineering/about">About</a></li>
    <li class="nav-contact"><a href="http://feature.engineering/contact">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">The Hype of Machine Learning</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/derekchen14" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://feedly.com/i/subscription/feed/http://localhost:2368/rss/" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post tag-machine-learning no-image no-image">

            <header class="post-full-header">

                <section class="post-full-tags">
                    <a href="../tag/machine-learning/index.html">machine-learning</a>
                </section>

                <h1 class="post-full-title">The Hype of Machine Learning</h1>


                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                                    <div class="author-info">
                                        <h2>Derek Chen</h2>
                                        <p>Read <a href="../author/derek/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/derek/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/derek/index.html">Derek Chen</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2015-11-22">22 Nov 2015</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 3 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>


            <section class="post-full-content">
                <div class="post-content">
                    <!--kg-card-begin: markdown--><p>Many folks in the tech industry are <a href="http://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html">skeptical</a> about the hype of machine learning, since the world has been promised to them before, but the reality hasn't really panned out.  In fact this has happened so many times, there is a term for this in the machine learning community called an <a href="https://en.wikipedia.org/wiki/AI_winter" title="AI Winter">AI Winter</a>. A number of fundamental factors are different though in 2015 that warrant taking a closer look this time around.</p>
<h3 id="whymachinelearningistherealdeal">Why Machine Learning is the Real Deal###</h3>
<ul>
<li><strong>Larger data sets</strong> - The explosion of big data is probably the number one driver of the new effectiveness of machine learning. It turns out that using the exact same techniques, more data can yield significantly better predictions.  This new data comes from many sources:
<ul>
<li><em>Data collection</em> - this is the obvious one, we now have more devices like smartwatches, smartphones, and computers generating more data.  More analytics packages and more people spending their time in a digital world</li>
<li><em>Data labeling</em> - gathering more data through unsupervised learning methods first, such as auto-encoders</li>
<li><em>Feature engineering</em> - creating new training data through skewing, translations, rotations, and other transformations</li>
</ul>
</li>
<li><strong>Improved Hardware</strong> - Following Moore's Law, the processing power of chips and other hardware components continue to steadily improve over time, which simply leads to faster training.
<ul>
<li><em>GPU vs CPU vs FGPA</em> - the emergence of <a href="http://www.nvidia.com/object/machine-learning.html">GPUs</a> for training machine learning algorithms has given a huge boost in speed in calculating matrix operations.  While a GPU can't do many types of operations, it can focus on doing one operation really, really well.  Same is true with using <a href="https://gigaom.com/2014/08/14/researchers-hope-deep-learning-algorithms-can-run-on-fpgas-and-supercomputers/">FPGAs</a> for training neural networks.</li>
<li><em>Better Processors</em> - The chips themselves are just faster and more powerful over time.  Although, there is some news that rate of improvement is not as fast anymore, things are still <a href="http://venturebeat.com/2015/11/19/intel-says-moores-law-isnt-dead-yet/">getting better</a>.</li>
</ul>
</li>
<li><strong>Better Algorithms</strong> - Lastly, there have also been advances in how to train these machines to learn.  While this third component is what academic researchers and the media often like to focus on, it is worth stating again that the first two are both equally important advances.  Examples include:
<ul>
<li><em>Neural networks</em> - whereas most functions of the past used some combination of statistics (<a href="https://en.wikipedia.org/wiki/Probabilistic_classification">Naive Bayes</a>), data transformations (kernels in <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a>), or increasingly sophisticated ensembling techniques (<a href="https://en.wikipedia.org/wiki/Ensemble_learning">Random Forests</a>), neural networks model the problem from a totally different perspective.  By mimicking how the human brain functions, <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a> are able to achieve a surprising level of complexity without losing the flexibility to adapt to unique domains.</li>
<li><em>Network architecture</em> - as researchers continued to explore neural networks, they began adding significantly more layers to the model, giving rise to <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a>. Not only were there more layers though, sometimes these layers performed different functions, such as activation, max-pooling, or convolutions.  Finally, sometimes the network also grew larger in the front due to pre-training layers using unsupervised methods, or in the end due to extra recurrent layers from RNNs.</li>
<li><em>Sum is greater than the parts</em> - lots of other little wins that incrementally are not worth much, but combined have drastically improved the accuracy of machine learning.  Without going into too much detail, some noteworthy advances include improved initialization methods (<a href="http://localhost:2368/the-hype-of-machine-learning/yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Lecun</a>, <a href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot</a>, <a href="http://arxiv.org/abs/1502.01852.">He</a>) to start off training and improved learning rates (<a href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp</a>, <a href="http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf">AdaGrad</a>, <a href="http://arxiv.org/pdf/1412.6980.pdf">Adam</a>) to speed up the process.  And let's not forget improved activation functions (<a href="http://ufldl.stanford.edu/wiki/index.php/Neural_Networks">Sigmoid</a>, <a href="http://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function">tanH</a>, <a href="https://imiloainf.wordpress.com/2013/11/06/rectifier-nonlinearities/">ReLU</a>) to avoid neuron saturation and regularization techniques (<a href="http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf">lasso</a>, <a href="https://en.wikipedia.org/wiki/Tikhonov_regularization">ridge</a>, <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout</a>) to avoid overfitting on training data.</li>
</ul>
</li>
</ul>
<h3 id="examplesofimprovedmachinelearning">Examples of Improved Machine Learning###</h3>
<p>Frankly, words can't do justice in explaining the amazing advances that have been made in the last year â€“ so just watch.</p>
<ul>
<li><em>Image Recognition</em> - 5 years ago, computers could not tell the difference between an image of cat and an image of dog.  Now, they can interpret objects better than most people: <iframe width="560" height="315" src="https://www.youtube.com/embed/40riCqvRoMs" frameborder="0" allowfullscreen></iframe></li>
<li><em>NLP and Translation</em> - Not too long ago, computers could barely understand the words you were speaking even if spoken slowly from a limited vocabulary.  Now, they can translate entire sentences on the fly: <iframe width="420" height="315" src="https://www.youtube.com/embed/Nu-nlQqFCKg" frameborder="0" allowfullscreen></iframe></li>
<li><em>Memory Networks</em> - Not too long ago, computers could not really interpret any meaning or context from data.  Now, they can chain together sentences to <a href="http://www.techinsider.io/facebook-lord-of-the-rings-teach-memory-network-2015-11">reason through logic</a>.</li>
</ul>
<p>It's no surprise then that <a href="https://code.facebook.com/posts/1478523512478471/teaching-machines-to-see-and-understand-advances-in-ai-research/">all</a> <a href="http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html">the</a> <a href="http://blogs.microsoft.com/next/2015/11/11/happy-sad-angry-this-microsoft-tool-recognizes-emotions-in-pictures/">big</a> <a href="http://blogs.wsj.com/digits/2015/08/26/amazon-com-offers-machine-learning-platform-hires-scientists/">names</a> are joining in on the fun.</p>
<p><strong>Conclusion</strong></p>
<p>To be clear, we aren't in this wonderful new world yet. The majority of machine learning news you hear these days is simply about incremental improvements in data science that allow a researcher to better predict outcome Y based on features X<sub>1</sub>, X<sub>2</sub>, and X<sub>3</sub>. But we are quickly moving into a reality where machines will simply understand our desires and cater to those personal interests with little to no prompting.  And because <a href="https://www.ted.com/talks/ray_kurzweil_on_how_technology_will_transform_us?language=en">technology improves exponentially</a>, the age of machine learning and artificial intelligence is arriving much sooner than you may have been led to believe.</p>
<!--kg-card-end: markdown-->
                </div>
            </section>



        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">

                <article class="post-card post no-image no-image">


    <div class="post-card-content">

        <a class="post-card-content-link" href="../getting-started-with-machine-learning-in-python/index.html">

            <header class="post-card-header">
                <h2 class="post-card-title">Getting Started with Machine Learning in Python</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>Recently, you've read numerous HN articles telling you machine learning is the future. You've heard that Google uses it for search, Facebook uses it to detect faces, and Netflix uses it for recommendations.  Your interest is sufficiently piqued.  But you might still be left</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Derek Chen
                    </div>
            
                    <a href="../author/derek/index.html" class="static-avatar">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/derek/index.html">Derek Chen</a></span>
                <span class="post-card-byline-date"><time datetime="2015-12-26">26 Dec 2015</time> <span class="bull">&bull;</span> 5 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post no-image no-image">


    <div class="post-card-content">

        <a class="post-card-content-link" href="../contact/index.html">

            <header class="post-card-header">
                <h2 class="post-card-title">Contact</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>Email me at derekchen 14 @ gmail.</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Derek Chen
                    </div>
            
                    <a href="../author/derek/index.html" class="static-avatar">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/derek/index.html">Derek Chen</a></span>
                <span class="post-card-byline-date"><time datetime="2015-10-23">23 Oct 2015</time> <span class="bull">&bull;</span> 1 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../index.html">More Than One Turn</a> &copy; 2020</section>
                <nav class="site-footer-nav">
                    <a href="../index.html">Latest Posts</a>
                    
                    <a href="https://twitter.com/derekchen14" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js%3Fv=245ecc38c2"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
