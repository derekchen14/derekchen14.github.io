<!DOCTYPE html>
<html lang="en">
<head>

    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <title>Speech Recognition and Synthesis at Human Parity</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <link rel="stylesheet" type="text/css" href="../assets/built/screen.css%3Fv=245ecc38c2.css" />

    <link rel="canonical" href="index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    <link rel="amphtml" href="amp/index.html" />
    
    <meta property="og:site_name" content="More Than One Turn" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Speech Recognition and Synthesis at Human Parity" />
    <meta property="og:description" content="Microsoft Research dominates Speech-to-Text Achieving Human Parity in Conversational Speech Recognition (W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig) [Published: Oct 2016]  Microsoft Research has made a number of advancements in order to achieve this momentous accomplishment. First, they measured the human" />
    <meta property="og:url" content="http://localhost:2368/speech-recognition-and-synthesis-at-human-parity-2/" />
    <meta property="article:published_time" content="2016-11-17T17:12:12.000Z" />
    <meta property="article:modified_time" content="2016-11-17T20:45:53.000Z" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Speech Recognition and Synthesis at Human Parity" />
    <meta name="twitter:description" content="Microsoft Research dominates Speech-to-Text Achieving Human Parity in Conversational Speech Recognition (W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig) [Published: Oct 2016]  Microsoft Research has made a number of advancements in order to achieve this momentous accomplishment. First, they measured the human" />
    <meta name="twitter:url" content="http://localhost:2368/speech-recognition-and-synthesis-at-human-parity-2/" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Derek Chen" />
    <meta name="twitter:site" content="@derekchen14" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "More Than One Turn",
        "url": "http://localhost:2368/",
        "logo": {
            "@type": "ImageObject",
            "url": "http://localhost:2368/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "Derek Chen",
        "image": {
            "@type": "ImageObject",
            "url": "//www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&d=mm&r=x",
            "width": 250,
            "height": 250
        },
        "url": "http://localhost:2368/author/derek/",
        "sameAs": []
    },
    "headline": "Speech Recognition and Synthesis at Human Parity",
    "url": "http://localhost:2368/speech-recognition-and-synthesis-at-human-parity-2/",
    "datePublished": "2016-11-17T17:12:12.000Z",
    "dateModified": "2016-11-17T20:45:53.000Z",
    "description": "Microsoft Research dominates Speech-to-Text\nAchieving Human Parity in Conversational Speech Recognition\n[https://arxiv.org/abs/1610.05256] (W. Xiong, J. Droppo, X. Huang, F. Seide, M.\nSeltzer, A. Stolcke, D. Yu, G. Zweig) [Published: Oct 2016]\n\nMicrosoft Research has made a number of advancements in order to achieve this\nmomentous accomplishment. First, they measured the human error rate on NIST’s\n2000 conversational telephone speech recognition task, which consists of two\nparts: the Switchboard",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:2368/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.29" />
    <link rel="alternate" type="application/rss+xml" title="More Than One Turn" href="../rss/index.html" />
    <style>
.page-title,
.page-description {
   color:#3D0F90!important;
   text-shadow: #F7FAEB 0px 0px 0.3em;
}
a { color: #3399ff; }  
.nav li a:after { display: none; }
.nav li:before { display: none; }
li { margin: 0.2em 0; }
ol, ul {
	margin: 0;
    position: relative;
}
ol > li, ul > li { bottom: 0; }
p { margin: 0 0 0.7em 0; }
.main-header { height: 90vh; }

article.post { width: 100%; }
section.post-full-content {
    padding: 0;
    line-height: 1em;
    font-size: 1.4em;
}
h1.site-title, h2.site-description { 
    text-shadow: 2px 2px 3px #232522;
} 
    
</style>

</head>
<body class="post-template">

    <div class="site-wrapper">

        

<header class="site-header">
    <div class="outer site-nav-main">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left-wrapper">
        <div class="site-nav-left">
                <a class="site-nav-logo" href="../index.html">More Than One Turn</a>
            <div class="site-nav-content">
                    <ul class="nav">
    <li class="nav-home"><a href="../index.html">Home</a></li>
    <li class="nav-about"><a href="http://feature.engineering/about">About</a></li>
    <li class="nav-contact"><a href="http://feature.engineering/contact">Contact</a></li>
</ul>

                    <span class="nav-post-title dash">Speech Recognition and Synthesis at Human Parity</span>
            </div>
        </div>
    </div>
    <div class="site-nav-right">
            <div class="social-links">
                    <a class="social-link social-link-tw" href="https://twitter.com/derekchen14" title="Twitter" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            </div>
                <a class="rss-button" href="https://feedly.com/i/subscription/feed/http://localhost:2368/rss/" title="RSS" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><circle cx="6.18" cy="17.82" r="2.18"/><path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/></svg>
</a>

    </div>
</nav>
    </div>
</div></header>


<main id="site-main" class="site-main outer">
    <div class="inner">

        <article class="post-full post no-image no-image">

            <header class="post-full-header">


                <h1 class="post-full-title">Speech Recognition and Synthesis at Human Parity</h1>


                <div class="post-full-byline">

                    <section class="post-full-byline-content">

                        <ul class="author-list">
                            <li class="author-list-item">

                                <div class="author-card">
                                    <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                                    <div class="author-info">
                                        <h2>Derek Chen</h2>
                                        <p>Read <a href="../author/derek/index.html">more posts</a> by this author.</p>
                                    </div>
                                </div>

                                <a href="../author/derek/index.html" class="author-avatar">
                                    <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                                </a>

                            </li>
                        </ul>

                        <section class="post-full-byline-meta">
                            <h4 class="author-name"><a href="../author/derek/index.html">Derek Chen</a></h4>
                            <div class="byline-meta-content">
                                <time class="byline-meta-date" datetime="2016-11-17">17 Nov 2016</time>
                                <span class="byline-reading-time"><span class="bull">&bull;</span> 6 min read</span>
                            </div>
                        </section>

                    </section>


                </div>
            </header>


            <section class="post-full-content">
                <div class="post-content">
                    <!--kg-card-begin: markdown--><h4 id="microsoftresearchdominatesspeechtotext">Microsoft Research dominates Speech-to-Text</h4>
<p><a href="https://arxiv.org/abs/1610.05256"><strong>Achieving Human Parity in Conversational Speech Recognition</strong></a> (<em>W. Xiong, J. Droppo, X. Huang, F. Seide, M. Seltzer, A. Stolcke, D. Yu, G. Zweig</em>) [Published: Oct 2016]</p>
<p>Microsoft Research has made a number of advancements in order to achieve this momentous accomplishment. First, they measured the human error rate on NIST’s 2000 conversational telephone speech recognition task, which consists of two parts: the Switchboard<br>
and CallHome subsets, with 5.9% and 11.3% error rates respectively.  Significantly, they found a great deal of variability between the two datasets, which highlights the fact that it would be a misnomer to place a single number to represent &quot;human-level&quot; accuracy - it really depends on the dataset.  With that said, MS claims that for the first time, automatic recognition performance is on par with human performance on this task.</p>
<p>This was accomplished through a variety of SOTA tricks from deep learning. To start, a combination of triplet of CNNs are fused together for acoustic modeling, namely a (1) VGGNet for senone prediction (2) ResNet with ReLUs and batch norm (3) a LACE network.   What is a senone? Well, a senone is a set of tied triphone states, normally in a HMM, but this time in a CNN.  From <a href="http://cmusphinx.sourceforge.net/wiki/tutorialconcepts">CMU Sphinx</a>: Speech is a continuous audio stream where rather stable states mix with dynamically changed states. In this sequence of states, one can define more or less similar classes of sounds, or phones.<br>
Words are understood to be built of phones, but this is certainly not true. The acoustic properties of a waveform corresponding to a phone can vary greatly depending on many factors - phone context, speaker, style of speech and so on. The so called coarticulation makes phones sound very different from their “canonical” representation. Next, since transitions between words are more informative than stable regions, developers often talk about diphones - parts of phones between two consecutive phones. Sometimes developers talk about subphonetic units - different substates of a phone. Often three or more regions of a different nature can easily be found.<br>
The number three is easily explained. The first part of the phone depends on its preceding phone, the middle part is stable, and the next part depends on the subsequent phone. That's why there are often three states in a phone selected for HMM recognition.<br>
Sometimes phones are considered in context. There are triphones or even quinphones. But note that unlike phones and diphones, they are matched with the same range in waveform as just phones. They just differ by name. That's why we prefer to call this object senone. A senone's dependence on context could be more complex than just left and right context. It can be a rather complex function defined by a decision tree, or in some other way.&quot;<br>
What is a LACE? Well it stands for a layer-wise context expansion<br>
with attention model, but that doesn't help too much either. LACE is variation on time-delay neural networks where each higher layer studies the broad context, which &quot;expands&quot; the context of lower layers. Put another way, lower layers focus on extracting simple local patterns while higher layers extract complex patterns that cover broader contexts. Where Since not all frames in a window carry the same importance, an attention is applied to tell the network which window to focus on.</p>
<p>A whole bunch of LSTMs are used for acoustic and language modeling as well.  In order to extract all the data, many steps of processing are performed. We trained both standard, forward-predicting RNNLMs and backward RNN-LMs that predict words in<br>
reverse temporal order. The log probabilities from both<br>
models are added.  There is also some interpolation with N-grams.  Then some work is done to account for words that are out-of-domain of the training data. Some other techniques are used for scoring and rescoring the potential candidate terms.  To quote: &quot;Here we also used a two-phase training schedule to train the LSTM LMs. First we train the model on the combination of in-domain and out-domain data for four data passes without any learning rate adjustment. We then start from the resulting model and train on<br>
in-domain data until convergence.&quot;  As is now clear, speech recognition is a deep learning task beyond my understanding, but this is just too great a result to not report :P</p>
<p>One final note, while I don't think the &quot;human parity&quot; claim is marketing fluff, I do think the rebranding of CNTK to &quot;Cognitive Toolkit&quot; <em>is</em> PR from MS to combat the popularity of Torch (FB) and Tensorflow (GOOG).  Why is this worth mentioning?  Because they devoted an entire section in the paper to say how great their library is.  Anyway, great achievement for the team!</p>
<h4 id="googledeepminddominatestexttospeech">Google Deepmind dominates Text-to-Speech</h4>
<p><a href="https://arxiv.org/abs/1609.03499"><strong>Wavenet: a Generative Model for Raw Audio</strong></a><br>
(<em>A. Oord, K. Simonyan, N. Kalchbrenner, S. Dieleman, O. Vinyals, A. Senior, H. Zen, A. Graves, K. Kavukcuoglu</em>) [Published: Sept 2016]</p>
<p>Traditionally, raw audio waveforms are created using <a href="https://en.wikipedia.org/wiki/Concatenative_synthesis">Concatenative </a> methods or <a href="http://www.cstr.ed.ac.uk/downloads/publications/2010/king_hmm_tutorial.pdf">Statistical Parametric</a> methods.  For the former, there's just a giant library of sound snippets (likely phonemes) that are glued together to form each words and sentences.  For the later, the combinations and transitions from one sound bite to another are done a bit smarter using Hidden Markov Models and other statistical techniques. As of 2016, this represents the state-of-the-art.  Well, represented state-of-the-art until WaveNet came by and improved on <em>both</em> by over 50%!</p>
<p>WaveNet is a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones.  Human listeners rate the speech as significantly more natural sounding than the best parametric and concatenative systems for both English and Chinese.  This was supposed to be very hard because audio files used for training are now at 16,000 samples per second, which means too much data for most other models to handle.  But what works in PixelRNNs for generating images also works for generating waveforms, once the convolution filters are tweaked a bit for this new purpose.</p>
<p>At each timestep, it takes in the word it is trying to say, and the outputs samples at all previous timesteps to generate the current output.  But there are no timesteps since its a CNN, not a RNN! Ok true, but basically, the CNN &quot;looks back&quot; at previous timesteps by expanding the receptive field - higher layers have higher dilation leading to larger receptive fields.  A normal dilation will expand in both directions (forward and backward), so to prevent the CNN from looking ahead, the authors apply masks to form causal convolutions.  (Actually a dilation normally expands in four directions up, down, left, right, but the waveform is 2D, so it can only expand left and right.)  Since wave From what I can gather, this basically means applying a mask tensor on the convolution kernel through elementwise multiplication (that blocks signal from the &quot;future&quot;) before applying the kernel to the waveform at that layer.</p>
<p>The output of the model has the same time dimensionality as the input, which makes taking the output from one layer as the input of the next layer really easy (no deconvolutions necessary).  From the image though, it's clear each output comes from multiple inputs.  However, this is <em>not</em> achieved pooling layers in the network (as typical in CNNs).  Instead, in order to shrink down the dimensions as we move up the layers, the dilated convolutions are used to pad zeros between the filters of each layer. To quote: &quot;The intuition behind this configuration is two-fold. First, exponentially increasing the dilation factor results in exponential receptive field growth with depth (Yu &amp; Koltun, 2016). For example each 1, 2, 4, . . . , 512 block has receptive field of size 1024, and can be seen as a more efficient and dis- criminative (non-linear) counterpart of a 1×1024 convolution. Second, stacking these blocks further increases the model capacity and the receptive field size.&quot;</p>
<p><img src="http://localhost:2368/content/images/2016/11/wavenet.gif" alt="WaveNet"></p>
<p>Also interesting, by tweaking one of the inputs, WaveNet can even generate the same speech using different speakers, such as female or male, when conditioned on a speaker identity. Just for fun, it can also generate &quot;novel and often highly realistic musical fragments&quot;.  Finally, it might just be me, but do all the sound clips sound like they have some underlying static in the background?  Might it have to do with the <a href="http://distill.pub/2016/deconv-checkerboard/">checkerboard artifacts</a> in other generative models?</p>
<p>Perhaps most promising is the use of WaveNet as a discriminative model, since it generally points to the possibility of semi-supervised learning that takes advantage of unlabeled data.  Here, the output is chopped off, and replaced with a mean-pool followed by some more non-causal convs.  (Was there a softmax at the end?)  Applying this to TIMIT, the authors acheieved 18.8 PER on the test set, which is SOTA when trained on raw audio (as opposed to log mel-filterbank energies or mel-frequency cepstral coefficients.)  The <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">official blog post</a> is pretty easy to understand, and includes sound clips from the system.  Definitely worth a look.</p>
<!--kg-card-end: markdown-->
                </div>
            </section>



        </article>

    </div>
</main>

<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">

                <article class="post-card post no-image no-image">


    <div class="post-card-content">

        <a class="post-card-content-link" href="../deep-learning-for-recommendation-systems/index.html">

            <header class="post-card-header">
                <h2 class="post-card-title">Deep Learning for Recommendation Systems</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>Wide &amp; Deep Learning for Recommender Systems  (Cheng, Koc, Harmsen, Shaked, Chandra, Aradhye, Anderson, et. al.)  While deep learning is great for many things, it's progress in recommendations systems has been limited, partially because recommendation systems inherently have a cold-start problem and sparse data,</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Derek Chen
                    </div>
            
                    <a href="../author/derek/index.html" class="static-avatar">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/derek/index.html">Derek Chen</a></span>
                <span class="post-card-byline-date"><time datetime="2016-11-19">19 Nov 2016</time> <span class="bull">&bull;</span> 2 min read</span>
            </div>
        </footer>

    </div>

</article>

                <article class="post-card post no-image no-image">


    <div class="post-card-content">

        <a class="post-card-content-link" href="../training-a-neural-network/index.html">

            <header class="post-card-header">
                <h2 class="post-card-title">Training a Neural Network</h2>
            </header>

            <section class="post-card-excerpt">
                    <p>(aka. Living a Successful Life)  One way to look at training a neural network is that you are minimizing a cost function to increase accuracy.  This is done through gradient descent, not trying to calculate the hessian.  Because the overall search space is non-convex</p>
            </section>

        </a>

        <footer class="post-card-meta">
            <ul class="author-list">
                <li class="author-list-item">
            
                    <div class="author-name-tooltip">
                        Derek Chen
                    </div>
            
                    <a href="../author/derek/index.html" class="static-avatar">
                        <img class="author-profile-image" src="http://www.gravatar.com/avatar/9f3ee838eeb42849d4880676cedd85d6?s=250&amp;d=mm&amp;r=x" alt="Derek Chen" />
                    </a>
                </li>
            </ul>
            <div class="post-card-byline-content">
                <span><a href="../author/derek/index.html">Derek Chen</a></span>
                <span class="post-card-byline-date"><time datetime="2016-10-22">22 Oct 2016</time> <span class="bull">&bull;</span> 6 min read</span>
            </div>
        </footer>

    </div>

</article>
        </div>
    </div>
</aside>




        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="../index.html">More Than One Turn</a> &copy; 2020</section>
                <nav class="site-footer-nav">
                    <a href="../index.html">Latest Posts</a>
                    
                    <a href="https://twitter.com/derekchen14" target="_blank" rel="noopener">Twitter</a>
                    <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a>
                </nav>
            </div>
        </footer>

    </div>


    <script
        src="https://code.jquery.com/jquery-3.4.1.min.js"
        integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
        crossorigin="anonymous">
    </script>
    <script src="../assets/built/casper.js%3Fv=245ecc38c2"></script>

    <script>
        // Parse the URL parameter
        function getParameterByName(name, url) {
            if (!url) url = window.location.href;
            name = name.replace(/[\[\]]/g, "\\$&");
            var regex = new RegExp("[?&]" + name + "(=([^&#]*)|&|#|$)"),
                results = regex.exec(url);
            if (!results) return null;
            if (!results[2]) return '';
            return decodeURIComponent(results[2].replace(/\+/g, " "));
        }

        // Give the parameter a variable name
        var action = getParameterByName('action');

        $(document).ready(function () {
            if (action == 'subscribe') {
                $('body').addClass("subscribe-success");
            }

            $('.subscribe-success-message .subscribe-close').click(function () {
                $('.subscribe-success-message').addClass('close');
            });

            // Reset form on opening subscrion overlay
            $('.subscribe-button').click(function() {
                $('.subscribe-overlay form').removeClass();
                $('.subscribe-email').val('');
            });
        });
    </script>

    <script>
    $(document).ready(function () {
        // FitVids - start
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // FitVids - end

        // Replace nav with title on scroll - start
        Casper.stickyNavTitle({
            navSelector: '.site-nav-main',
            titleSelector: '.post-full-title',
            activeClass: 'nav-post-title-active'
        });
        // Replace nav with title on scroll - end

        // Hover on avatar
        var hoverTimeout;
        $('.author-list-item').hover(function () {
            var $this = $(this);

            clearTimeout(hoverTimeout);

            $('.author-card').removeClass('hovered');
            $(this).children('.author-card').addClass('hovered');

        }, function () {
            var $this = $(this);

            hoverTimeout = setTimeout(function () {
                $this.children('.author-card').removeClass('hovered');
            }, 800);
        });
    });
</script>


    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

</body>
</html>
