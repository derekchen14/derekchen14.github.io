<!DOCTYPE html>
<!--
    Type on Strap jekyll theme v2.3.1
    Copyright 2016-2020 Sylhare
    Theme free for personal and commercial use under the MIT license
    https://github.com/sylhare/Type-on-Strap/blob/master/LICENSE
-->
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <!-- Main JS (navbar.js, katex_init.js and masonry_init.js)-->
    <script defer src="/assets/js/main.min.js"></script>
    
    <!-- CSS -->
    <link rel="stylesheet" href="/assets/css/main.css">

    <!--Favicon-->
    <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

    <!-- RSS -->
    <link rel="alternate" type="application/atom+xml" title="More Than One Turn" href="https://derekchen14.github.io/feed.xml"/>
    
    

    <!-- KaTeX 0.12.0 -->
    
    <script defer src="/assets/js/vendor/katex.min.js"></script>
    <script defer src="/assets/js/vendor/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
    

    <!-- Mermaid 8.8.2 -->
    
    <!-- <script src=”https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.2/mermaid.min.js" onload="mermaid.initialize({startOnLoad:true});"></script> -->
    <script defer src="/assets/js/vendor/mermaid.min.js" onload="mermaid.initialize({startOnLoad:true});"></script>
    

    <!-- Simple-Jekyll-Search 1.17.12 -->
    <script src="/assets/js/vendor/simple-jekyll-search.min.js" type="text/javascript"></script>

    <!-- Google Analytics / Cookie Consent -->
    <script>
      const cookieName = 'cookie-notice-dismissed-https://derekchen14.github.io';
      const isCookieConsent = 'false';
      const analyticsName = 'G-KGN8PKZ5K7';
    </script>

    
    
        <!-- Global site tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-KGN8PKZ5K7"></script>
        <!-- Page analysis (analytics.js) -->
        <script async src='https://www.google-analytics.com/analytics.js'></script>
    

    <!-- seo tags -->
    <meta property="og:image" content="https://derekchen14.github.io/assets/img/feature-img/triangular.jpeg">
    
    <meta property="og:type" content="website" />
    
    <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Is Reinforcement Learning a Good Fit for Dialogue? | More Than One Turn</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Is Reinforcement Learning a Good Fit for Dialogue?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks. We can easily model the problem as a POMDP where the partially observed state represents the user’s intent. During each turn, the dialogue agent must make a decision about how to respond. This action space is represented as either a series of tokens or simplified even further into a single dialogue act. Lastly, task-oriented dialogue offers a natural reward — whether or not the dialogue succeeded. And yet, we don’t see (m)any deployed dialogue systems trained with RL. Why is that?" />
<meta property="og:description" content="On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks. We can easily model the problem as a POMDP where the partially observed state represents the user’s intent. During each turn, the dialogue agent must make a decision about how to respond. This action space is represented as either a series of tokens or simplified even further into a single dialogue act. Lastly, task-oriented dialogue offers a natural reward — whether or not the dialogue succeeded. And yet, we don’t see (m)any deployed dialogue systems trained with RL. Why is that?" />
<link rel="canonical" href="https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html" />
<meta property="og:url" content="https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html" />
<meta property="og:site_name" content="More Than One Turn" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-28T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Is Reinforcement Learning a Good Fit for Dialogue?" />
<script type="application/ld+json">
{"@type":"BlogPosting","url":"https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html","headline":"Is Reinforcement Learning a Good Fit for Dialogue?","dateModified":"2022-07-28T00:00:00-04:00","datePublished":"2022-07-28T00:00:00-04:00","description":"On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks. We can easily model the problem as a POMDP where the partially observed state represents the user’s intent. During each turn, the dialogue agent must make a decision about how to respond. This action space is represented as either a series of tokens or simplified even further into a single dialogue act. Lastly, task-oriented dialogue offers a natural reward — whether or not the dialogue succeeded. And yet, we don’t see (m)any deployed dialogue systems trained with RL. Why is that?","mainEntityOfPage":{"@type":"WebPage","@id":"https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->


    <!-- RSS -->
    <link type="application/atom+xml" rel="alternate" href="https://derekchen14.github.io/feed.xml" title="More Than One Turn" />

    <!-- Twitter Cards -->
    <meta name="twitter:title" content="Is Reinforcement Learning a Good Fit for Dialogue?">
    <meta name="twitter:description" content="On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks.  We can easily model the problem as a POMDP where the parti...">
    
    <meta name="twitter:creator" content="@derekchen14">
    <meta name="twitter:site" content="@derekchen14">
    
    <meta name="twitter:card" content="summary">
    <meta name="twitter:image" content="https://derekchen14.github.io/assets/img/feature-img/triangular.jpeg">
    <meta name="twitter:image:alt" content="Is Reinforcement Learning a Good Fit for Dialogue?">
</head>

  <body>
    <header class="site-header">

    <!-- Logo and title -->
	<div class="branding">
        
		<a href="/">
			<img alt="logo img" class="avatar" src="/assets/img/triangle.png" />
		</a>
        
        <a class="site-title" aria-label="More Than One Turn" href="/">
        More Than One Turn
		</a>
	</div>

    <!-- Toggle menu -->
    <nav class="clear">
    <a aria-label="pull" id="pull" class="toggle" href="#">
    <i class="fa fa-bars fa-lg"></i>
    </a>

    <!-- Menu -->
    <ul class="hide">
        

        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="About Me" title="About Me" href="/about/">
                     About Me 
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Contact" title="Contact" href="/contact/">
                     Contact 
                </a>
            </li>
            
            
        
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Publications" title="Publications" href="/publication/">
                     Publications 
                </a>
            </li>
            
            
        
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Search" title="Search" href="/search/">
                     <i class="fa fa-search" aria-hidden="true"></i>
                    
                </a>
            </li>
            
            
        
            
            <li class="separator"> | </li>
            <li>
                <a class="clear" aria-label="Tags" title="Tags" href="/tags/">
                     <i class="fa fa-tags" aria-hidden="true"></i>
                    
                </a>
            </li>
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
            
            
        
    </ul>

	</nav>
</header>

    <div class="content">
      <article class="feature-image" >
  <header id="main" style="">
    <div class="title-padder">
      
      <h1 id="Is+Reinforcement+Learning+a+Good+Fit+for+Dialogue%3F" class="title">Is Reinforcement Learning a Good Fit for Dialogue?</h1>
      


<div class="post-info">
    <p class="meta">
      July 28, 2022
    </p></div>

      
    </div>
  </header>

  <section class="post-content">
  
      <p>On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks.  We can easily model the problem as a POMDP where the partially observed state represents the user’s intent.  During each turn, the dialogue agent must make a decision about how to respond. This action space is represented as either a series of tokens or simplified even further into a single dialogue act.  Lastly, task-oriented dialogue offers a natural reward — whether or not the dialogue succeeded.  And yet, we don’t see (m)any deployed dialogue systems trained with RL.  Why is that?
 <!--more--></p>

<h2 id="rl-as-supervised-learning-in-sheeps-clothing">RL as Supervised Learning in Sheep’s Clothing</h2>

<p>In order to apply RL in realistic environments, a number of adjustments are often made to make the training tractable. While classic RL operates in an online setting, training an agent in such a manner quickly becomes impractical (ie. customer service), costly (ie. robotics), or even downright unethical (ie. healthcare).  So as a simplification, trajectories from a human agent are stored in an experience replay buffer for offline training.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup> Since the target policy \(\pi_g\) is obviously not the same as the behavioral policy \(\pi_b\) used to collect the data, off-policy evaluation methods are also applied.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote">2</a></sup> Conversations within even a narrow domain are hard to model completely, so now we must adopt model-free RL.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote">3</a></sup> Finally, since the state and action spaces of conversations are unbounded, modern RL systems do away with a table of Q-values, and instead use neural networks as function approximators.  In fact, why even calculate Q-values when directly optimizing for the policy with REINFORCE works just as well.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote">4</a></sup> At the end of the day, we’re left with an RL agent trained on mini-batches of examples on a loss function that takes into account neither world models nor discount factors. And since the underlying implementation in both cases is just a Transformer,<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote">5</a></sup>  this version of RL starts to look practically indistinguishable from regular ol’ supervised learning (SL).</p>

<p>It turns out, even folks working in reinforcement learning will readily admit that <a href="https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/">RL can be simply viewed as SL with a twist</a>. Specifically, the tweak is that the RL algorithms select good slices of data before applying behavior cloning (aka. supervised learning) to master the task.<sup id="fnref:6" role="doc-noteref"><a href="#fn:6" class="footnote">6</a></sup>  For example, hindsight relabeling provides better data by changing the agent’s original goal with the goal that was actually achieved.<sup id="fnref:7" role="doc-noteref"><a href="#fn:7" class="footnote">7</a></sup> However, this technique of labeling the data after it was collected also has its supervised learning equivalent. In fact, labeling the user intents from a large pool of unannotated conversations is precisely the most common method of providing supervision for the intent detection task.<sup id="fnref:8" role="doc-noteref"><a href="#fn:8" class="footnote">8</a></sup> More critically, isn’t data manipulation just some hack to improve training stability rather than a core part of reinforcement learning methodology?  The algorithmic part of the equation is behavior cloning which seems to be identical to supervised learning. So does RL really boil down to just SL wrapped with some fancy math?</p>

<h2 id="exploring-sub-optimal-trajectories">Exploring Sub-optimal Trajectories</h2>

<p>The key benefit which allows RL to potentially outshine SL is that it can learn from non-optimal trajectories.<sup id="fnref:9" role="doc-noteref"><a href="#fn:9" class="footnote">9</a></sup> In other words, the student can overcome the teacher by avoiding (or even learning from) the mistakes that the teacher has made. Based on my understanding so far, this concept can manifest itself in at least three different ways: maximizing good outcomes, minimizing bad outcomes, and changing bad outcomes into good ones.</p>

<p>First off, RL learning encourages exploration which allows for finding better reward regions.<sup id="fnref:10" role="doc-noteref"><a href="#fn:10" class="footnote">10</a></sup> Since RL operates with sparse rewards, sometimes going to a bad part of the environment is ok as long as the agent eventually learns to return back to the good part. This exploration occasionally pays off when the agent discovers areas of high reward that mindless imitation of past behavior could never visit. Secondly, trajectories that end in a bad outcome are not as damaging because the reward signal will automatically lead to the model to ignore such experiences.<sup id="fnref:11" role="doc-noteref"><a href="#fn:11" class="footnote">11</a></sup>  This useful mechanism is not found in supervised learning, as evidenced by the pervasive issue of generating boring, non-committal responses (ie. I don’t know) when such dialogue is clearly not ideal. The SL agent simply copies what it sees most, whereas the RL agent is able to downplay such utterances. Lastly, an RL agent can take advantage of a poor outcome by using it as an example of what <em>not</em> to do. Incorporating a penalty into training is trivial by adding a negative reward.  While modifying the loss function within supervised learning is certainly also possible, figuring out the exact formulation is not as straightforward. We also shouldn’t forget our previously mentioned technique of hindsight replay as another way to turn dirt into gold.<sup id="fnref:12" role="doc-noteref"><a href="#fn:12" class="footnote">12</a></sup></p>

<p>What we saw earlier in the previous section was not a single, isolated case of RL providing better data, but actually a part of larger trend where a RL model achieves superior performance by directly tackling the data problem along with the modeling (policy). How realistic are these advantages though?  While these three cases allow RL to <em>theoretically</em> outperform SL, does this play out in practice? To start, we’ve already discussed the catastrophic consequences of deploying an RL agent to learn from real users as it explores unsuccessful paths. So while the model won’t be harmed too much by sub-optimal trajectories, the product surrounding the model will suffer. To get around this, one could consider developing a user simulator for to mimic customer behavior.<sup id="fnref:13" role="doc-noteref"><a href="#fn:13" class="footnote">13</a></sup> Much like the gaming environment of Atari allowed RL to achieve super-human performance on video games, an accurate user simulator should allow an agent to take full advantage of what RL can offer.</p>

<h2 id="ideal-data-generators-for-rl">Ideal Data Generators for RL</h2>

<p>If the key to unlocking reinforcement learning is to design a robust user simulator, how does one go about doing that exactly?  Well, a reinforcement learning environment is expected to take in the current state along with an agent action to produce the next state along with its associated reward.<sup id="fnref:11:1" role="doc-noteref"><a href="#fn:11" class="footnote">11</a></sup>  This implies that a proper user simulator contains two components, a representative model of the user to output next states and an accurate method of evaluation to output well-calibrated rewards.</p>

<p>Touching upon the user model first, we argue this is essentially intractable since there is no feasible way to predict how a user would react in any given situation.  If we knew what the user wanted, we wouldn’t need agent interaction in the first place, and could just fulfill the customer request immediately. In the context of games, the goal of an RL agent is obviously to maximize the score. But humans don’t have a come with a universal scoring function. Even for the same task, different people might want completely different things.  For example, two groups want to book a dinner reservation at the same Italian restaurant, but one group is for a large party with complex dietary restrictions, while the other is a romantic dinner for two.  Frankly, even for the same person, the ideal outcome differs over time.  Consider someone buying a movie ticket for one week, but wanting to watch a different movie the next week.  Building a truly accurate model would entail constructing a new environment for every user. Clearly, we must relax the assumptions about users and instead assume that there exist patterns among different users even if their individual circumstances are slightly unique. We can operationalize the idea that two different users want the same things by offering the agent identical rewards when performing some desired action that equally benefits both users.</p>

<p>What we’ve learned so far is that reinforcement learning is beneficial only insofar as we can build useful user simulators for producing agent experiences, and that the only aspect of user simulators we can realistically control is their ability to generate appropriate reward signals. However, real-life has no intrinsic rewards, so how do we determine dialogue success?  Open domain chat includes subjective measures such as user satisfaction and dialogue fluency<sup id="fnref:14" role="doc-noteref"><a href="#fn:14" class="footnote">14</a></sup>, but defining success in task-oriented dialogue is also imprecise.<sup id="fnref:15" role="doc-noteref"><a href="#fn:15" class="footnote">15</a></sup> Suppose the user is booking a flight and the goal is to select the correct flight from a set of options with a KB.  Forget the extra details of dealing with discount codes or fees for checked bags.  Let’s assume the agent is given the straightforward task of collecting information on a finite set of constraints, such as desired price range, departure and destination locations, and number of seats.  Then suppose the agent successfully books the correct flight that meets all such criteria and deserves a (discounted) reward for all steps taken.  Where does this reward score come from?  We don’t know a priori what the user wanted, so a human would need go in after the fact to mark the conversation as successful. To the extent that we are just labeling data, it doesn’t seem like reinforcement learning is any better (or any worse) than training under a supervised learning paradigm.</p>

<p>Ultimately, RL algorithms are great, but the real limitation is the ability to quickly and scalably label collected conversations.  To take it a step further, one might say the real problem comes down to getting the right data.  Of course, that was always the key to begin with, since good data is the answer to everything on this blog ;)</p>

<hr />

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Lipton et al. (2016) <a href="https://www.microsoft.com/en-us/research/publication/efficient-exploration-dialogue-policy-learning-bbq-networks-replay-buffer-spiking/">Efficient Exploration for Dialog Policy Learning with BBQ Networks &amp; Replay Buffer Spiking</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Weisz et al. (2018) <a href="https://arxiv.org/abs/1802.03753">Sample Efficient Deep Reinforcement Learning for Dialogue Systems with Large Action Spaces</a> (ACER) <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>Jiang et al. (2021) <a href="https://aclanthology.org/2021.emnlp-main.589/">Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach</a> (ENIGMA) <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>Liu et al. (2017) <a href="https://arxiv.org/abs/1711.10712">End-to-End Optimization of Task-Oriented Dialogue Model with Deep RL</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>Chen et al. (2021) <a href="https://openreview.net/forum?id=a7APmM4B9d">Decision Transformer: Reinforcement Learning via Sequence Modeling</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:6" role="doc-endnote">
      <p>Eysenbach, Kumar, and Gupta (2020) <a href="https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/">Reinforcement Learning is Supervised Learning on Optimized Data</a> <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:7" role="doc-endnote">
      <p>Kumar, Peng, and Levine (2019) <a href="https://arxiv.org/abs/1912.13465">Reward-Conditioned Policies</a> <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8" role="doc-endnote">
      <p>Casaneuva et al. (2020) <a href="https://aclanthology.org/2020.nlp4convai-1.5/">Efficient Intent Detection with Dual Sentence Encoders</a> <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9" role="doc-endnote">
      <p>Agrawal and Wu (2021) <a href="https://professional.mit.edu/news/articles/reinforcement-learning-right-your-ai-problem">Is Reinforcement Learning Right for your AI Problem?</a> <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10" role="doc-endnote">
      <p>Weng (2020) <a href="https://lilianweng.github.io/posts/2020-06-07-exploration-drl/">Exploration Strategies in Deep Reinforcement Learning</a> <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11" role="doc-endnote">
      <p>Sutton and Barto (2014) <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement Learning: An Introduction</a> (2nd Edition) <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:11:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:12" role="doc-endnote">
      <p>Ghosh et al. (2019) <a href="https://arxiv.org/abs/1912.06088">Learning to Reach Goals via Iterated Supervised Learning</a> <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13" role="doc-endnote">
      <p>Shi et al. (2019) <a href="https://aclanthology.org/D19-1206/">How to Build User Simulators to Train RL-based Dialog Systems</a> <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:14" role="doc-endnote">
      <p>Saleh et al. (2019) <a href="https://arxiv.org/abs/1909.07547">Hierarchical Reinforcement Learning for Open-Domain Dialog</a> <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15" role="doc-endnote">
      <p>Wang, Peng, and Wong (2020) <a href="https://aclanthology.org/2020.acl-main.566/">Learning Efficient Dialogue Policy from Demonstrations through Shaping</a> (S^2 Agent) <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

    
  </section>

  <!-- Social media shares -->
  <!-- 

<div class="share-buttons">
    <ul class="share-buttons">
        <div class="meta">Share</div>
        
        <li>
            <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fderekchen14.github.io%2F2022%2F07%2F28%2Fis-rl-a-good-fit-for-dialogue.html" target="_blank" title=" Facebook">
			<i class="fa fa-facebook-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Facebook</span>
		</a>
        </li>
         
        <li>
            <a href="https://twitter.com/intent/tweet?text=Is+Reinforcement+Learning+a+Good+Fit+for+Dialogue%3F%20https%3A%2F%2Fderekchen14.github.io%2F2022%2F07%2F28%2Fis-rl-a-good-fit-for-dialogue.html" target="_blank" title="">
			<i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Tweet</span>
		</a>
        </li>
            
        <li>
            <a href="http://www.reddit.com/submit?url=https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html&title=Is+Reinforcement+Learning+a+Good+Fit+for+Dialogue%3F%20%7C%20More+Than+One+Turn" target="_blank" title=" Reddit">
			<i class="fa fa-reddit-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Share on Reddit</span>
		</a>
        </li>
           
        <li>
            <a href="mailto:?subject=Is+Reinforcement+Learning+a+Good+Fit+for+Dialogue%3F%20%7C%20More+Than+One+Turn&body=:%20https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html" target="_blank" title="">
			<i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
			<span class="sr-only">Email</span>
		</a>
        </li>
        
    </ul>
</div>


 -->

   <!-- Tag list -->
  
  


<footer>
  <div class="tag-list">
    
      <div class="meta">Tags</div>
    

    
    <a class="button" href="/tags#dialogue">
      <p><i class="fa fa-tag fa-fw"></i> dialogue</p>
    </a>
    
    <a class="button" href="/tags#modeling">
      <p><i class="fa fa-tag fa-fw"></i> modeling</p>
    </a>
    
    <a class="button" href="/tags#product-strategy">
      <p><i class="fa fa-tag fa-fw"></i> product-strategy</p>
    </a>
    
    <a class="button" href="/tags#rl">
      <p><i class="fa fa-tag fa-fw"></i> rl</p>
    </a>
    
  </div>
</footer>


</article>

<!-- Disqus -->


<!-- Post navigation -->

  <div id="post-nav">
    

    
    <div id="next-post">
        <a alt="Building User Simulators for Scalable Dialogue Systems" href="/2022/05/24/building-user-simulators.html">
            <p>Next post</p>
            Building User Simulators for Scalable Dialogue Systems
        </a>
    </div>
    
</div>



<!-- To change color of links in the page -->
<style>
  
  .feature-image a { color: green !important; }
  div#post-nav a { color: green !important; }
  footer a { color: green !important; }
  .site-header nav a:hover {  color: green !important; }
  header#main { background-color: green !important; }
  

  header#main {
    background-repeat:no-repeat;
  background-image: url('/assets/img/lineart.png');  
  }
</style>

    </div>
    <footer class="site-footer">
    <p class="text">
        Aut viam inveniam aut faciam.</p>
            <div class="footer-icons">
                <ul>
                <!-- Social icons from Font Awesome, if enabled -->
                

<li>
    <a href="mailto:derekchen14@gmail.com" title="Email">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x"></i>
            <i class="fa fa-envelope fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>













<li>
    <a href="https://github.com/derekchen14" title="Follow on GitHub">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#333333"></i>
            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>







<li>
    <a href="https://www.linkedin.com/in/derekchen14/" title="Follow on LinkedIn">
		<span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#2867B2"></i>
            <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>









<li>
    <a feed.xml href="/feed.xml"
       title="Follow RSS feed">
        <span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:orange"></i>
            <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>














<li>
    <a class="type" href="https://twitter.com/derekchen14"
       title="Follow on Twitter">
        <span class="fa-stack fa-lg">
            <i class="fa fa-square fa-stack-2x" style="color:#1DA1F2"></i>
            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
        </span>
    </a>
</li>








                </ul>
            </div>
</footer>



  </body>
</html>
