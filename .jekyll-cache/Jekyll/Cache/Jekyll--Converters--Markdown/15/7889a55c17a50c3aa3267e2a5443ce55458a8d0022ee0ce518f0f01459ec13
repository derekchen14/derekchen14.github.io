I"<p>So if youâ€™ve started studying RNNs, and you heard that LSTMs and GRUs at the type of RNNs you should use because vanilla RNNs suffer from the vanishing gradient problem.  That makes sense because the hidden state is passed along for each iteration, so when back-propagating, the same Jacobian matrix is multiplied by itself over and over again.  If that matrix has a principal eigenvalue less than one, then we have a vanishing gradient.  Incidentally, if the matrix has a principal eigenvalue greater than one: exploding gradient.</p>
:ET