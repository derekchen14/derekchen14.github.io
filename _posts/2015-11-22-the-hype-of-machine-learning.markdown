---
layout: post
title: The Hype of Machine Learning
date: '2015-11-22 02:01:00'
tags:
- machine-learning
---

Many folks in the tech industry are [skeptical](http://www.kdnuggets.com/2015/01/deep-learning-flaws-universal-machine-learning.html) about the hype of machine learning, since the world has been promised to them before, but the reality hasn't really panned out.  In fact this has happened so many times, there is a term for this in the machine learning community called an [AI Winter](https://en.wikipedia.org/wiki/AI_winter "AI Winter"). A number of fundamental factors are different though in 2015 that warrant taking a closer look this time around.

###Why Machine Learning is the Real Deal###
* **Larger data sets** - The explosion of big data is probably the number one driver of the new effectiveness of machine learning. It turns out that using the exact same techniques, more data can yield significantly better predictions.  This new data comes from many sources:
  * *Data collection* - this is the obvious one, we now have more devices like smartwatches, smartphones, and computers generating more data.  More analytics packages and more people spending their time in a digital world
  * *Data labeling* - gathering more data through unsupervised learning methods first, such as auto-encoders
  * *Feature engineering* - creating new training data through skewing, translations, rotations, and other transformations
* **Improved Hardware** - Following Moore's Law, the processing power of chips and other hardware components continue to steadily improve over time, which simply leads to faster training.
  * *GPU vs CPU vs FGPA* - the emergence of [GPUs](http://www.nvidia.com/object/machine-learning.html) for training machine learning algorithms has given a huge boost in speed in calculating matrix operations.  While a GPU can't do many types of operations, it can focus on doing one operation really, really well.  Same is true with using [FPGAs](https://gigaom.com/2014/08/14/researchers-hope-deep-learning-algorithms-can-run-on-fpgas-and-supercomputers/) for training neural networks.
  * *Better Processors* - The chips themselves are just faster and more powerful over time.  Although, there is some news that rate of improvement is not as fast anymore, things are still [getting better](http://venturebeat.com/2015/11/19/intel-says-moores-law-isnt-dead-yet/).
* **Better Algorithms** - Lastly, there have also been advances in how to train these machines to learn.  While this third component is what academic researchers and the media often like to focus on, it is worth stating again that the first two are both equally important advances.  Examples include:
  * *Neural networks* - whereas most functions of the past used some combination of statistics ([Naive Bayes](https://en.wikipedia.org/wiki/Probabilistic_classification)), data transformations (kernels in [SVMs](https://en.wikipedia.org/wiki/Support_vector_machine)), or increasingly sophisticated ensembling techniques ([Random Forests](https://en.wikipedia.org/wiki/Ensemble_learning)), neural networks model the problem from a totally different perspective.  By mimicking how the human brain functions, [neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) are able to achieve a surprising level of complexity without losing the flexibility to adapt to unique domains.
  * *Network architecture* - as researchers continued to explore neural networks, they began adding significantly more layers to the model, giving rise to [deep learning](https://en.wikipedia.org/wiki/Deep_learning). Not only were there more layers though, sometimes these layers performed different functions, such as activation, max-pooling, or convolutions.  Finally, sometimes the network also grew larger in the front due to pre-training layers using unsupervised methods, or in the end due to extra recurrent layers from RNNs.
  * *Sum is greater than the parts* - lots of other little wins that incrementally are not worth much, but combined have drastically improved the accuracy of machine learning.  Without going into too much detail, some noteworthy advances include improved initialization methods ([Lecun](yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf), [Glorot](http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf), [He](http://arxiv.org/abs/1502.01852.)) to start off training and improved learning rates ([RMSProp](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf), [AdaGrad](http://www.ark.cs.cmu.edu/cdyer/adagrad.pdf), [Adam](http://arxiv.org/pdf/1412.6980.pdf)) to speed up the process.  And let's not forget improved activation functions ([Sigmoid](http://ufldl.stanford.edu/wiki/index.php/Neural_Networks), [tanH](http://stats.stackexchange.com/questions/101560/tanh-activation-function-vs-sigmoid-activation-function), [ReLU](https://imiloainf.wordpress.com/2013/11/06/rectifier-nonlinearities/)) to avoid neuron saturation and regularization techniques ([lasso](http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf), [ridge](https://en.wikipedia.org/wiki/Tikhonov_regularization), [dropout](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)) to avoid overfitting on training data.

###Examples of Improved Machine Learning###
Frankly, words can't do justice in explaining the amazing advances that have been made in the last year â€“ so just watch.

* *Image Recognition* - 5 years ago, computers could not tell the difference between an image of cat and an image of dog.  Now, they can interpret objects better than most people: <iframe width="560" height="315" src="https://www.youtube.com/embed/40riCqvRoMs" frameborder="0" allowfullscreen></iframe>
* *NLP and Translation* - Not too long ago, computers could barely understand the words you were speaking even if spoken slowly from a limited vocabulary.  Now, they can translate entire sentences on the fly: <iframe width="420" height="315" src="https://www.youtube.com/embed/Nu-nlQqFCKg" frameborder="0" allowfullscreen></iframe>
* *Memory Networks* - Not too long ago, computers could not really interpret any meaning or context from data.  Now, they can chain together sentences to [reason through logic](http://www.techinsider.io/facebook-lord-of-the-rings-teach-memory-network-2015-11).

It's no surprise then that [all](https://code.facebook.com/posts/1478523512478471/teaching-machines-to-see-and-understand-advances-in-ai-research/) [the](http://googleresearch.blogspot.com/2015/06/inceptionism-going-deeper-into-neural.html) [big](http://blogs.microsoft.com/next/2015/11/11/happy-sad-angry-this-microsoft-tool-recognizes-emotions-in-pictures/) [names](http://blogs.wsj.com/digits/2015/08/26/amazon-com-offers-machine-learning-platform-hires-scientists/) are joining in on the fun.

**Conclusion**

To be clear, we aren't in this wonderful new world yet. The majority of machine learning news you hear these days is simply about incremental improvements in data science that allow a researcher to better predict outcome Y based on features X<sub>1</sub>, X<sub>2</sub>, and X<sub>3</sub>. But we are quickly moving into a reality where machines will simply understand our desires and cater to those personal interests with little to no prompting.  And because [technology improves exponentially](https://www.ted.com/talks/ray_kurzweil_on_how_technology_will_transform_us?language=en), the age of machine learning and artificial intelligence is arriving much sooner than you may have been led to believe.


