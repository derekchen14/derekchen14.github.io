I"<p>A key sub-issue in designing conversational agents is being able to reliably calculate uncertainty over the modelâ€™s beliefs. Â In doing so, the model would be able to recognize when it does not understand something, and appropriately ask for clarification. Â Thus, we can imagine the output of an uncertainty model feeding into a dialogue policy manager which then decides to either retrieve an answer from the knowledge base when it feels fairly certain it knows what the customer wants, or to ask a follow-up question when it is unsure. Â From a information-theory point of view, this can be seen as a model which asks questions to minimize entropy until it reaches a certain threshold, at which point it will return an answer. Beyond improving model behavior, measuring uncertainty also gives a view into how the model is thinking for improved debugging and enhanced interpretability.</p>
:ET