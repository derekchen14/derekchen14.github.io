<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://derekchen14.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://derekchen14.github.io/" rel="alternate" type="text/html" /><updated>2022-10-26T17:02:58-04:00</updated><id>https://derekchen14.github.io/feed.xml</id><title type="html">More Than One Turn</title><subtitle>Blog about dialogue modeling and data efficiency</subtitle><entry><title type="html">Solving the Long Tail of Data Acquisition</title><link href="https://derekchen14.github.io/2022/10/24/machine-assisted-data-labeling.html" rel="alternate" type="text/html" title="Solving the Long Tail of Data Acquisition" /><published>2022-10-24T00:00:00-04:00</published><updated>2022-10-24T00:00:00-04:00</updated><id>https://derekchen14.github.io/2022/10/24/machine-assisted-data-labeling</id><content type="html" xml:base="https://derekchen14.github.io/2022/10/24/machine-assisted-data-labeling.html">&lt;div class=&quot;hidden&quot;&gt;The deep learning revolution was initiated not by intelligent algorithms alone, but with the help of powerful compute and access to data. While models have continued to improve and compute power has continued to advance, progress on data collection has not been so fortunate.  Data labeling companies would have you believe that they alone are the solution to all our data problems - provided, of course, that we pay their exorbitant fees.  However, relying on vendors avoids the core issue since the underlying cost and complexity of gathering high quality data remains untouched.  As things currently stand, the effort needed to annotate each additional batch of data increases over time as the examples reach the long tail of the distribution. Any company able to figure out a way to transform data annotation from a cost-center into an area of innovation gains an enviable competitive advantage. But how?&lt;/div&gt;

&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;The deep learning revolution was initiated not by intelligent algorithms alone, but with the help of powerful compute and access to data.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; While models have continued to improve and &lt;a href=&quot;https://blogs.nvidia.com/blog/2022/09/20/keynote-gtc-nvidia-ceo/&quot;&gt;compute power has continued to advance&lt;/a&gt;, progress on data collection has not been so fortunate.  Data labeling companies would have you believe that they alone are the solution to all our data problems ‚Äì provided, of course, that we pay their exorbitant fees.  However, relying on vendors avoids the core issue since the underlying cost and complexity of gathering high quality data remains untouched.  As things currently stand, the effort needed to annotate each additional batch of data &lt;em&gt;increases&lt;/em&gt; over time as the examples reach the long tail of the distribution. Any company able to figure out a way to transform data annotation from a cost-center into an area of innovation gains an enviable competitive advantage. But how?&lt;/p&gt;

&lt;h2 id=&quot;the-long-tail-of-data-annotation&quot;&gt;The Long Tail of Data Annotation&lt;/h2&gt;

&lt;!-- ![Jedi IQ on Labeled Data](/assets/img/jedi_labeled_data.png) --&gt;

&lt;p&gt;Given the obvious dependence on labels to perform supervised learning, even newcomers to AI recognize the necessity of annotated data.  It doesn‚Äôt take much longer though to realize just how hard it is to collect those labels, so the conversation quickly shifts to the brighter and shinier parts of machine learning ‚Äî the modeling.  But as alluded to earlier, this attitude just doesn‚Äôt cut it though when an organization must face the reality that their raison d‚Äô√™tre is to satisfy customer needs and not to play with cutting-edge toys.&lt;/p&gt;

&lt;p&gt;In a &lt;a href=&quot;https://future.com/new-business-ai-different-traditional-software/&quot;&gt;thought-provoking review&lt;/a&gt; of the AI landscape, Martin Casado and Matt Bornstein from A16Z called out the fundamental flaws that prevented AI companies from enjoying the typical returns found in their SaaS counterparts.  They noted, in Feb 2020, that ‚Äúscaling AI systems can be rockier than expected, because AI lives in the long tail.‚Äù  The boost provided to the ML system by the first 10,000 training examples is significantly greater than the boost seen when adding data in later on.  Furthermore, the cost of labeling examples 100,000 to 110,000 has also increased exponentially along the way.  Instead of getting stronger, the &lt;a href=&quot;https://a16z.com/2019/05/09/data-network-effects-moats/&quot;&gt;data moat erodes as the corpus grows&lt;/a&gt;.  The result: a negative fly-wheel where more customers means higher costs to serve them. The exact opposite effect that a scaling start-up wants to experience.&lt;/p&gt;

&lt;p&gt;The same authors &lt;a href=&quot;https://a16z.com/2020/08/12/taming-the-tail-adventures-in-improving-ai-economics/&quot;&gt;provide some hints&lt;/a&gt; at improving the unit economics of such a business model.  Use the right tool for the job (ie. not deep learning).  Narrow the focus of the system into something manageable.  Consider the benefits of transfer learning.  These points are all correct, but they‚Äôre mostly just bandages on a fundamentally difficult situation.  How exactly does transfer learning solve the long tail of data labeling?  It doesn‚Äôt. So, where do we look to find the AI revolution we so desperately seek?&lt;/p&gt;

&lt;h2 id=&quot;machine-in-the-loop-annotation&quot;&gt;Machine-in-the-Loop Annotation&lt;/h2&gt;

&lt;p&gt;A sizable number of incumbents already exist in the data labeling industry, providing annotations for self-driving cars, named entity recognizers and everything in between.  Millions of dollars have been poured in to prop these companies up into unicorn status.  If lack of labeled data is the sin, might these companies be our saviors?&lt;/p&gt;

&lt;p&gt;At first glance, it does seem like such businesses should play an important role in moving the field forward.  They have invested heavily into streamlining their processes, in addition to designing high-usability platforms to make the annotation experience as smooth as possible.  There is no doubt that they will eventually build ML driven data augmentation right into the annotation process if they haven‚Äôt already done so.  And there‚Äôs a growing body of research to suggest that such machine-assisted labeling is indeed the way of the future.&lt;/p&gt;

&lt;p&gt;As a quick aside, let‚Äôs visit just a handful of examples.  To start, it might not be surprising that if you ‚ÄúWant To Reduce Labeling Costs, GPT-3 Can Help‚Äù through pseudo-labeling.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;  You can also use the model to generate the datasets, including synthesized inputs and outputs.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;  Others have showed that using GPT-3 followed by human verification improves Extractive QA.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; You can even use models to help with the verification process as well.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Now, back to the data labeling effort.  Does machine-in-the-loop annotation solve our problem?  &lt;a href=&quot;https://twitter.com/arankomatsuzaki/status/1529278580189908993&quot;&gt;Let‚Äôs think step-by-step&lt;/a&gt;.  An AI revolution requires scalable access to labeled data.  Data labeling companies outsource the effort to overseas workers, lowering their costs.  They employ technology to lower the costs further.  Now the cost of dealing with the long-tail is much lower than before. But the negative feedback cycle still remains.&lt;/p&gt;

&lt;p&gt;The issue has been mitigated, but not eliminated.&lt;/p&gt;

&lt;p&gt;Now, let‚Äôs be clear.  If you are running a firm or department in charge of annotating large amounts of data, you should certainly make an effort to take full advantage of the current state-of-the-art techniques.  The point is not that machine-assistance is bad, but more that machine-assistance is not enough.&lt;/p&gt;

&lt;blockquote&gt;
&quot;We must flip the script such that the burden of annotation is placed on the machine, with humans merely assisting.&quot;
&lt;/blockquote&gt;

&lt;p&gt;Data labeling firms generate a profit by farming tasks out to lower paid crowdsource workers, taking advantage of labor arbitrage.  Notably, they are &lt;em&gt;not&lt;/em&gt; taking advantage of economies of scale since the cost of annotating each marginal data point doesn‚Äôt budge.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;  AI-assisted annotations certainly helps, but improved GUIs and task reformulation are likely to help just as much.&lt;/p&gt;

&lt;p&gt;In order for an AI-first company to survive, we have to move from a &lt;em&gt;machine-assisted&lt;/em&gt; data labeling process to a &lt;em&gt;machine-driven&lt;/em&gt; one. At the moment, humans still bear the bulk of the annotation effort, and machines just help where possible. Instead, we must flip the script such that the burden of annotation is placed in the machine, with humans merely assisting.  Only then would we be able to invert the negative flywheel into a positive loop.&lt;/p&gt;

&lt;h2 id=&quot;towards-a-human-in-the-loop-paradigm&quot;&gt;Towards a Human-in-the-Loop Paradigm&lt;/h2&gt;

&lt;p&gt;A fully automated annotation system that can operate without any intervention is essentially AGI.  To expect such an outcome is unrealistic.  However, we can ask ourselves if it is possible to reach a reality where human intervention is minimized, or perhaps even where the amount of data being labeled is decoupled from the level of human involvement.&lt;/p&gt;

&lt;p&gt;To see how a sustainable business might operate in this world, we can consider how building in the cloud has evolved.  Let‚Äôs define Software 1.0 as a process whereby a number of software developers write a bunch of code instructing the machine on what do to.  In contrast, machines in Software 2.0 learn what to do for themselves by training on a bunch of labeled data. Now if it weren‚Äôt for that pesky step of manually labeling the data, we would have a completely machine-driven process.  That‚Äôs not the case at the moment, but let‚Äôs suppose it were.  Assume the data labelers were replaced by a magical technology fairy.  Would humans still be involved at all?&lt;/p&gt;

&lt;p&gt;If we were to replace software developers in the same manner (ü§® &lt;a href=&quot;https://betterprogramming.pub/github-copilot-a-tool-or-doom-for-programmers-dfa79ba84bbc?gi=b2851c166d2a&quot;&gt;CoPilot&lt;/a&gt;?), we would see that there are still many humans involved with launching new features.  Even discounting the product managers, designers and business analysts outside the development process, there already exist a number of programmers helping with code review, QA and IT Ops.&lt;/p&gt;

&lt;p&gt;As more code is deployed, the more QA is needed to review the code.  This level of human intervention is acceptable though because there isn‚Äôt a long tail effect where the amount of code written exacerbates the amount of QA needed.  The increase in effort is linear rather than exponential.  Moreover, the degree of QA controls the likelihood of the bugs in production, and not the lines of code being written.  This decoupling means the human QA effort does not bottleneck the coding effort.
Arguably, there is even a positive feedback loop.  As the programmer writes more code, the more feedback they receive from QA and the better code they write as they learn to avoid common mistakes over time.&lt;/p&gt;

&lt;p&gt;How does this translate to a Software 2.0 environment?  If the data labeling pipeline is managed in the same manner, then the humans-in-the-loop are there simply to QA the quality of the data.  Model deployments (ie. feature releases) are no longer blocked by manual exercises, yet model accuracy improves when more human effort is applied.  We have effectively transformed heavy human involvement into lightweight human verification, letting the machine perform the bulk of the work instead.  The development process has shifted into something safe and reliable, almost mundane, in its implementation.&lt;/p&gt;

&lt;p&gt;Why hasn‚Äôt this already happened?  Looking back, we notice that this world starts with the assumption of a technology fairy.  What we need to now is figure out how to engineer such a system capable of generating labeled data.&lt;/p&gt;

&lt;h2 id=&quot;controllable-reliable-explainable&quot;&gt;Controllable, Reliable, Explainable&lt;/h2&gt;

&lt;p&gt;Large, pre-trained LMs are capable of generating high quality, high fluency text.  Fantastic performance is possible even without explicit training through in-context learning.  But their gargantuan size can make them hard to control,&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; lexically or semantically.  When they work well, all is fine, but when predictions are incorrect or generations are toxic, there‚Äôs not much that can be done about it.  Fine-tuning is impractical at best, and unattainable at worst since the model is hidden behind a pay-to-play API.  With training already this difficult, probing for explanations is effectively impossible.&lt;/p&gt;

&lt;p&gt;Human annotators on the other hand are also able to produce fluent and coherent text.  They are relatively easy to control through guidelines and training manuals.  They behave in well understood ways and often make a good faith effort to be correct.  Critically, when mistakes are inevitably made, human annotators can easily explain their thought process and quickly adapt to changing circumstances.&lt;/p&gt;

&lt;p&gt;If we‚Äôre willing to look past the hype, we see that LLMs still have a way to go before matching human performance on data labeling.  They fall short on three key desiderata: controllability, reliability and explainability.&lt;/p&gt;

&lt;blockquote&gt;
&quot;By leveraging large LMs as data generators rather than directly as classifiers, a number of strategic benefits are now unlocked.&quot;
&lt;/blockquote&gt;

&lt;p&gt;Is there a way out?  The key insight is to control the output of the LLM enough to be able to generate label-preserving training data.  We then fine-tune smaller, more manageable models on this data for downstream applications.  By leveraging large LMs indirectly as data generators rather than directly as classifiers, a number of strategic benefits are now unlocked.&lt;/p&gt;

&lt;p&gt;Rather than using the model directly for prediction, we now have access to tangible data which can be further molded and manipulated as desired.  If the diversity generated data is too low, we can apply additional data augmentation procedures.  If the noise of the generated data is too high, we can apply data denoising or data filtering techniques.  This format allows us to reliably produce high quantity and high quality data since we can repeat the process indefinitely until we have converged on a result we are satisfied with.&lt;/p&gt;

&lt;p&gt;The discrete nature of the outputs means we can also directly observe what the large LM is thinking (to a certain extent).  If it starts to generate biased text, we can deal with it right away before feeding to the downstream model.  If we‚Äôre exploring a new area due to domain shift, we can see interpretable examples of what the LLM believes is right.  This gets us the best of both worlds - automated, scalable data annotation leading directly to improved downstream model performance.&lt;/p&gt;

&lt;p&gt;But where did the control of the large model come from?  While it‚Äôs true that we can‚Äôt fine-tune the entire model (ie. CTRL),&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; we can nudge it in the right direction using soft-prompting and adapters. The outputs aren‚Äôt perfect, but they don‚Äôt have to be.  The augmented data is directly manageable so we can clean and denoise until it fits our goals.  The research on how to make this happen is an active, ongoing direction.&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Finally, and maybe most critically, we can use the improved data to train better models.  The improved models interact with humans, collecting more real-world cases that are then fed back to the large LM for labeling.  For the first time, we have a truly positive feedback loop, where AI genuinely gets better the more it is used.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Machine-assisted data labeling naturally hit limitations due to the complexities of real-life scenarios.  Machine-driven data labeling is required instead to cover the long tail, where humans are in the loop simply to verify.  This allows AI-first companies to scale into the future long after the funding and hype have faded away.&lt;/p&gt;

&lt;!-- Large language models are Few-Shot Learners.[^1] And Zero-Shot Learners.[^2] And Zero-shot Reasoners too, why not?[^3]  So why hasn&apos;t AI [taken over the world](https://www.gwern.net/fiction/Clippy) yet? Well, apocalyptic scenarios aside, applying AI to real world problems just isn&apos;t that simple. --&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(Sun et al., 2017) &lt;a href=&quot;https://arxiv.org/abs/1707.02968&quot;&gt;Revisiting Unreasonable Effectiveness of Data in Deep Learning Era&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(Wang et al., 2021) &lt;a href=&quot;https://aclanthology.org/2021.findings-emnlp.354/&quot;&gt;Want To Reduce Labeling Costs, GPT-3 Can Help&lt;/a&gt;¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;WANLI (Liu et al., 2021) &lt;a href=&quot;https://arxiv.org/abs/2201.05955&quot;&gt;Worker and AI Collaboration for NLI Dataset Creation&lt;/a&gt;¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;DADC (Bartolo et al., 2022) &lt;a href=&quot;https://aclanthology.org/2022.naacl-main.275.pdf&quot;&gt;Models in the Loop: Aiding Crowdworkers with Generative Annotation Assistants&lt;/a&gt;¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;(Wiegreffe et al., 2022) &lt;a href=&quot;https://aclanthology.org/2022.naacl-main.47/&quot;&gt;Reframing Human-AI Collaboration for Generating Free-Text Explanations&lt;/a&gt;¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Data labeling companies are possibly the &lt;em&gt;least&lt;/em&gt; likely group to innovate a novel method of data annotation.  Their business model incentives them to maintain the status quo since any deviation risks depreciating the tremendous investment poured into their current infrastructure, including their network of crowdworkers, optimized onboarding routines and refined sales copy.  This pushback is common whenever a new way of doing things makes the old way obsolete.¬†&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Prefix-tuning (Li and Liang, 2021) &lt;a href=&quot;https://aclanthology.org/2021.acl-long.353/&quot;&gt;Prefix-Tuning: Optimizing Continuous Prompts for Generation&lt;/a&gt;¬†&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;CTRL (Keskar et al., 2019) &lt;a href=&quot;https://arxiv.org/abs/1909.05858&quot;&gt;A Conditional Transformer Language Model for Controllable Generation&lt;/a&gt;¬†&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Driven by yours truly¬†&lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="data-strategy" /><category term="product-strategy" /><category term="research" /><category term="startups" /><summary type="html">The deep learning revolution was initiated not by intelligent algorithms alone, but with the help of powerful compute and access to data. While models have continued to improve and compute power has continued to advance, progress on data collection has not been so fortunate. Data labeling companies would have you believe that they alone are the solution to all our data problems - provided, of course, that we pay their exorbitant fees. However, relying on vendors avoids the core issue since the underlying cost and complexity of gathering high quality data remains untouched. As things currently stand, the effort needed to annotate each additional batch of data increases over time as the examples reach the long tail of the distribution. Any company able to figure out a way to transform data annotation from a cost-center into an area of innovation gains an enviable competitive advantage. But how?</summary></entry><entry><title type="html">Is Reinforcement Learning a Good Fit for Dialogue?</title><link href="https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html" rel="alternate" type="text/html" title="Is Reinforcement Learning a Good Fit for Dialogue?" /><published>2022-07-28T00:00:00-04:00</published><updated>2022-07-28T00:00:00-04:00</updated><id>https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue</id><content type="html" xml:base="https://derekchen14.github.io/2022/07/28/is-rl-a-good-fit-for-dialogue.html">&lt;p&gt;On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks.  We can easily model the problem as a POMDP where the partially observed state represents the user‚Äôs intent.  During each turn, the dialogue agent must make a decision about how to respond. This action space is represented as either a series of tokens or simplified even further into a single dialogue act.  Lastly, task-oriented dialogue offers a natural reward ‚Äî whether or not the dialogue succeeded.  And yet, we don‚Äôt see (m)any deployed dialogue systems trained with RL.  Why is that?
 &lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;rl-as-supervised-learning-in-sheeps-clothing&quot;&gt;RL as Supervised Learning in Sheep‚Äôs Clothing&lt;/h2&gt;

&lt;p&gt;In order to apply RL in realistic environments, a number of adjustments are often made to make the training tractable. While classic RL operates in an online setting, training an agent in such a manner quickly becomes impractical (ie. customer service), costly (ie. robotics), or even downright unethical (ie. healthcare).  So as a simplification, trajectories from a human agent are stored in an experience replay buffer for offline training.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; Since the target policy \(\pi_g\) is obviously not the same as the behavioral policy \(\pi_b\) used to collect the data, off-policy evaluation methods are also applied.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Conversations within even a narrow domain are hard to model completely, so now we must adopt model-free RL.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Finally, since the state and action spaces of conversations are unbounded, modern RL systems do away with a table of Q-values, and instead use neural networks as function approximators.  In fact, why even calculate Q-values when directly optimizing for the policy with REINFORCE works just as well.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; At the end of the day, we‚Äôre left with an RL agent trained on mini-batches of examples on a loss function that takes into account neither world models nor discount factors. And since the underlying implementation in both cases is just a Transformer,&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;  this version of RL starts to look practically indistinguishable from regular ol‚Äô supervised learning (SL).&lt;/p&gt;

&lt;p&gt;It turns out, even folks working in reinforcement learning will readily admit that &lt;a href=&quot;https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/&quot;&gt;RL can be simply viewed as SL with a twist&lt;/a&gt;. Specifically, the tweak is that the RL algorithms select good slices of data before applying behavior cloning (aka. supervised learning) to master the task.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;  For example, hindsight relabeling provides better data by changing the agent‚Äôs original goal with the goal that was actually achieved.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; However, this technique of labeling the data after it was collected also has its supervised learning equivalent. In fact, labeling the user intents from a large pool of unannotated conversations is precisely the most common method of providing supervision for the intent detection task.&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; More critically, isn‚Äôt data manipulation just some hack to improve training stability rather than a core part of reinforcement learning methodology?  The algorithmic part of the equation is behavior cloning which seems to be identical to supervised learning. So does RL really boil down to just SL wrapped with some fancy math?&lt;/p&gt;

&lt;h2 id=&quot;exploring-sub-optimal-trajectories&quot;&gt;Exploring Sub-optimal Trajectories&lt;/h2&gt;

&lt;p&gt;The key benefit which allows RL to potentially outshine SL is that it can learn from non-optimal trajectories.&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; In other words, the student can overcome the teacher by avoiding (or even learning from) the mistakes that the teacher has made. Based on my understanding so far, this concept can manifest itself in at least three different ways: maximizing good outcomes, minimizing bad outcomes, and changing bad outcomes into good ones.&lt;/p&gt;

&lt;p&gt;First off, RL learning encourages exploration which allows for finding better reward regions.&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; Since RL operates with sparse rewards, sometimes going to a bad part of the environment is ok as long as the agent eventually learns to return back to the good part. This exploration occasionally pays off when the agent discovers areas of high reward that mindless imitation of past behavior could never visit. Secondly, trajectories that end in a bad outcome are not as damaging because the reward signal will automatically lead to the model to ignore such experiences.&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;  This useful mechanism is not found in supervised learning, as evidenced by the pervasive issue of generating boring, non-committal responses (ie. I don‚Äôt know) when such dialogue is clearly not ideal. The SL agent simply copies what it sees most, whereas the RL agent is able to downplay such utterances. Lastly, an RL agent can take advantage of a imperfect outcomes by stitching together the good parts of bad trajectories. &lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt; It can also take advantage of poor outcomes as examples of what &lt;em&gt;not&lt;/em&gt; to do. Incorporating a penalty into training is trivial by adding a negative reward.  While modifying the loss function within supervised learning is certainly also possible, figuring out the exact formulation is not as straightforward.  Lest we forget, our previously mentioned hindsight replay is another way to turn dirt into gold.&lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;What we saw earlier in the previous section was not a single, isolated case of RL providing better data, but actually a part of larger trend where a RL model achieves superior performance by directly tackling the data problem along with the modeling (policy). How realistic are these advantages though?  While these three cases allow RL to &lt;em&gt;theoretically&lt;/em&gt; outperform SL, does this play out in practice? To start, we‚Äôve already discussed the catastrophic consequences of deploying an RL agent to learn from real users as it explores unsuccessful paths. So while the model won‚Äôt be harmed too much by sub-optimal trajectories, the product surrounding the model will suffer. To get around this, one could consider developing a user simulator for to mimic customer behavior.&lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; Much like the gaming environment of Atari allowed RL to achieve super-human performance on video games, an accurate user simulator should allow an agent to take full advantage of what RL can offer.&lt;/p&gt;

&lt;h2 id=&quot;ideal-data-generators-for-rl&quot;&gt;Ideal Data Generators for RL&lt;/h2&gt;

&lt;p&gt;If the key to unlocking reinforcement learning is to design a robust user simulator, how does one go about doing that exactly?  Well, a reinforcement learning environment is expected to take in the current state along with an agent action to produce the next state along with its associated reward.&lt;sup id=&quot;fnref:11:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;  This implies that a proper user simulator contains two components, a representative model of the user to output next states and an accurate method of evaluation to output well-calibrated rewards.&lt;/p&gt;

&lt;p&gt;Touching upon the user model first, we argue this is essentially intractable since there is no feasible way to predict how a user would react in any given situation.  If we knew what the user wanted, we wouldn‚Äôt need agent interaction in the first place, and could just fulfill the customer request immediately. In the context of games, the goal of an RL agent is obviously to maximize the score. But humans don‚Äôt have a come with a universal scoring function. Even for the same task, different people might want completely different things.  For example, two groups want to book a dinner reservation at the same Italian restaurant, but one group is for a large party with complex dietary restrictions, while the other is a romantic dinner for two.  Frankly, even for the same person, the ideal outcome differs over time.  Consider someone buying a movie ticket for one week, but wanting to watch a different movie the next week.  Building a truly accurate model would entail constructing a new environment for every user. Clearly, we must relax the assumptions about users and instead assume that there exist patterns among different users even if their individual circumstances are slightly unique. We can operationalize the idea that two different users want the same things by offering the agent identical rewards when performing some desired action that equally benefits both users.&lt;/p&gt;

&lt;p&gt;What we‚Äôve learned so far is that reinforcement learning is beneficial only insofar as we can build useful user simulators for producing agent experiences, and that the only aspect of user simulators we can realistically control is their ability to generate appropriate reward signals. However, real-life has no intrinsic rewards, so how do we determine dialogue success?  Open domain chat includes subjective measures such as user satisfaction and dialogue fluency&lt;sup id=&quot;fnref:15&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt;, but defining success in task-oriented dialogue is also imprecise.&lt;sup id=&quot;fnref:16&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt; Suppose the user is booking a flight and the goal is to select the correct flight from a set of options with a KB.  Forget the extra details of dealing with discount codes or fees for checked bags.  Let‚Äôs assume the agent is given the straightforward task of collecting information on a finite set of constraints, such as desired price range, departure and destination locations, and number of seats.  Then suppose the agent successfully books the correct flight that meets all such criteria and deserves a (discounted) reward for all steps taken.  Where does this reward score come from?  We don‚Äôt know a priori what the user wanted, so a human would need go in after the fact to mark the conversation as successful. To the extent that we are just labeling data, it doesn‚Äôt seem like reinforcement learning is any better (or any worse) than training under a supervised learning paradigm.&lt;/p&gt;

&lt;p&gt;Ultimately, RL algorithms are great, but the real limitation is the ability to quickly and scalably label collected conversations.  To take it a step further, one might say the real problem comes down to getting the right data.  Of course, that was always the key to begin with, since good data is the answer to everything on this blog ;)&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lipton et al. (2016) &lt;a href=&quot;https://www.microsoft.com/en-us/research/publication/efficient-exploration-dialogue-policy-learning-bbq-networks-replay-buffer-spiking/&quot;&gt;Efficient Exploration for Dialog Policy Learning with BBQ Networks &amp;amp; Replay Buffer Spiking&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Weisz et al. (2018) &lt;a href=&quot;https://arxiv.org/abs/1802.03753&quot;&gt;Sample Efficient Deep Reinforcement Learning for Dialogue Systems with Large Action Spaces&lt;/a&gt; (ACER)¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jiang et al. (2021) &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.589/&quot;&gt;Towards Automatic Evaluation of Dialog Systems: A Model-Free Off-Policy Evaluation Approach&lt;/a&gt; (ENIGMA)¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Liu et al. (2017) &lt;a href=&quot;https://arxiv.org/abs/1711.10712&quot;&gt;End-to-End Optimization of Task-Oriented Dialogue Model with Deep RL&lt;/a&gt;¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021) &lt;a href=&quot;https://openreview.net/forum?id=a7APmM4B9d&quot;&gt;Decision Transformer: Reinforcement Learning via Sequence Modeling&lt;/a&gt;¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Eysenbach, Kumar, and Gupta (2020) &lt;a href=&quot;https://bair.berkeley.edu/blog/2020/10/13/supervised-rl/&quot;&gt;Reinforcement Learning is Supervised Learning on Optimized Data&lt;/a&gt;¬†&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kumar, Peng, and Levine (2019) &lt;a href=&quot;https://arxiv.org/abs/1912.13465&quot;&gt;Reward-Conditioned Policies&lt;/a&gt;¬†&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Casaneuva et al. (2020) &lt;a href=&quot;https://aclanthology.org/2020.nlp4convai-1.5/&quot;&gt;Efficient Intent Detection with Dual Sentence Encoders&lt;/a&gt;¬†&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Agrawal and Wu (2021) &lt;a href=&quot;https://professional.mit.edu/news/articles/reinforcement-learning-right-your-ai-problem&quot;&gt;Is Reinforcement Learning Right for your AI Problem?&lt;/a&gt;¬†&lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Weng (2020) &lt;a href=&quot;https://lilianweng.github.io/posts/2020-06-07-exploration-drl/&quot;&gt;Exploration Strategies in Deep Reinforcement Learning&lt;/a&gt;¬†&lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sutton and Barto (2014) &lt;a href=&quot;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&quot;&gt;Reinforcement Learning: An Introduction&lt;/a&gt; (2nd Edition)¬†&lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;¬†&lt;a href=&quot;#fnref:11:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Kumar et al. (2022) &lt;a href=&quot;https://openreview.net/forum?id=AP1MKT37rJ&quot;&gt;When Should We Prefer Offline RL Over Behavior Cloning?&lt;/a&gt;¬†&lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ghosh et al. (2019) &lt;a href=&quot;https://arxiv.org/abs/1912.06088&quot;&gt;Learning to Reach Goals via Iterated Supervised Learning&lt;/a&gt;¬†&lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shi et al. (2019) &lt;a href=&quot;https://aclanthology.org/D19-1206/&quot;&gt;How to Build User Simulators to Train RL-based Dialog Systems&lt;/a&gt;¬†&lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Saleh et al. (2019) &lt;a href=&quot;https://arxiv.org/abs/1909.07547&quot;&gt;Hierarchical Reinforcement Learning for Open-Domain Dialog&lt;/a&gt;¬†&lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wang, Peng, and Wong (2020) &lt;a href=&quot;https://aclanthology.org/2020.acl-main.566/&quot;&gt;Learning Efficient Dialogue Policy from Demonstrations through Shaping&lt;/a&gt; (S^2 Agent)¬†&lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="modeling" /><category term="rl" /><category term="dialogue" /><category term="product-strategy" /><summary type="html">On the surface, reinforcement learning (RL) seems like a great method for solving dialogue tasks. We can easily model the problem as a POMDP where the partially observed state represents the user‚Äôs intent. During each turn, the dialogue agent must make a decision about how to respond. This action space is represented as either a series of tokens or simplified even further into a single dialogue act. Lastly, task-oriented dialogue offers a natural reward ‚Äî whether or not the dialogue succeeded. And yet, we don‚Äôt see (m)any deployed dialogue systems trained with RL. Why is that?</summary></entry><entry><title type="html">Prerequisites for a Conversational AI Revolution</title><link href="https://derekchen14.github.io/2022/06/14/conversational-ai-revolution.html" rel="alternate" type="text/html" title="Prerequisites for a Conversational AI Revolution" /><published>2022-06-14T00:00:00-04:00</published><updated>2022-06-14T00:00:00-04:00</updated><id>https://derekchen14.github.io/2022/06/14/conversational-ai-revolution</id><content type="html" xml:base="https://derekchen14.github.io/2022/06/14/conversational-ai-revolution.html">&lt;p&gt;Conversational AI was all the rage a few years back, when people were shouting from the rooftops that chatbots were going to take over the world.  But for all the fanfare and hullabaloo, the trumpeting of a new era has given away lately to a low, dull roar.  Depending on who you ask, we either have AGI right around the corner, or all this noise is simply over-hyped technology soon to float away like vaporware of the past.  I believe the more likely outcome is that the true answer lies somewhere in the middle ‚Äì there will be a revolution, but it won‚Äôt happen overnight.  Instead, changes will start out incremental as the technology is rolled out and users will slowly adopt new social norms around dealing with virtual assistants.  I don‚Äôt claim to know when this will happen or exactly what it will look like, but certainly there are some clues.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;It seems that from the technological perspective, there are a set of seven criteria that will usher in the wave of a Conversational AI revolution.  Is this a hot take?  Perhaps a little, but anyone who disagrees would have a hard time making a convincing case otherwise. Without further ado, the list goes as follows:&lt;/p&gt;

&lt;h3 id=&quot;long-term-context&quot;&gt;Long-term Context&lt;/h3&gt;
&lt;p&gt;Going back ten years, being able to handle context from multiple turns back in the dialogue seemed like an impossibility.  The best dependency parsers would struggle to perform proper co-reference resolution even within the same utterance, much less across utterances.  With the explosion in performance provided by &lt;a href=&quot;https://papers.nips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html&quot;&gt;Transformers&lt;/a&gt; though, this has completely changed.  Models now routinely handle documents with many paragraphs and conversations with many turns.  While this problem is largely solved now, we should recognize that it certainly is a prerequisite for usable and useful conversational AI systems.&lt;/p&gt;

&lt;h3 id=&quot;syntax-and-coherency&quot;&gt;Syntax and Coherency&lt;/h3&gt;
&lt;p&gt;If you take a Transformer and pre-train it with masked language modeling (MLM) or just regular language modeling (LM) you get &lt;a href=&quot;https://aclanthology.org/N19-1423/&quot;&gt;BERT&lt;/a&gt; and &lt;a href=&quot;https://openai.com/blog/better-language-models/&quot;&gt;GPT&lt;/a&gt;, respectively. These models have shown tremendous gains in producing fluent and coherent speech across a wide variety of applications, including dialogue.  More recently, you can combine these encoder and decoder components to get a seq2seq model, such as &lt;a href=&quot;https://arxiv.org/abs/1910.10683&quot;&gt;T5&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot;&gt;BART&lt;/a&gt;, which can theoretically handle all of the above.  Perhaps most critically, scaling these models to 100s of billions of parameters and training them with giant amounts of data leads to super-human performance across a number of NLP tasks. Once again, although the &lt;a href=&quot;https://blog.google/technology/ai/lamda/&quot;&gt;LaMDA&lt;/a&gt;s and &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot;&gt;Chinchilla&lt;/a&gt;s have largely solved the syntax problem, this wasn‚Äôt the case a decade ago, and the progress should be recognized.&lt;/p&gt;

&lt;h3 id=&quot;consistent-persona&quot;&gt;Consistent Persona&lt;/h3&gt;
&lt;p&gt;Moving beyond long-term syntactic control, a proper virtual assistant also requires semantic control.  Not &lt;a href=&quot;https://aclanthology.org/2021.eacl-main.24/&quot;&gt;too long ago&lt;/a&gt;, asking a dialogue agent about its profession (&lt;em&gt;What do you do? I am a 3rd grade teacher.&lt;/em&gt;) would yield one answer at the beginning of a conversation, but would quickly yield a different answer (&lt;em&gt;I commute to the hospital each day as a doctor.&lt;/em&gt;) just a few turns later.  A working system should avoid such contradictions and maintain a consistent personality.  In addition to the power of large PLMs, works around &lt;a href=&quot;https://aclanthology.org/2020.acl-main.428/&quot;&gt;unlikelihood training&lt;/a&gt; though have done a reasonable job at making sure the model remembers what it said about itself earlier in the conversation.&lt;/p&gt;

&lt;h3 id=&quot;memory-extensions&quot;&gt;Memory Extensions&lt;/h3&gt;
&lt;p&gt;What about remembering what the other person said?  While we‚Äôve already discussed the ability of modern models to cover simple details within their latent state, a truly useful system should also be endowed with the ability to track discrete states.  This serves at least two practical purposes. First, the main goal of any task-oriented dialogue system is to extract the proper slot-values from an utterance for policy decision-making.  Predicted slot-values must be exact in order to execute API calls and need to be tracked precisely over time.  Even in the open domain scenario, having a discrete state to represent the user‚Äôs preferences would be immensely valuable. Secondly, having a discrete state allows for the ability to inspect and modify such state for improved model control. If done before model deployment, we can consider initializing the model‚Äôs memory as a way of injecting some commonsense reasoning as a prior.  A &lt;a href=&quot;https://arxiv.org/abs/1410.3916&quot;&gt;long line of work&lt;/a&gt; from FAIR has focused on this area, culminating with the release of &lt;a href=&quot;https://blenderbot.ai&quot;&gt;BlenderBot3&lt;/a&gt; which can not only &lt;a href=&quot;https://arxiv.org/abs/2107.07567&quot;&gt;remember discrete facts&lt;/a&gt; about both speakers, but can augment &lt;a href=&quot;https://arxiv.org/abs/2111.05204&quot;&gt;that knowledge&lt;/a&gt; by &lt;a href=&quot;https://aclanthology.org/2022.acl-long.579/&quot;&gt;retrieving facts from the Internet&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;interactive-learning&quot;&gt;Interactive Learning&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2208.03270&quot;&gt;Learning new skills after deployment&lt;/a&gt; is also an absolute requirement for working conversational agents since no matter how much a model knows or remembers while it is trained, the knowledge will become outdated over time.  Being able to adapt, either through &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot;&gt;human feedback&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/abs/1710.11248&quot;&gt;other reward signals&lt;/a&gt; will be critical to staying relevant over time.  More specifically, the difference between these human-in-the-loop systems is that the former provides an explicit signal through direct interaction, such as clicking a thumbs up / thumbs down button at the end of a chat.  On the other hand, the much more scalable (but also noisy) option is to &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.489/&quot;&gt;follow an implicit signal&lt;/a&gt;, such as measuring user satisfaction (with a separately trained model) or keeping track of user engagement (number of dialogue turns).&lt;/p&gt;

&lt;h3 id=&quot;ambiguity-and-uncertainty&quot;&gt;Ambiguity and Uncertainty&lt;/h3&gt;
&lt;p&gt;By satisfying all the criteria above, we end up with a model that is amazingly powerful not only in the short-term, but arguably gets stronger as time goes on.  With that said, no dialogue system is ever perfect, so one more requirement is the ability to ask for clarification when it does not know. As the famous &lt;a href=&quot;https://en.wikipedia.org/wiki/There_are_known_knowns&quot;&gt;Donald Rumsfeld quote&lt;/a&gt; goes, ‚ÄúThere are known knowns; there are things we know we know. We also know there are known unknowns; that is to say we know there are some things we do not know. But there are also unknown unknowns‚Äîthe ones we don‚Äôt know we don‚Äôt know.‚Äù  Ergo, a working system must learn to how to convert unknown unknowns into known unknowns.&lt;/p&gt;

&lt;p&gt;Recognized unknown or out-of-distribution topics covers multiple fields, including &lt;a href=&quot;https://aclanthology.org/2021.acl-long.84/&quot;&gt;abstention&lt;/a&gt;, &lt;a href=&quot;https://aclanthology.org/2020.findings-emnlp.277/&quot;&gt;uncertainty calibration&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/1811.08581&quot;&gt;open set recognition&lt;/a&gt;, and &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.35/&quot;&gt;out-of-scope detection&lt;/a&gt;.  While tremendous progress has been made on this problem &lt;a href=&quot;https://arxiv.org/abs/2110.11334&quot;&gt;in general&lt;/a&gt;, not much of it has transferred over into the &lt;a href=&quot;https://aclanthology.org/E09-1078/&quot;&gt;dialogue domain&lt;/a&gt; yet. In some sense, this may never be fully solved since, on occasion, conversations are just &lt;a href=&quot;https://aclanthology.org/Q19-1043/&quot;&gt;inherently ambiguous&lt;/a&gt;.  As a result, a dialogue system should not aim towards the impossible task of resolving all ambiguity, but instead should endeavor to learn how to advance a conversation despite the uncertainty.&lt;/p&gt;

&lt;h3 id=&quot;low-data-regime&quot;&gt;Low Data Regime&lt;/h3&gt;
&lt;p&gt;Last but not least, the raison d‚Äô√™tre of the blog ‚Äì dealing with limited and noisy data.  Even when the dialogue system can do everything we imagine, there is no revolution if the cost of training such a model requires an inordinate amount of high-quality data.  Pre-training certainly helps, but even for Large PLMs, there is evidence that &lt;a href=&quot;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications&quot;&gt;data is the bottleneck&lt;/a&gt;.  &lt;a href=&quot;https://arxiv.org/abs/2105.03075&quot;&gt;Data augmentation&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/2105.10311&quot;&gt;synthetic text generation&lt;/a&gt; certainly has a role to play too.  As &lt;a href=&quot;https://morethanoneturn.com/2022/05/24/building-user-simulators.html&quot;&gt;previous articles&lt;/a&gt; have hinted at though, I believe we need to find &lt;a href=&quot;https://morethanoneturn.com/2022/03/11/data-augmentation-for-scalable-product.html&quot;&gt;dialogue-specific methods&lt;/a&gt; of data generation to solve this problem.  Overall, I am quite hopeful that this last issue will also be solved well enough to bring around some real change.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Since all these components are at least somewhat satisfied, does that mean the revolution is right around the corner?  Well, not quite.  While the technology is mostly there, that doesn‚Äôt imply that you can build a business around it overnight.  The ability to execute and adapt to the changing environment will play a big role in determining who ends up pulling this off, and when.  The time seems soon though.&lt;/p&gt;</content><author><name></name></author><category term="ai" /><category term="startups" /><category term="trends" /><category term="lists" /><summary type="html">Conversational AI was all the rage a few years back, when people were shouting from the rooftops that chatbots were going to take over the world. But for all the fanfare and hullabaloo, the trumpeting of a new era has given away lately to a low, dull roar. Depending on who you ask, we either have AGI right around the corner, or all this noise is simply over-hyped technology soon to float away like vaporware of the past. I believe the more likely outcome is that the true answer lies somewhere in the middle ‚Äì there will be a revolution, but it won‚Äôt happen overnight. Instead, changes will start out incremental as the technology is rolled out and users will slowly adopt new social norms around dealing with virtual assistants. I don‚Äôt claim to know when this will happen or exactly what it will look like, but certainly there are some clues.</summary></entry><entry><title type="html">Building User Simulators for Scalable Dialogue Systems</title><link href="https://derekchen14.github.io/2022/05/24/building-user-simulators.html" rel="alternate" type="text/html" title="Building User Simulators for Scalable Dialogue Systems" /><published>2022-05-24T00:00:00-04:00</published><updated>2022-05-24T00:00:00-04:00</updated><id>https://derekchen14.github.io/2022/05/24/building-user-simulators</id><content type="html" xml:base="https://derekchen14.github.io/2022/05/24/building-user-simulators.html">&lt;p&gt;Training dialogue agents for real-life use cases is immensely difficult since manual data annotation quickly hits scaling issues.  One way around this is to build a user simulator which can theoretically then generate tons of examples for the agent to learn from.  However, to build a system representing the user, you would need a model that understands how to react and respond to agents.  But to train such a user model you would then need some dialogue system that acts as an agent.  So we have a chicken-and-egg problem, right?  Well, not quite.  There is at least one key distinction between a user simulator and a dialogue agent.
 &lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;Note that while a dialogue agent takes in a natural language, follows some policy (often to make API calls), and then generates a natural language response, the simulator does not have to complete all those steps.  In particular, the user simulator can avoid the NLU component and go straight into policy management.  Furthermore, the number of user actions is inherently limited by the number of APIs that the product or service can provide.  To be clear, a real life user engages in a countless number of actions, but only a small subset of those actions are expected to be handled by the agent anyway ‚Äî in all other cases, the agent can simply abstain.&lt;/p&gt;

&lt;p&gt;Armed with these two facts, how can we go about designing a practical user simulator? Our first insight tells us that unlike a dialogue agent, what we want to build is a system that can handle reasonably complex actions followed by extremely sophisticated NLG component for producing responses.  Framed in this way, we see that such a system already exists: open-domain chat bots.  In fact, a large part of building task-oriented chatbots is that users often break out of script and engage in chit-chat.  If our user simulator already produced such outputs, in addition to directed information, then our dialogue agent would naturally be more robust to such variations.&lt;/p&gt;

&lt;p&gt;Our second insight is that our user simulator does not need a terribly complex policy manager.  Instead, we can enumerate them with a set of rules to produce all combinations of user intents.  We can then sample from this set to produce plausible scenarios.  To see how this works, let‚Äôs look at an example.  Suppose we are dealing with the a flight domain.  A user may want to book a flight, cancel a flight, check on an existing flight or ask about some FAQs.  In reality, there are maybe a dozen more actions that user would want to take, but not that much more.&lt;/p&gt;

&lt;p&gt;What makes emulating and/or understanding the user difficult is the number of values they provide and the way in which they provide them.  For example, when booking a flight, the user may choose from thousands of (departure/destination) locations and times.  This can lead to millions of values, but there are only four possible slot combinations.  Taking into account edge cases such as price requirements, seating requests and airline preferences, we might expand this into a total of 100 slots for a given domain.  Ultimately though, the number of slots is relatively small compared to the number of values.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;  We separately mentioned that the way the user‚Äôs express a preference have lots of variation: there are a hundred ways to say that you want to ‚ÄúFly from San Diego to Miami on Saturday morning‚Äù.  What all these permutations have in common though is that all the complexity is in how to express the exact request.&lt;/p&gt;

&lt;p&gt;In other words, with a limited number of domains, slots and intents it is not particularly hard to design a policy which can scale to as many combinations as you want since the heavy lifting is actually within the text generation component.  And what existing models are great at generating text given a context?  Open-domain chatbots.  Such models can already directly query the internet and read through millions of Wikipedia pages.  Trying to parse a few dozen parameters before generating a natural language response should be a walk in the park.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Further notice that the slots in a given domain are often re-used across intents which dramatically simplifies the task.¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="modeling" /><category term="rl" /><category term="dialogue" /><summary type="html">Training dialogue agents for real-life use cases is immensely difficult since manual data annotation quickly hits scaling issues. One way around this is to build a user simulator which can theoretically then generate tons of examples for the agent to learn from. However, to build a system representing the user, you would need a model that understands how to react and respond to agents. But to train such a user model you would then need some dialogue system that acts as an agent. So we have a chicken-and-egg problem, right? Well, not quite. There is at least one key distinction between a user simulator and a dialogue agent.</summary></entry><entry><title type="html">Data Augmentation for Scalable Product Development</title><link href="https://derekchen14.github.io/2022/03/11/data-augmentation-for-scalable-product.html" rel="alternate" type="text/html" title="Data Augmentation for Scalable Product Development" /><published>2022-03-11T00:00:00-05:00</published><updated>2022-03-11T00:00:00-05:00</updated><id>https://derekchen14.github.io/2022/03/11/data-augmentation-for-scalable-product</id><content type="html" xml:base="https://derekchen14.github.io/2022/03/11/data-augmentation-for-scalable-product.html">&lt;p&gt;Data augmentation methods are a staple when training computer vision models, with methods like flipping, resizing, cropping and blurring used so ubiquitously that they are a foregone conclusion in most systems.&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; These methods help improve model robustness such that anyway you change the image of a cat, the model still recognizes the item in the picture as a cat.  This is relatively straight forward since all aforementioned techniques keep the main object the same such that a cat remains a cat, and does not somehow magically morph into a dog.  But does this work for NLP as well?&lt;/p&gt;

&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;naive-data-augmentation&quot;&gt;Naive Data Augmentation&lt;/h2&gt;

&lt;p&gt;Text is composed of discrete tokens rather than continous vectors, so what does blurring or flipping even mean in this setting?  It turns out that cropping and rotation can be approximated by first converting a sentence into a dependency parse and then pruning branches or shifting them around.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; If we define blurring as finding nearby pixels for mixing, then we can view swapping tokens within a sentence as an analogous operation.  Swapping is part of Easy Data Augmentation (EDA) which also includes randomly inserting or deleting tokens, as well as synonym replacement.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Auto-augment for images automatically learns an optimal mixture of augmentations&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;, which has inspired similar forms of automatically designing augmentation policies for dialogue.&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;  These all roughly fall under the umbrella of &lt;em&gt;surface form alteration&lt;/em&gt; which changes the actual tokens of text to create new examples.&lt;/p&gt;

&lt;p&gt;Another popular family of augmentation techniques is &lt;em&gt;latent perturbation&lt;/em&gt; where the raw text is first mapped into some hidden state before being transformed back to natural langauge.  Variational Auto-encoders (VAEs) fit nicely into this paradigm since you can perturb the latent state before decoding to theoretically generate an arbitrary number of augmentations.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; Posterier collapse aside, such a generative model can be hard to train since it requires learning the entire distribution of outcomes.  Oftentimes, it is much easier to learn a direct mapping from one piece of raw text to another, where the hidden state is untouched.  This is basically paraphrasing.&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; If we jump to a pivot langauge in the middle of encoding and decoding, this is Round Trip Translation (RTT).&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;  There are countless other techniques in the realm (see &lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt; for one survey), but a last one worth mentioning is Mix-match which interpolates the hidden states of two examples and also the hidden states of their labels to produce new examples.&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt; Temperature sharpening may also be applied.&lt;/p&gt;

&lt;p&gt;While all these methods have seen some success in NLP, naively applying them to intent classification and other dialogue tasks may not be so straightforward because they can easily alter the semantic meaning of an utterance. To see why, take for example the sentence ‚ÄúI would prefer not to eat Indian food, it‚Äôs too spicy‚Äù.  Suppose we were to perform word replacement, and switch Indian to American: ‚ÄúI would prefer not to eat American food, it‚Äôs too spicy‚Äù.  Then the utterance doesn‚Äôt make a lot of sense, and the user‚Äôs intent has also been altered.  Suppose token dropout were used instead: ‚ÄúI would prefer to eat Indian food‚Äù.  Now the user‚Äôs preference has been completely inverted!  Unlike in the image setting, tiny perturbations can cause large semantic shifts in text.&lt;/p&gt;

&lt;h2 id=&quot;label-preserving-data-augmentation&quot;&gt;Label Preserving Data Augmentation&lt;/h2&gt;

&lt;p&gt;To preserve semantics as we generate new examples, the augmentation technique ought to have an awareness of the importance of certain entities or phrases before performing its transformation.  For example, rather than dropping random tokens, this can be accomplished more intelligently by only focusing on stopwords or other low order segments as determined by a POS tagger.  When operating in a latent space, &lt;em&gt;consistency training&lt;/em&gt; induces robustness applying general perturbations around a region of a label, while encouraging the model to still maintain the same prediction.&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt; A typical loss function to enforce matching predictions can be a cross entropy loss.  Others have also tried MSE, KL-divergence, JSD or any other distance metric.&lt;sup id=&quot;fnref:17&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Taking this a step further, rather than operating at the token-level, we can expand this to encompass entire segments where different phrase fragments are shuffled or recombined to form new utterances as part of compositional augmentation.&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;13&lt;/a&gt;&lt;/sup&gt; Typically, the focus is on the unimportant spans in order to avoid accidentally changing the label, but if we have access to an ontology, we can actually target the label by switching known values for slot-filling with other values belonging to that same slot.&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;If the goal is to make sure the label is preserved, we can actually work on the problem in reverse as well!  Concretely, we can consider creating new examples from scratch starting from the labels themselves using &lt;em&gt;text generation&lt;/em&gt; methods.  Language model (LM) decoding pre-trains a language model to auto-regressively predict a text span composed of a label followed by a corresponding utterance. During inference, the model is fed the label alone and asked to produce a novel example.&lt;sup id=&quot;fnref:13&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:13&quot; class=&quot;footnote&quot;&gt;14&lt;/a&gt;&lt;/sup&gt; Alternatively, we can use the label set to write out templates which are then paraphrased by humans into natural language.&lt;sup id=&quot;fnref:14&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:14&quot; class=&quot;footnote&quot;&gt;15&lt;/a&gt;&lt;/sup&gt; This can be applied to the dialogue setting by having entire conversations written out in template format before paraphrasing into natural text.&lt;sup id=&quot;fnref:15&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:15&quot; class=&quot;footnote&quot;&gt;16&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Even though the methods now preserve labels, they still face a host of other issues. To start, there is no single method that is reliably useful.&lt;sup id=&quot;fnref:16&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt;  This lack of consistency holds not only when dealing with a wide range of NLP tasks, but even when optimizing just for specific tasks such as intent classification.&lt;sup id=&quot;fnref:17:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:17&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;  Moreover, most methods have only been studied on classification, rather than on complicated tasks such as dialogue state tracking. Consequently, there is no consensus to how well these augmentation methods work in practice. But what if the goal isn‚Äôt even model robustness on in-domain examples?  What if what we really want is to build scalable dialogue systems that can survive the complexity of real-life conversations?&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation-for-diversity&quot;&gt;Data Augmentation for Diversity&lt;/h2&gt;

&lt;p&gt;Using data augmentation to achieve robustness on out-of-domain (OOD) examples requires a shift in mindset from targeting label quality towards focusing on label diversity.  This mirrors a similar shift in computer vision in optimizing for both Affinity and Diversity.&lt;sup id=&quot;fnref:21&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:21&quot; class=&quot;footnote&quot;&gt;18&lt;/a&gt;&lt;/sup&gt; Template paraphrasing may generate high quality annotations since we know the labels unfront, but the breadth of coverage is limited by the number of manually crafted templates.  LM decoding fine-tuned on your dataset may start to overfit to the distribution it has encountered. In order to produce novel examples then, we must inject some alternate prior into the data distribution.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Auxilliary supervision&lt;/em&gt; techniques obtain data following a different distribution by literally using a separate dataset. Essentially, some small seed set can be used to query a large pool of unlabeled utterances to find examples that are close enough to maintain the label, but far away enough to expand the coverage.&lt;sup id=&quot;fnref:18&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:18&quot; class=&quot;footnote&quot;&gt;19&lt;/a&gt;&lt;/sup&gt;  In order to maintain the label preserving properties, an additional filtering process can be applied.  An alternative to moving towards a different distribution is to add noise to the data manifold, which has also been shown to improve out-of-domain robustness.&lt;sup id=&quot;fnref:19&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:19&quot; class=&quot;footnote&quot;&gt;20&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Text generation based on large-scale pretrained LMs may be especially well-suited for promote diversity since it was (likely) trained on data unrelated to the downstream task, yet can still output coherent examples.  To extract the information stored in its billions of parameters, we can lean on tricks such as beam search, nucleus sampling, or top-k sampling when applied to LM decoding.&lt;sup id=&quot;fnref:10:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;  Text in-filling with a masked language model (MLM) can also be utilized in a similar manner.&lt;sup id=&quot;fnref:16:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:16&quot; class=&quot;footnote&quot;&gt;17&lt;/a&gt;&lt;/sup&gt; And although it hasn‚Äôt happened yet, using prompt-based methods to generate new examples is likely right around the corner.  These method resolve OOD issues to some extent, but are unsatisfying because they fail to cover all possible conversational scenarios. But since the space of possible topics is infinite, does that mean data augmentation is fundamentally flawed?&lt;/p&gt;

&lt;p&gt;To see how this may work, we first have to recognize that the limitation in coverage is not a flaw of data augmentation, but rather a general difficulty of building dialogue systems. So to make things practical, we will not aim to build systems that can discuss any topic.  Instead, we limit the conversations to pertinent aspects helpful for solving specific use cases (ie. buy a chair, find an email, book a flight).  Additionally, we set expectations by letting users know that the chatbot has its limitations.  However, this leads to a new issue since the current techniques expand coverage of the solution space in an arbitrary manner, and while we no longer need to expand to an infinitely large horizon, we still have to expand to new domains in a targeted and controllable manner.&lt;/p&gt;

&lt;h2 id=&quot;augmentation-as-conditional-generation&quot;&gt;Augmentation as Conditional Generation&lt;/h2&gt;

&lt;p&gt;Controllable data augmentation allows for scaling to new domains and use cases in a reliable manner, which is absolutely essential for building products on top of this technology.  To achieve this, we must first clearly define what we mean by use cases.  Specifically, use cases can be considered individual skills (ie. find a restaurant, make a reservation, update a reservation to add more peoeple, cancel a reservation) that all belong to some domain (ie. restaurant reservation management). Suppose we are able to build a dialogue system that can handle three use cases, but we would now like to expand into a fourth.  More than just haphazard data augmentation, we would like to synthesize new training examples in a scalable fashion targeted to help the model learn this fourth skill.&lt;/p&gt;

&lt;p&gt;By conditioning on the new use case we care about when performing generation, we should be able to create new examples for model training that specifically fit our criteria.  Concretely, suppose the fourth skill we want to add is the ability for the dialogue system to converse about restaurant cancellations.  Then, we outline the actions agents must take to handle cancellations.&lt;sup id=&quot;fnref:20&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:20&quot; class=&quot;footnote&quot;&gt;21&lt;/a&gt;&lt;/sup&gt;  The discrete actions can be translated into templates using automated means, which are then paraphrased into natural language.  In contrast to direct paraphrasing, the goal here would be to produce examples in totally new directions rather than re-hashing the existing data.&lt;/p&gt;

&lt;p&gt;Once we have some starting examples, we can then use data augmentation in a traditional manner to produce many more.  In conclusion, in order to make data augmenation useful for scalable product development, we moved away from using it as a means of promoting robustness, and instead as an alternate form of data collection.  The only step left is making this entire process operate efficiently, which is admittedly easier said than done.&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation-taxonomy&quot;&gt;Data Augmentation Taxonomy&lt;/h2&gt;

&lt;p&gt;To recap, we have explored a whirlwind of data augmentation options, starting with ideas inspired from computer vision and gradually moved closer to techniques specialized for scaling development of dialogue systems.  What we discovered along the way is that key application of data augmentation is not necessarily for improving model performance, but rather for dealing with long-tail out-of-domain use cases.  To wrap up, we now organize the smorgasboard of augmentation techniques into a more formal structure for future study.&lt;/p&gt;

&lt;p&gt;One way to organize all the different types of augmentation methods is to think of the end goal in mind ‚Äî- namely the newly formed training example.  A key benefit is this allows us to formulate the solution space with a directed acyclic graph (DAG) which ensures that our taxonomy is collectively exhaustive.  In doing so we end up with the diagram above which outlines five key categories of data augmentation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/data-aug.png&quot; alt=&quot;Data Augmentation Taxonomy&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Surface Form Alteration (A ‚Äì&amp;gt; B): EDA, Character Swapping, Synonym Replacement, Rules-based Systems&lt;/li&gt;
  &lt;li&gt;Latent Perturbation (A ‚Äì&amp;gt; C ‚Äì&amp;gt; B): Pivot languages, VAEs, GANs, Direct paraphrasing&lt;/li&gt;
  &lt;li&gt;Text Generation (C ‚Äì&amp;gt; B): LM Decoding, Text In-filling, User Simulation&lt;/li&gt;
  &lt;li&gt;Auxiliary Supervision (D ‚Äì&amp;gt; B): External data pool, kNN retrieval on unlabeled utterances, Weak supervision&lt;/li&gt;
  &lt;li&gt;Template Paraphrasing (D ‚Äì&amp;gt; C ‚Äì&amp;gt; B): M2M, Overnight, Agenda-based Simulators&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;An interesting observation is that while template paraphrasing seems like a subgroup, according to the DAG it is a parent level method.  Ultimately it seems that while all methods are useful, some are more useful than others.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2020), &lt;a href=&quot;http://proceedings.mlr.press/v119/chen20j/chen20j.pdf&quot;&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Louvan and Magnini, 2020), &lt;a href=&quot;https://aclanthology.org/2020.paclic-1.20/&quot;&gt;Lightweight Data Augmentation for Low Resource Slot Filling and Intent Classification&lt;/a&gt;¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;¬†&lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wei and Zou (2019), &lt;a href=&quot;https://aclanthology.org/D19-1670/&quot;&gt;Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks&lt;/a&gt;¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Cubuk et al. (2020), &lt;a href=&quot;https://openaccess.thecvf.com/content_CVPR_2019/papers/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.pdf&quot;&gt;AutoAugment: Learning Augmentation Policies from Data&lt;/a&gt;¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Niu and Bansal (2019), &lt;a href=&quot;https://aclanthology.org/D19-1132/&quot;&gt;Automatically Learning Data Augmentation Policies for Dialogue Tasks&lt;/a&gt;¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Zhao et al. (2017), &lt;a href=&quot;https://aclanthology.org/P17-1061/&quot;&gt;Learning Discourse-level Diversity for Neural Dialog Models using Conditional VAEs&lt;/a&gt;¬†&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Prakash et al. (2016), &lt;a href=&quot;https://aclanthology.org/C16-1275/&quot;&gt;Neural Paraphrase Generation with Stacked Residual LSTM Networks&lt;/a&gt;¬†&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Sennrich et al. (2016), &lt;a href=&quot;https://aclanthology.org/P16-1009/&quot;&gt;Improving Neural Machine Translation Models with Monolingual Data&lt;/a&gt;¬†&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Feng et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.findings-acl.84/&quot;&gt;A Survey of Data Augmentation Approaches for NLP&lt;/a&gt;¬†&lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;¬†&lt;a href=&quot;#fnref:10:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Berthelot et al. (2020), &lt;a href=&quot;http://papers.neurips.cc/paper/8749-mixmatch-a-holistic-approach-to-semi-supervised-learning.pdf&quot;&gt;MixMatch: A Holistic Approach to Semi-Supervised Learning&lt;/a&gt;¬†&lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Xie et al. (2020), &lt;a href=&quot;https://proceedings.neurips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf&quot;&gt;Unsupervised data augmentation for consistency training&lt;/a&gt;¬†&lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:17&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021), &lt;a href=&quot;https://openreview.net/forum?id=4DXQP8laTU2&quot;&gt;An Empirical Survey of Data Augmentation for Limited Data Learning in NLP&lt;/a&gt;¬†&lt;a href=&quot;#fnref:17&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;¬†&lt;a href=&quot;#fnref:17:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Jacob Andreas (2020), &lt;a href=&quot;https://aclanthology.org/2020.acl-main.676/&quot;&gt;Good-Enough Compositional Data Augmentation&lt;/a&gt;¬†&lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:13&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Anaby-Tavor et al. (2020), &lt;a href=&quot;https://aaai.org/ojs/index.php/AAAI/article/view/6233&quot;&gt;Do not have enough data? Deep learning to the rescue!&lt;/a&gt;¬†&lt;a href=&quot;#fnref:13&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:14&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wang et al. (2015), &lt;a href=&quot;https://aclanthology.org/P15-1129/&quot;&gt;Building a Semantic Parser Overnight&lt;/a&gt;¬†&lt;a href=&quot;#fnref:14&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:15&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shah et al. (2018), &lt;a href=&quot;https://aclanthology.org/N18-3006/&quot;&gt;Bootstrapping a Neural Conversational Agent with Dialogue Self-Play&lt;/a&gt;¬†&lt;a href=&quot;#fnref:15&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:16&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen and Yin (2021), &lt;a href=&quot;https://datacentricai.org/neurips21/papers/138_CameraReady_Data_Aug_v5.pdf&quot;&gt;Data Augmentation for Intent Classification&lt;/a&gt;¬†&lt;a href=&quot;#fnref:16&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;¬†&lt;a href=&quot;#fnref:16:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:21&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lopes et al. (2021), &lt;a href=&quot;https://openreview.net/forum?id=ZcKPWuhG6wy&quot;&gt;Tradeoffs in Data Augmentation: An Empirical Study&lt;/a&gt;¬†&lt;a href=&quot;#fnref:21&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:18&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen and Yu (2021), &lt;a href=&quot;https://aclanthology.org/2021.emnlp-main.35/&quot;&gt;GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation&lt;/a&gt;¬†&lt;a href=&quot;#fnref:18&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:19&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ng et al. (2020), &lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.97/&quot;&gt;SSMBA: Self-Supervised Manifold Based Data Augmentation for Improving OOD Robustness&lt;/a&gt;¬†&lt;a href=&quot;#fnref:19&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:20&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.naacl-main.239/&quot;&gt;ABCD: Action-Based Conversations Dataset&lt;/a&gt;¬†&lt;a href=&quot;#fnref:20&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="data-strategy" /><category term="product-strategy" /><category term="startups" /><summary type="html">Data augmentation methods are a staple when training computer vision models, with methods like flipping, resizing, cropping and blurring used so ubiquitously that they are a foregone conclusion in most systems.1 These methods help improve model robustness such that anyway you change the image of a cat, the model still recognizes the item in the picture as a cat. This is relatively straight forward since all aforementioned techniques keep the main object the same such that a cat remains a cat, and does not somehow magically morph into a dog. But does this work for NLP as well? Chen et al. (2020), A Simple Framework for Contrastive Learning of Visual Representations¬†&amp;#8617;</summary></entry><entry><title type="html">Designing Meaning Representations for Dialogue Systems</title><link href="https://derekchen14.github.io/2021/11/26/designing-meaning-representation.html" rel="alternate" type="text/html" title="Designing Meaning Representations for Dialogue Systems" /><published>2021-11-26T00:00:00-05:00</published><updated>2021-11-26T00:00:00-05:00</updated><id>https://derekchen14.github.io/2021/11/26/designing-meaning-representation</id><content type="html" xml:base="https://derekchen14.github.io/2021/11/26/designing-meaning-representation.html">&lt;p&gt;In order for a virtual assistant to be useful, the agent should do more than just information retrieval and basic chit-chat.  Rather than pattern recognition on the response level, the agent should be able to perform pattern recognition on the discourse level so it can mimic human-reasoning (even as true understanding remains an elusive goal).  If a model were to reason about an utterance, it must have been trained to do so.  Furthermore, we argue that such training must be explicitly performed through (weakly) supervised learning, rather than implicitly extracted from a large pre-trained LM (eg. through careful prompting).
&lt;!--more--&gt;
This article aims to show explicit training is indeed necessary and that such training is possible through well-designed meaning representations of dialogue.&lt;/p&gt;

&lt;h1 id=&quot;necessity-of-fine-tuning&quot;&gt;Necessity of Fine-tuning&lt;/h1&gt;

&lt;p&gt;The explosion of Foundation Models such as BART, GPT, DeBERTa, and others may make it seem like the solutions to all our NLP problems are right around the corner.  However, any serious investigation into the matter will quickly dissuade us of any such illusions.  These gigantic models are hard to train, hard to control and hard to deploy into real-world environments.  Moreover, while large language models generate realistic sounding text, even the creators of such models would likely hesitate to claim that those same models &lt;em&gt;comprehend&lt;/em&gt; the text they are generating.  Indeed, the standard practice is to pre-train a large LM and then fine-tune it on the task of your choosing.&lt;/p&gt;

&lt;p&gt;Given our core task is dialogue systems, how far can we go using only a PLM?&lt;/p&gt;

&lt;h3 id=&quot;lemma-1-dialogue-semantics-is-not-sufficiently-modeled-by-language-modeling&quot;&gt;Lemma 1: Dialogue semantics is not sufficiently modeled by Language modeling.&lt;/h3&gt;
&lt;p&gt;Language models predict next tokens to minimize perplexity.  Dialogue also contains an element of this skill since knowing what the speaker will say next is indicative of understanding, implying you are following along with the conversation.  However, we believe that dialogues are not captured by language modeling in a proof by induction:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Starting with individual tokens, note that specific values are impossible to predict because they vary for each new interaction (ie. aleatoric uncertainty).  If a customer mentions ‚ÄúMy name is X‚Äù or ‚ÄúYou can call me at Y‚Äù that name or phone number is different for each person.  Some names may be more likely than others (ie. John), but for the most part, names are not shared.&lt;/li&gt;
  &lt;li&gt;Sequences of tokens also follow this general pattern.  Certain phrases have higher probability than others, but conversations overall do not follow a predictable flow.  For example, there exist numerous generic phrases such as ‚Äúthere are plenty of fish in the sea‚Äù, ‚Äúdancing in the moonlight‚Äù, or ‚Äúdistance makes the heart grow fonder‚Äù.  Given the first few tokens, it‚Äôs not hard to fill in the rest.  However, dialogues are not always so predictable.  Not only is it possible to tweak these phrases, but folks will occasionally do so precisely for the impact of the unexpected twist: ‚ÄúI would visit, but distance makes the heart grow tired since I don‚Äôt like running.‚Äù
Expanding this from phrases to sentence, from sentences to multi-turn utterances, and so on, we see that semantics are not so easy to capture by language modeling alone.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For further proof, consider the Nucleus Sampling paper (Holtzmann, 2019) which showed that true natural language has much higher perplexity than machine generated output.  We also don‚Äôt want to forget about co-reference, ellipsis and anaphora.  Finally, consider that most common sense reasoning is left unstated.  Overall, if so much of communication is never explicitly stated, then merely being good at predicting what will be said is not enough to understand what is being said.&lt;/p&gt;

&lt;h3 id=&quot;lemma-2-conversational-outcomes-are-subject-to-interpretation&quot;&gt;Lemma 2: Conversational outcomes are subject to interpretation&lt;/h3&gt;
&lt;p&gt;Even when something is explicitly stated, the meaning of such utterances may be ambiguous.  ‚ÄúYea, we‚Äôll get there around 8.‚Äù  Is that 8 AM or 8 PM?  You didn‚Äôt mention if your arrival was later today or tomorrow.  Is 8:30 still ‚Äúaround 8‚Äù or would that be considered too late?  All this makes dialogue complicated.  As another example, consider ‚ÄúThat sounds sooo bad.‚Äù  Does that mean the activity is actually bad, or did the speaker mean that ironically the activity is actually quite good. Is the activity wrong to some folks but debatably acceptable to others?&lt;/p&gt;

&lt;p&gt;Language is too complicated to understand without full context.  And even with that, only training on target labels can help guide a model towards meaningful (ie. practically useful) understanding.  Encoding an utterance into a embedding through a language model or auto-encoder misses out on the critical human-in-the-loop validation. Foundation Models only perform language modeling, and thus are not sufficient for capturing the complexities for dialogue semantics.  In order to capture dialogue, extra supervision is necessary.&lt;/p&gt;

&lt;p&gt;The proof up to this point is not fully satisfying because we have only really shown that pre-training is insufficient with high probability.  Alternate forms of dialogue pre-training are still theoretically possible.  To tackle this in a more complete manner, we also offer some theoretical evidence.  We argue that language exists for communication and such communication is for achieving shared goals.  Since unsupervised pre-training does not have a labeled target, it does not have a clear goal, and therefore can never mimic this aim.  We are confident additional linguistic theory can be added to further support this claim.  In any case, despite the incomplete logic, the overall intuition should be clear: language models alone are not enough to build powerful dialogue models, and explicit training targets are needed.  We hope the reader finds this point convincing enough, so we now move onto discussing how such targets can be formed.&lt;/p&gt;

&lt;h1 id=&quot;meaning-representations-as-supervision&quot;&gt;Meaning Representations as Supervision&lt;/h1&gt;

&lt;p&gt;Since dialogues are so complex, one could convincingly argue that even defining a comprehensive schema is impossible since enumerating all options is beyond the capabilities of any organization.  We actually agree with this claim, and yet we still believe that a useful meaning representation (MR) of dialogue exists and can be obtained in cost effective manner.&lt;/p&gt;

&lt;p&gt;To see how, let us first define what we mean by a ‚Äúmeaning representation‚Äù.  A meaning representation is a way of defining the conversational semantics that can be concretely expressed and therefore can be used as a training signal for a model to learn. Many such expressions already exist, most commonly through the dialogue states as seen in Dialogue State Tracking benchmarks or policy states in dialogue-based reinforcement learning.  They have also been referred to as abstract meaning representations (AMRs) when used in the context of structured prediction.  Traditionally, these representations are also the target programs of semantic parsing.&lt;/p&gt;

&lt;h3 id=&quot;what-are-the-desiderata-for-a-good-mr&quot;&gt;What are the desiderata for a good MR?&lt;/h3&gt;
&lt;p&gt;Given that many meaning representations are possible, what would a ‚Äúgood‚Äù meaning representation look like?  For our purposes, the ideal label set would strike a balance between being:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;easy enough to annotate at scale&lt;/li&gt;
  &lt;li&gt;complex enough to faithfully capture the details of a conversation
Let‚Äôs study these trade-offs in more detail.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What does it mean to be easy to annotate? Scalable annotation means that a ‚Äúcommodity‚Äù annotator can perform the job without excessive training and at reasonable cost. This can be achieved by making the annotation task simple.  For example, you can make the process a matter of verifying a model‚Äôs predictions, rather than requiring the annotators to come up with the labels from scratch.  (How to bootstrap this process from a cold start is the topic for future discussion.)  Separately, we can also limit the scope of annotation to a single domain when starting out. Finally, we can limit the size of the ontology so not every variation is covered, which greatly simplifies the annotation task.  Suffice to say, there are a number of ways to design the annotation task such that annotators can move quickly, yet still produce accurate labels.&lt;/p&gt;

&lt;p&gt;The simplifications introduced in the previous paragraph beg the question: if the ontology is so restricted, how can we hope for a model to learn all the desired details within a dialogue.  Well, what do we want from a complex ontology?  Which details are actually necessary to be captured?  We refer back to Lemma 1, which implies that all we want to capture are specific slot-values (ie. tokens) as well as the semantics of longer sequences (ie. phrases).  Training large PLMs towards these two targets is entirely possible given sufficient training data, which itself is possible since the annotation task has been simplified tremendously.&lt;/p&gt;

&lt;p&gt;Critically, note what we do &lt;em&gt;not&lt;/em&gt; capture within our ontology.  (1) Beyond semantics, we can safely assume that a Foundation model can capture the syntax of dialogue, meaning this is not something we need to annotate.  Along those lines, (2) we assume our problem is scoped such that we do not require capturing pragmatics either.  Finally, while we care about slot-values, (3) we assume that slots themselves are pre-defined by an API, not something a model needs to predict.  Overall, we rely heavily on the capabilities of pre-trained LMs so the amount of explicit supervision required is actually quite minimal.&lt;/p&gt;

&lt;h3 id=&quot;corollary-manfacturing-cars-required-standardization&quot;&gt;Corollary: Manfacturing cars required standardization&lt;/h3&gt;
&lt;p&gt;As a slight tangent, let us draw a parallel from designing MRs to producing cars during the Industrial Revolution.
In order to manufacture a large number of cars, you first had to simplify and standardize the process of building a car.
Similarly, in order to label a large number of conversations, you have to first reduce and standardize the meaning representation of an utterance.  All car parts were joined together within an assembly line.  All conversations are labeled in succession by crowdworkers.  Automation helps turn each individual‚Äôs task into something extremely simple.  You transform each job from ‚Äúcan you build this car part‚Äù into ‚Äúcan you verify this car part was added correctly‚Äù.  Alternatively, you transform each annotation from ‚Äúcan you annotate this example‚Äù into ‚Äúcan you verify this proposed label is correct.‚Äù  In the end, you complete many labels/cars in a quick, yet high quality manner.&lt;/p&gt;

&lt;p&gt;In conclusion, we are able to capture the important details of a conversation by considering the key entities and semantics of a conversation.  We are able to annotate this quickly by concurrently not considering anything else, as well as making the process itself quite efficient.  Finally, we spend time designing the form of the meaning representation to be easy to manage.  This allows us to train powerful dialogue systems at scale.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Authors Note: This post is clearly not a formal proof, just having a bit of fun with the idea :)&lt;/em&gt;&lt;/p&gt;</content><author><name></name></author><category term="data-strategy" /><category term="explainer" /><category term="dialogue" /><summary type="html">In order for a virtual assistant to be useful, the agent should do more than just information retrieval and basic chit-chat. Rather than pattern recognition on the response level, the agent should be able to perform pattern recognition on the discourse level so it can mimic human-reasoning (even as true understanding remains an elusive goal). If a model were to reason about an utterance, it must have been trained to do so. Furthermore, we argue that such training must be explicitly performed through (weakly) supervised learning, rather than implicitly extracted from a large pre-trained LM (eg. through careful prompting).</summary></entry><entry><title type="html">Data Collection Best Practices</title><link href="https://derekchen14.github.io/2021/09/11/data-collection-best-practices.html" rel="alternate" type="text/html" title="Data Collection Best Practices" /><published>2021-09-11T00:00:00-04:00</published><updated>2021-09-11T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/09/11/data-collection-best-practices</id><content type="html" xml:base="https://derekchen14.github.io/2021/09/11/data-collection-best-practices.html">&lt;p&gt;The first step to truly becoming an &lt;a href=&quot;https://morethanoneturn.com/2021/05/23/embracing-ai-first.html&quot;&gt;AI-first&lt;/a&gt; company is to adopt a &lt;a href=&quot;http://datacentricai.org/&quot;&gt;data-centric&lt;/a&gt; view, which naturally implies taking data collection seriously as a core competency of the business.  Even before involving any sophistcated algorithms to improve data quality, there are already many best practices to consider when performing manual data collection.  At a high level, this can be broken down into improvements 
&lt;!--more--&gt;
on the people, the tools and the process itself.  In total, they constitute a basic checklist of items to consider when performing data labeling for NLP tasks.&lt;/p&gt;

&lt;h2 id=&quot;annotator-training&quot;&gt;Annotator Training&lt;/h2&gt;

&lt;h3 id=&quot;recruiting&quot;&gt;Recruiting&lt;/h3&gt;
&lt;p&gt;Gathering quality data begins with reliable annotators who understand the problem you‚Äôre working on.  Your first line of defense are the tools built directly into MTurk including filters and quals.  Filters should be set to 90-95% acceptance rate and specific English-speaking locations.  A pro-tip is to consider places beyond the United States, such as to include Canada, Britain and Singapore.  Of course, if your NLP task requires multi-language or code-switched labels, then you should branch out even further.  Qualifications are mini-exams that you can require annotators pass before starting your HIT.  The benefit of using these quals to both (a) prevent spammers and (b) ensure that the worker understands the task cannot be overstated.&lt;/p&gt;

&lt;p&gt;Of course, you should also pay the crowd-workers a reasonable rate for their work.  It should be commonsense, but there is also a clear correlation between offering higher pay and attracting higher quality workers.  If you happen to be doing repeated work, it might make sense to keep a running list of known experts on your task such that you can invite them back rather than recruiting new folks every time.&lt;/p&gt;

&lt;h3 id=&quot;onboarding&quot;&gt;Onboarding&lt;/h3&gt;
&lt;p&gt;As new workers start your data collection task, they will need a well-written training manual to explain the task and how to go about labeling the text for any given situation.  Natural language utterances are extremely nuanced and thus require clear guidelines for picking out these detailed differences.  Dialogue in particular is heavily context dependent.  As you work more with crowdworkers, you will find almost all of them acting in good faith to provide the best annotations possible.  When something is mislabeled, it is much more likely the cause is due to lackluster guidelines or truly tricky conversations rather than workers behaving with ill-intent.  Thus, putting in the time to really write clear instructions is well worth the effort.&lt;/p&gt;

&lt;p&gt;Tips for writing great guidelines:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;give an explanation of your task in clear, simple writing ‚Äì without the jargon&lt;/li&gt;
  &lt;li&gt;remember that your reader is not an expert in linguistics or NLP&lt;/li&gt;
  &lt;li&gt;explain what is happening from the start, not from what you think they should already know&lt;/li&gt;
  &lt;li&gt;highlight the most important details in &lt;em&gt;italics&lt;/em&gt;, &lt;strong&gt;bold&lt;/strong&gt; or &lt;span style=&quot;color:green;&quot;&gt;color&lt;/span&gt;&lt;/li&gt;
  &lt;li&gt;offer examples of good annotations and examples of common mistakes when annotating&lt;/li&gt;
  &lt;li&gt;repeat yourself on the most critical parts if necessary&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you want to go above and beyond, consider creating tutorial videos and/or offering feedback through email.  When collecting data for ABCD,&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; we found that offering live feedback through a Slack/Discord channel yielded tremendous gains.  Other research has corroborated the findings that expert feedback can be quite helpful.&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;  Intutively, it also makes sense that superficial interventions, such as asking the workers to provide a justification for their labels does not help much.  An interesting trick is to have the highest rated crowdworkers provide feedback to other crowdworkers which can help to naturally scale the process.&lt;/p&gt;

&lt;h3 id=&quot;retention&quot;&gt;Retention&lt;/h3&gt;
&lt;p&gt;For long-running tasks, you can also add in certain aspects to make sure the workers keep on their toes.  In particular, it is common practice to include occassional gold labels that you know are correct.  Then, any workers who mess up too many of these may have their qualifications revoked. Additionally, ideas such as time limits or other thresholds can help to prevent folks getting lazy.  More specifically, suppose you are collecting a dialogue chat, then a token minimum can be automatically checked to make sure the utterances they have generated match some minimum length.  Finally, you should perform occasional spot checks to ensure quality has not dropped.&lt;/p&gt;

&lt;p&gt;Just like running any company though, employee satisfaction becomes paramount.  It makes much more sense to think about how to retain your best workers than to waste extra time worrying about a few bad apples.  One of the unmentioned benefits of feedback and iteration is that it keeps the workers engaged.  Along those lines any sort of reward system, such as bonuses for particularly good labels, warrant a bit of discussion.  Bonuses can be offered per HIT that is done well, or as a one-off system for people who perform well in aggregate.  If there are folks managing and supporting other crowdsource workers then this would certainly warrant some sort of bonus. Be creative in your rewards! Utimately, anything that can help the workers do their job better will end up helping you.&lt;/p&gt;

&lt;h2 id=&quot;annotation-experience&quot;&gt;Annotation Experience&lt;/h2&gt;

&lt;h3 id=&quot;general-principles&quot;&gt;General principles&lt;/h3&gt;
&lt;p&gt;Making the data collection process as realistic as possible will drastically improve the data being collected because workers no longer need to conciously think about all the rules and regulations in the guidelines.  Instead, they can just focus on acting normally and letting their natural tendencies take over, which is what we want to train a model to do anyway.  This insight compelled us to develop Expert Live Chat,&lt;sup id=&quot;fnref:1:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; which differs from typical Wizard-of-Oz data collection by three aspects:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Conversations are conducted continuously in real-time.
    - no set number of turns
    - no templated responses
    - interlocutors can speak for multiple turns in a row&lt;/li&gt;
  &lt;li&gt;Users involved are not interchangeable.
    - there is an explicit agent and customer relationship, which mimics real-life
    - since people have distinct roles, the typical customs of how to behave naturally arise&lt;/li&gt;
  &lt;li&gt;Players are informed that all participants are human.
    ‚Äì there is no wizard behind the scenes
    - encourages people to act like they would with humans rather than with machines
While these aspects make data collection more difficult (methods for resolving this discussed in the paper), they dramatically increase the verisimilitude of the generated conversations.  More generally, even when dealing with non-dialogue related data tasks, any way to make the task itself feel natural should improve results.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;human-computer-interaction&quot;&gt;Human-computer interaction&lt;/h3&gt;
&lt;p&gt;There exists a large body of work in &lt;a href=&quot;https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction&quot;&gt;HCI&lt;/a&gt; and &lt;a href=&quot;https://www.interaction-design.org/literature/article/what-is-design-thinking-and-why-is-it-so-popular&quot;&gt;Design Thinking&lt;/a&gt; around how to make better software, much of which can be carried over to the &lt;a href=&quot;https://aclanthology.org/W12-3613/&quot;&gt;design of annotation tools&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The basic idea behind designing great user experiences is to make the task as simple and intuitive as possible.  This encompasses making the labeling task itself straightforward (more on this in the next section), but also includes keeping the tool simple and easy to use as well.  If there are ways to minimize, or even eliminate, clicking around on a mouse, then you should do so.  For example, rather than having to click [Next Item] or [Submit], allow these to be transformed into keyboard shortcuts. Rather than a long dropdown menu the user must scroll through, allow for fuzzy search on the list of labels.  Even when restricted to just typing, find ways to minimize the number of keystrokes needed.  For example, offer a text autocomplete function for common phrases or inputs.&lt;/p&gt;

&lt;p&gt;Prevent users from making mistakes, but if they do, then allow those users to fix them easily.  We can instantiate these principles through distinct elements in the user interface.  Buttons meant to be clicked should seem clickable (e.g. bolded, shaded), whereas buttons meant to be avoided should not (e.g. grayed out).  If a user clicks on a button that causes irreverisible changes or destructive actions (e.g. delete all annotations), show a warning dialog to check if this action is something they actually wanted to perform.  If a user &lt;em&gt;does&lt;/em&gt; click on something accidentally, provide an option to easily recover from the error; for example, a back button to return to the previous example.&lt;/p&gt;

&lt;h3 id=&quot;system-architecture&quot;&gt;System architecture&lt;/h3&gt;
&lt;p&gt;Real world data collection tasks don‚Äôt just end when the text has been annotated.  We should be desiging for the end-to-end data collection pipeline from ideation to shipping into production.  Is there anything that allows researchers or engineers to quickly spin up new experiments?  Perhaps it is very common for you to perform human evaluation on new iterations of your dialogue model.  Templates can be put into place to easily spin up new evaluation rounds.  This can even be automated such that whenever a new model is fully trained, automatically kick off qualitative evaluation (e.g. coherency, fluency) the same way you might kick off quantitative evaluation (e.g. BLEU, perplexity).&lt;/p&gt;

&lt;p&gt;Automation can work in reverse as well.  Suppose the task is sentiment analysis where given an input utterance, you want to classify as either positive, negative or neutral.  After finishing a round of data collection, the system will automatically pass the labels to an in-house annotation team to QA.  High confidence labels can be directly integrated into data store to immediately start-up a new cycle of model training.&lt;/p&gt;

&lt;h2 id=&quot;annotation-process&quot;&gt;Annotation Process&lt;/h2&gt;

&lt;h3 id=&quot;labeling-platforms&quot;&gt;Labeling platforms&lt;/h3&gt;
&lt;p&gt;When performing data collection, realize that a wide spectrum of services are available to do the actual annotation.  Starting with the broadest audience, serices such as &lt;a href=&quot;https://toloka.yandex.com/&quot;&gt;Toloka&lt;/a&gt; and &lt;a href=&quot;https://www.mturk.com/&quot;&gt;Amazon Mechnical Turk&lt;/a&gt; allow you to tap into a much wider pool for NLP annotation tasks.&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;  Large commerical vendors are also an option, but frankly I would only recommend them when there are strict PII requirements, such as dealing with medical or enterprise data, since otherwise their high price points aren‚Äôt justified.  The next level involves working with contractors such as those found through &lt;a href=&quot;https://www.upwork.com/&quot;&gt;upwork&lt;/a&gt; or &lt;a href=&quot;https://www.fiverr.com/&quot;&gt;fiverr&lt;/a&gt;.  The benefit is the ability to retain knowledge over time, at the cost of a higher upfront investment to vet for quality workers.&lt;/p&gt;

&lt;p&gt;If you have the budget, then moving the entire process in-house would obviously be preferred.  An in-house annotation team can be guided to the exact task you prefer and will always be available.  The internal communication can also allow for side-by-side iteration on the task design and data collection, since the ontology can often shift due to feedback from the raw data.  Finally, if you can afford to hire a handful of PhD linguists or a team of in-house experts to perform annotation, then you are probably a FAANG company.  In all seriousness, expert annotation was the traditional method of obtaining labels, but fails to scale to the size needed to train modern ML models.&lt;/p&gt;

&lt;h3 id=&quot;task-design&quot;&gt;Task Design&lt;/h3&gt;
&lt;p&gt;If you can formulate your problem in a simple manner, then you can obviate the need for subject matter experts and take full advantage of crowd-source workers to scale the effort.  For example, rather than offering a long list of options to choose from for a label, offer just a few multiple choice options.  Even with a limited ontology, annotation can still be difficult when dealing with multi-class and multi-label tasks (ie. many intents can be present in a single dialogue utterance). One ingenious method is to break down the labeling task into a series of simple choices, effectively turning the annotator into a human decision tree where the number of choices for a single HIT is equivalent to the branching factor.&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;  Another technique is to minimize the cognitive load so you can build a semantic parser&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; or dialogue agent overnight.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt; The authors transformed the task by reversing the process.  Rather than providing an utterance and asking for a label, the task starts with a known label and asking the worker to paraphrase a template associated with the known label into a natural language utterance.&lt;/p&gt;

&lt;p&gt;If you can transform the problem into binary selection, perhaps with a contrastive learning algorithm, the task becomes that much easier to label and review.  But even more than the time savings, there is a certain level of simplicity where the task can rely on System I rather than System II processing.  Whereas human eyes and brains are naturally pre-disposed to perform image classification, concious effort must be made to parse speech and perform contextual reasoning.  Labeling then becomes a task the annotator can simply react to rather than think about conciously, which more closely matches how a neural network operates. In other words, effort spent on task simplication may offer exponential gains on model improvement.&lt;/p&gt;

&lt;h3 id=&quot;active-learning&quot;&gt;Active learning&lt;/h3&gt;
&lt;p&gt;A common idea to appears when trying to improve the annotation process is to be more intelligent about how we select what to label.  While active learning does seem to provide noticeable benefits,&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; much of the research suggests that the gains are inconsistent&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt; or limited&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.  More recent work has shown that the cause might be due to the fact that active learning often picks up on outliers which are hard or even impossible to learn from.&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Given the difficulties of getting things to work well in practice, it is my opinion that active learning might not be a great avenue to explore.  While you may see gains of a few weeks in speed-up for example selection, it will cost you a few weeks to set up the process as well.  Assuming there are problems along the way, you might even end up in a net-negative position in terms of annotation speed.  In conclusion, if there are obvious cases where certain examples need extra labels (eg. a brand new class was added to the ontology), then some focused annotation might be worthwhile. But in general, active learning might not be worth the effort.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Effective data collection is a skill and an art.  Depending on the nature of the task, certain methods may help speed up the process, but we want to careful that efficiency gains aren‚Äôt occuring at local maxima.  Ultimately, investments in data collection will pay great dividends and should be taken seriously by any company desiring to become AI-first.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Chen et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.naacl-main.239/&quot;&gt;Action-Based Conversations Dataset&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;¬†&lt;a href=&quot;#fnref:1:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Nangia et al. (2021), &lt;a href=&quot;https://aclanthology.org/2021.acl-long.98/&quot;&gt;Effective Crowdsourcing Protocol for Data Collection&lt;/a&gt;¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Snow et al. (2008), &lt;a href=&quot;https://aclanthology.org/D08-1027/&quot;&gt;Evaluating Non-Expert Annotations for NLP Tasks&lt;/a&gt;¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Dian Yu and Zhou Yu (2020), &lt;a href=&quot;https://aclanthology.org/2021.eacl-main.94/&quot;&gt;MIDAS: A Dialog Act Annotation Scheme&lt;/a&gt;¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Wang et al. (2015), &lt;a href=&quot;https://aclanthology.org/P15-1129/&quot;&gt;Building a Semantic Parser Overnight&lt;/a&gt;¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Shah et al. (2018), &lt;a href=&quot;https://aclanthology.org/N18-3006/&quot;&gt;Building a Conversational Agent Overnight&lt;/a&gt;¬†&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ash et al. (2020), &lt;a href=&quot;https://openreview.net/forum?id=ryghZJBKPS&quot;&gt;BADGE: Deep Batch Active Learning&lt;/a&gt;¬†&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lowell et al. (2019), &lt;a href=&quot;https://aclanthology.org/D19-1003/&quot;&gt;Practical Obstacles to Deploying Active Learning&lt;/a&gt;¬†&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ein-Dor et al. (2021), &lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.638/&quot;&gt;Active Learning for BERT&lt;/a&gt;¬†&lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Karamcheti et al. (2021),  &lt;a href=&quot;https://aclanthology.org/2021.acl-long.564/&quot;&gt;Investigating the Negative Impact of Outliers for VQA&lt;/a&gt;¬†&lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="ai" /><category term="data-strategy" /><category term="explainer" /><category term="lists" /><summary type="html">The first step to truly becoming an AI-first company is to adopt a data-centric view, which naturally implies taking data collection seriously as a core competency of the business. Even before involving any sophistcated algorithms to improve data quality, there are already many best practices to consider when performing manual data collection. At a high level, this can be broken down into improvements</summary></entry><entry><title type="html">Proper Assessment of Data Value</title><link href="https://derekchen14.github.io/2021/07/24/proper-assessment-data.html" rel="alternate" type="text/html" title="Proper Assessment of Data Value" /><published>2021-07-24T00:00:00-04:00</published><updated>2021-07-24T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/07/24/proper-assessment-data</id><content type="html" xml:base="https://derekchen14.github.io/2021/07/24/proper-assessment-data.html">&lt;p&gt;Data is the new oil.  It underpins an undeniable aspect of growth in the popularity and dominance of deep learning.  But is all data created equal? What makes some data points worth more than others?  How could we go about calculating what each data point is worth?  If data is so important, we should certainly want to have a proper way to assess its value.  To do so, we should begin by recognizing that
 &lt;!--more--&gt;
some data is worth more than others.&lt;/p&gt;

&lt;p&gt;For starters, labeled data should be worth more than unlabeled data.  Even with the most advanced pretraining, we would need a thousand to a million times more unlabeled data to rival the performance of valid labeled data for a given task.  As a next step, we might notice that even within annotated data, certain labels are more likely to be noisy as a natural consequence of crowdsourcing large datasets.  Thus, we could conclude that clean data is worth more than noisy data.  Going further, within the subset of correctly labeled data, certain examples will help the model learn more, perhaps because they are data points near the decision boundary or perhaps they are more closely aligned with test distribution.&lt;/p&gt;

&lt;p&gt;With all that said, how could we go about calculating such a thing?  Is there a systematic and principled method for determining the proper value of data?  That‚Äôs what this post will dive into.  Although there are no perfect answer, we chart three major directions worth exploring.  First, we consider data as more valuable when it maximizes information gain.  Next, we measure a datapoint‚Äôs value by observing its impact on the model‚Äôs training dynamics.  Lastly, we consider a training example‚Äôs value as how much improvement it provides over not having that example.&lt;/p&gt;

&lt;h2 id=&quot;1-maximizing-information-gain&quot;&gt;1) Maximizing Information Gain&lt;/h2&gt;

&lt;p&gt;Data that contains more information should be considered more valuable.  We can quantify the level of information within the data as examples which provide more diversity or examples that lowers uncertainty.&lt;/p&gt;

&lt;h3 id=&quot;diversity&quot;&gt;Diversity&lt;/h3&gt;

&lt;p&gt;Intuitively, diversity is helpful because data that adds something different from what I‚Äôve seen before is more informative.  We can measure through various forms of novelty&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or mutual information.  From the novelty perspective, data points that are embedded in a space further away from anything we‚Äôve seen so far is considered diverse.  Variants here include how we embed the data and how to calculate distance.  A notable case, as proposed by BADGE&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, is to use the gradient of the training example as its embedding, which encodes not only diverse but also high magnitude.&lt;/p&gt;

&lt;p&gt;From the information perspective, we can measure the mutual information of the training example compared to a subset of the training data. Mathematically, we measure mutual information as:&lt;/p&gt;

\[MI(X, Y) = D_{KL}  [ P(X,Y) || P(X) \cdot P(Y) ]\]

&lt;p&gt;where X can be our given data so far and Y is our current data point. Diverse data should be less predictive of current training data, and thus have lower mutual information.&lt;/p&gt;

&lt;h3 id=&quot;uncertainty&quot;&gt;Uncertainty&lt;/h3&gt;

&lt;p&gt;We also prefer data that allows us to learn more information regarding areas that I‚Äôm not so sure about. Mathematically, this can be formalized as lowering the entropy of a model‚Äôs outputs.  Alternatively, we can use uncertainty as a tool by measuring a model‚Äôs uncertain about that data point.  Then getting a correct label for that datapoint would be highly informative.  Since a single datapoint is less likely to have a large impact, we can measure the impact of a batch of data.  Then, to get the value of each data point, we average over all batches that the datapoint participated in.  Furthermore, recall that model uncertainty can be measured in multiple ways including variance approximation with dropout&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; or explicit Bayesian networks&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;2-observing-training-dynamics&quot;&gt;2) Observing Training Dynamics&lt;/h2&gt;

&lt;p&gt;Since modern neural networks are over-parameterized, they can completely memorize the training data&lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.  However, those same models typically learn cleanly labeled examples before the noisy examples.&lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.  Therefore, we can determine an example is being more valuable by observing how it fluctuates during training.&lt;/p&gt;

&lt;h3 id=&quot;observing-the-softmax&quot;&gt;Observing the Softmax&lt;/h3&gt;

&lt;p&gt;As suggested within Dataset Cartography&lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;, examples can be divided into those that are easy-to-learn, ambiguous and hard-to-learn.  The ambiguous ones supposedly help the most with learning, as long as some easy-to-learn examples are also included to stabilize training.  These ambiguous examples are those likely to be near the decision boundary, and thus help the model generalize.  Most interesting for our examination are the hard-to-learn examples, which can often be considered mislabeled.&lt;/p&gt;

&lt;p&gt;To find hard-to-learn examples, we track the model‚Äôs confidence across time to calculate a score.  More specifically, we track the output of the model for each training example as measured by:&lt;/p&gt;

\[\mu_i = \frac{1}{E} \sum_{e=1}^E p_{\theta^e} (y_i^{\star} | x_i)\]

&lt;p&gt;where \(\theta^e\) are the model‚Äôs parameters at epoch \(e\). Then, these scores are averaged over all training epochs where the examples that have a lower score are those most likely to be noisy.  We can then discard those examples or relabel them.  The general idea is that when a model is not confident in it‚Äôs output (ie. places low probability mass), this suggests that the label assigned to that example is wrong.&lt;/p&gt;

&lt;h3 id=&quot;observing-logits&quot;&gt;Observing Logits&lt;/h3&gt;

&lt;p&gt;Alternatively, rather than tracking the model‚Äôs final outputs, we can also consider looking at it‚Äôs logits. Specifically, Pleiss et al. suggest looking at the Area Under the Margin (AUM) of an example‚Äôs logit compared to the largest other logit&lt;sup id=&quot;fnref:8&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:8&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;.  In formula form, we measure the score as:&lt;/p&gt;

\[AUM(x,y) = \frac{1}{T} \sum_{t=1}^T z_y^{(t)} (x)  - max_{i \neq y} \left( z_i^{(t)} (x) \right)\]

&lt;p&gt;where \(z_y\) represents the assigned logit and \(z_{i \neq y}\) is the largest other logit.&lt;/p&gt;

&lt;p&gt;To understand what is going on, suppose we are classifying some images as either dogs, cats, horses, frogs, cows, etc.  Additionally, suppose that the given example is a dog, then the assigned logit is also dog.  The model‚Äôs next largest logit is, say, a cat but the the gap between them is large since the model has done a good job differentiating between the two.  This large gap translates to a large AUM score.  Now suppose we are given a fish example, labeled as a dog.  Then the model‚Äôs assigned logit may be quite low, whereas the next largest logit of fish is likely to be quite high since the model (rightly) believes that the image is a fish. Thus, the AUM will be low and once again we have identified a low value data point.&lt;/p&gt;

&lt;h3 id=&quot;observing-hidden-state&quot;&gt;Observing Hidden State&lt;/h3&gt;

&lt;p&gt;As a final direction, we might also consider how the hidden state of the model changes over time as training progresses.  In particular, I would assume that correctly labeled examples need smaller gradients and have more steady hidden states, whereas incorrectly labeled examples will exhibit more fluctuation.  The additional benefit of looking at the hidden state is that we can potentially have multiple output heads, representing multiple tasks.  Since each task would have a different set of logits and softmaxes, we instead look at the latest common hidden state of all tasks to measure the training dynamics.  This is a novel idea that hasn‚Äôt been tested yet, so it could be totally wrong, but I think it‚Äôs worth a shot.&lt;/p&gt;

&lt;h2 id=&quot;3-measuring-marginal-improvement&quot;&gt;3) Measuring Marginal Improvement&lt;/h2&gt;

&lt;h3 id=&quot;naive-difference&quot;&gt;Naive Difference&lt;/h3&gt;

&lt;p&gt;The basic ideas is that if a data point offers high marginal improvement, then it should be considered valuable. So as a baseline, we could measure the accuracy of the model trained on a subset of the data \(p\) and then measure the accuracy again after trained with the extra data point \(q\).  The marginal gain in accuracy \(p - q\) can then be attributed to the data point‚Äôs influence.  This is quite reminiscent of how LIME operates for the purposes of ML explainability&lt;sup id=&quot;fnref:9&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:9&quot; class=&quot;footnote&quot;&gt;9&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;The clear drawback is that this requires re-training the model for every single data point, which quickly becomes intractable.  The other issue is that a single datapoint might not make a much of a difference depending on the data that came before it, so the impact would be negligible.  As a degenerate example, suppose we had a datapoint \(x_1\) that was duplicated in the dataset as \(x_2\).  If the model encountered \(x_1\) first, then it would have some value, but if the model had encountered \(x_2\) in a prior round, then \(x_1\) would be deemed to have no value despite not actually changing.  To mitigate the issues described above, we first describe Shapley Values as a formal method for dealing with the ordering and then an algorithmic update to deal with the tractability.&lt;/p&gt;

&lt;h3 id=&quot;shapley-values&quot;&gt;Shapley Values&lt;/h3&gt;

&lt;p&gt;Originally developed for economic purposes, the Shapely Value (SV) calculates the marginal benefit offered by each person within a coalition&lt;sup id=&quot;fnref:10&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:10&quot; class=&quot;footnote&quot;&gt;10&lt;/a&gt;&lt;/sup&gt;.  People with high values are important, while those with especially low values are not helpful, and perhaps even harmful to the team.  In the case of data collection, each person represents an annotated label.  Accordingly, the SV of each datapoint tells us whether it is clean or noisy. and thus should be relabeled and/or eliminated.&lt;/p&gt;

&lt;p&gt;This is done by calculating the marginal utility of each datapoint, averaged over all permutations of the subsets of collected data.  The permutation of a subset represents shuffling the order of the data when training the neural network. Thus, not only would we need to retrain a network for all subsets of data, we would need to take into account all permutations of drawing the subset.&lt;/p&gt;

\[s_i = \frac{1}{N} \sum_{S \subseteq D \\ z_i} \binom{N-1}{|S|}^{-1} [v(S \cup {z_i}) - v(S)]\]

&lt;p&gt;In the equation above \(s_i\) stands for the Shapley Value of the \(i^{th}\) example out of &lt;em&gt;N&lt;/em&gt; training examples.  \(S \cup {z_i}\) represents the subset with the extra datapoint, whereas \(S\) by itself represents the subset without it.  The function \(v(\cdot)\) is the standalone point value of the data, which can be approximated by the accuracy of the model trained with that data.  Due to the special properties of this value, the Shapley Value actually provides a unique, ideal solution for calculating data values for a given model&lt;sup id=&quot;fnref:11&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:11&quot; class=&quot;footnote&quot;&gt;11&lt;/a&gt;&lt;/sup&gt;.  We still need to deal with the issue of calculating the value in a reasonable time frame.&lt;/p&gt;

&lt;h3 id=&quot;knn-approximation-with-lsh&quot;&gt;KNN approximation with LSH&lt;/h3&gt;

&lt;p&gt;Authors within Jia et al. ingeniously get around the problem of re-training the model for every data by choosing a model that requires no training to operate&lt;sup id=&quot;fnref:12&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:12&quot; class=&quot;footnote&quot;&gt;12&lt;/a&gt;&lt;/sup&gt;.  In particular, the authors cast the model as a KNN rather than a neural network, so training for each new datapoint is trivial (there isn‚Äôt any), with only memory limitations to consider.  First, recall that for each training input, the KNN gets that example correct if the test input matched with the training input has the correct label.  The test label becomes the prediction without any extra effort beyond a lookup call.&lt;/p&gt;

&lt;p&gt;With this in mind, the algorithm proceeds as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For each test example \(x_j\):
    &lt;ul&gt;
      &lt;li&gt;Sort the distance towards all training data to that test example&lt;/li&gt;
      &lt;li&gt;Calculate the value of each test example as the normalized marginal improvement it provides&lt;/li&gt;
      &lt;li&gt;Marginal improvement means:
        &lt;ul&gt;
          &lt;li&gt;Case 1: training example \(x_i\) and \(x_{i+1}\) are both still wrong, then the improvement is zero since 0 - 0 = 0&lt;/li&gt;
          &lt;li&gt;Case 2: training example \(x_i\) is right, while the point further away is wrong, then improvement is 1 - 0 = 1&lt;/li&gt;
          &lt;li&gt;Case 3: training example \(x_i\) is wrong while the point further away \(x_{i+1}\) is actually correct, then the improvement is negative 0 - 1 = -1&lt;/li&gt;
          &lt;li&gt;Case 4: both training examples are correct since we are dealing with nearby datapoints, so improvement is nullified since 1 - 1 = 0&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Normalized means to divide by the number of the remaining train examples being considered to keep things fair&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;For each train example \(x_i\):
    &lt;ul&gt;
      &lt;li&gt;Average over the contributions that \(x_i\) made to the test examples calculated earlier&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The secret is that calculating the value of each test example is much simpler compared to previous methods because it is simply a deciding which of the four cases the training label falls under.  Thus going through the loops is quite fast, where the bottleneck is the sorting of training data.&lt;/p&gt;

&lt;p&gt;As another speed-up, the authors turn to LSH (locality sensitive hashing) to minimize the amount of searching needed. To see how this works, first note that training examples far away from the test example likely contribute no marginal improvement to the \(s_j\).  So we choose some threshold T, where training points further than T are just assumed to have value = 0 and those items are all eliminated from calculation. To be even smarter, an extension could somehow eliminate the super close training examples as well, since they also offer no marginal value.  The authors do not explore this.&lt;/p&gt;

&lt;p&gt;The other part is to obtain the \(M - T\) candidates using truncated LSH, rather than sorting by exact distance.  Concretely, note that the LSH process will (a) embed all the documents into some fixed multi-dimensional space that has some semantic meaning.  Typically, minHash is used, but for this case we use a specially designed hash function that approximates a L2 norm. (b) Suppose each document is now a 80-dim vector.  Break apart all the documents into bands (eg. b = 8, which has 10-dim vector) and compare only the partial vectors within each bucket using a hash function. (c) The partial vectors which are hashed to the same bucket collide because they are supposedly similar.  The corresponding full documents become candidate pairs. (d) Compare the limited number of candidate pairs on an exact match basis.  So for our case, we just switch up Step D into calculating Shapely Values rather than searching for duplicate pairs.  Overall, this means that even with 1 million training examples, we can calculate approximate Shapley Values in under an hour.&lt;/p&gt;

&lt;h2 id=&quot;key-use-cases&quot;&gt;Key Use Cases&lt;/h2&gt;

&lt;p&gt;Since data collected from crowdsourcing is often noisy, redundant labels are collected for each datapoint so that we can determine a final label through a majority vote.  Given these methods of assessing data quality, we can now consider an alternative formulation whereby we do not need spend extra effort upfront for all training examples.  Instead, we can label once, identify low value / mislabled data and then throw that data away or put it up for relabeling.  Since most annotators act in good faith and provide the correct label, this direction should decrease costs by at least half.&lt;/p&gt;

&lt;p&gt;Additionally, some of the methods described above are able to assign value to unlabeled data.  In those cases, we can perform active learning to further maximize our limited resources.  If we go the other way, after all the data is collected, we could also consider paying more to workers who annotated better labels.  More generally, we can pay workers at a rate commensurate with the value of the labels they provide.&lt;/p&gt;

&lt;p&gt;Overall, having a measure of data quality is useful to understand not only the data but ultimately the strength of the model.  Higher quality data naturally leads to better performing models.  This means real-world impact for whatever we designed our models to do.&lt;/p&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://papers.nips.cc/paper/2018/file/b1301141feffabac455e1f90a7de2054-Paper.pdf&quot;&gt;Novelty Seeking Agents&lt;/a&gt; (Conti et al., 2018)¬†&lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1906.03671&quot;&gt;Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds&lt;/a&gt; (Ash et al., 2019)¬†&lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v48/gal16.html&quot;&gt;Dropout as a Bayesian Approximation&lt;/a&gt; (Gal and Ghahramani, 2016)¬†&lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://proceedings.mlr.press/v37/blundell15.html&quot;&gt;Weight Uncertainty in Neural Networks&lt;/a&gt; (Blundell et al., 2015)¬†&lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=Sy8gdB9xx&quot;&gt;Understanding Deep Learning Requires Rethinking Generalization &lt;/a&gt; (Zhang et al., 2017)¬†&lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://dl.acm.org/doi/10.5555/3305381.3305406&quot;&gt;A Closer Look at Memorization in Deep Networks&lt;/a&gt;  (Arpit et al., 2017)¬†&lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://aclanthology.org/2020.emnlp-main.746/&quot;&gt;Dataset Cartography&lt;/a&gt; (Swayamdipta et al., 2020)¬†&lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:8&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/2001.10528&quot;&gt;Identifying Mislabeled Data using AUM Ranking&lt;/a&gt; (Pleiss et al., 2020)¬†&lt;a href=&quot;#fnref:8&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:9&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.04938&quot;&gt;Local Interpretable Model-Agnostic Explanations (LIME)&lt;/a&gt; (Ribeiro et al., 2016)¬†&lt;a href=&quot;#fnref:9&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:10&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://www.rand.org/pubs/research_memoranda/RM0656.html&quot;&gt;Notes on the N-Person Game&lt;/a&gt; (Shapley, 1951)¬†&lt;a href=&quot;#fnref:10&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:11&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://proceedings.mlr.press/v97/ghorbani19c/ghorbani19c.pdf&quot;&gt;Data Shapley: Equitable Valuation of Data&lt;/a&gt; (Ghorbani and Zou, 2019)¬†&lt;a href=&quot;#fnref:11&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:12&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1908.08619&quot;&gt;Efficient Data Valuation for Nearest Neighbor Algorithms&lt;/a&gt; (Jia et al., 2019)¬†&lt;a href=&quot;#fnref:12&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="ai" /><category term="data-strategy" /><category term="explainer" /><summary type="html">Data is the new oil. It underpins an undeniable aspect of growth in the popularity and dominance of deep learning. But is all data created equal? What makes some data points worth more than others? How could we go about calculating what each data point is worth? If data is so important, we should certainly want to have a proper way to assess its value. To do so, we should begin by recognizing that</summary></entry><entry><title type="html">Meta Reinforcement Learning</title><link href="https://derekchen14.github.io/2021/06/01/meta-rl.html" rel="alternate" type="text/html" title="Meta Reinforcement Learning" /><published>2021-06-01T00:00:00-04:00</published><updated>2021-06-01T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/06/01/meta-rl</id><content type="html" xml:base="https://derekchen14.github.io/2021/06/01/meta-rl.html">&lt;p&gt;This post will explore meta reinforcement learning with only minimal math.  We attempt to dive into the core concepts without making it too complicated.&lt;/p&gt;

&lt;p&gt;&lt;!--more--&gt;&lt;/p&gt;

&lt;h2 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;Start by recalling that the RL problem can be defined by the attributes of a Markov Decision Process (MDP) with components of the set of states, action space, transition matrix, reward function and initial starting state:  \(\{ \mathcal{S}, \mathcal{A}, \mathcal{T}, \mathcal{R}, s_0 \}\).  We solve an RL problem by interacting with the environment through some trajectory \(\tau\) to maximize rewards.  More specifically, an agent starts at some initial state \(s_0\), takes actions \(a \in \mathcal{A}\) which will take it to some new state \(s\) and recieve some reward \(r_t\).  By maximizing the sum of discounted rewards, we have optimized the Bellman equation to arrive at a good agent.&lt;/p&gt;

&lt;h2 id=&quot;meta-learning&quot;&gt;Meta-learning&lt;/h2&gt;

&lt;p&gt;Traditional supervised learning aims to find an optimal model that is great at performing some target task during test time.  In contrast, meta-learning tries to develop a model \(\theta^{\star}\) that is not necessarily good to start with, but can become great at the target task using only a few update steps.  In other words, we develop models that have good performance in the few-shot learning setup.  To do so, a model ought to learn how to learn.&lt;/p&gt;

&lt;p&gt;To understand how this works, we take a brief detour into multi-task learning.  Multi-task learning wants a model that can perform well on many tasks at once. A key difficulty is to learn each incremental task without catastrophic forgetting of previously learned tasks.  The meta-RL setup also contains many tasks, which we refer to as support tasks, which can be used to aid the learning of the target task.  While multi-task learning aims to do well at all the task, classic meta-learning only cares about doing well on the one target task.  However, meta-learning can be viewed as being more difficult since it tries to learn the target task with substantially fewer datapoints during test time.&lt;/p&gt;

&lt;p&gt;One way of tackling meta-learning is to use metric based methods, many of which boil down to smarter versions of kNNs.  Suppose we have ten classes of images from CIFAR10. During training, we have to embed all the images into a shared embedding space, but during training we simply match our test image to the nearest one we‚Äôve seen before. Note that two areas to tweak include (1) how we embed these examples and (2) how we calculate the distance between two examples. A Matching Network (&lt;a href=&quot;https://arxiv.org/abs/1606.04080&quot;&gt;Vinyals et al., 2016&lt;/a&gt;) makes this a bit smarter by matching to a weighted combination of the nearest neighbors rather than to just the single nearest image.  Their embedding is a BiLSTM or CNN and the distance is cosine similarity.  A Relation Network (&lt;a href=&quot;https://arxiv.org/abs/1711.06025&quot;&gt;Sung et al., 2018&lt;/a&gt;) learn the a distance metric instead using a CNN that outputs a similarity score.  A Prototypical Network (&lt;a href=&quot;https://arxiv.org/abs/1703.05175&quot;&gt;Snell, Swersky &amp;amp; Zemel, 2017&lt;/a&gt;) change the embeddings during training to be the average of the support sets.  Given nine support classes, a ‚Äúprototype‚Äù vector is calculated for each class which is the weighted average of the embedded examples in that class.  The distance used is a squared Euclidean distance.  We can imagine many other versions based on other embedding functions and learned distance metrics.&lt;/p&gt;

&lt;p&gt;The other way of tackling meta-learning is through gradient based methods, popularized by Model-Agnostic Meta-Learning (MAML) from Finn et al. in 2017.  What we ultimately want is a model parameterized by \(\theta^{\star}\) that can learn with just a few datapoints (let‚Äôs say 20).  So we simply mimic this setup during training.  Start with some initial model \(\theta_t\) and pass in a bunch of training examples from the support set.  Take some gradient update steps to achieve an updated model \(\hat{\theta_t}\), which we refer to as the inner-loop.  Now, pass in 20 training examples for the target set (to match what we expect during test time) through \(\hat{\theta_t}\) to obtain new gradients.  Use these gradients to update \(\theta_t\), which results in \(\theta_{t+1}\).  Next, repeat the process with new training data, reaching \(\theta_{t+2}\).  We call this training the outer loop.  After some &lt;em&gt;n&lt;/em&gt; rounds of outer loop training we have model \(\theta_{t+n}\) which we use as \(\theta^{\star}\) during test time.  Model \(\theta^{\star}\) should work reasonably well since it should have learned basic concepts like edges and shapes that help it learn quickly in new settings.&lt;/p&gt;

&lt;h2 id=&quot;meta-reinforcement-learning&quot;&gt;Meta Reinforcement Learning&lt;/h2&gt;

&lt;p&gt;One way to view meta-learning is that rather than training on a bunch of datapoints, we instead train on a bunch of support tasks.  This gives another distinction with multi-task learning, namely that meta-learning &lt;em&gt;improves&lt;/em&gt; as the number of tasks increase, whereas multi-task performance will typically &lt;em&gt;decrease&lt;/em&gt; as the number of tasks grow.   To transfer into meta-RL, we just recognize that reinforcement learning tasks as defined by their MDPs.  In other words, rather than training on a bunch of support tasks, we train on a bunch of MDPs and test on the target MDP during test time.&lt;/p&gt;

&lt;p&gt;Meta-learning in a supervised setting tries to learn something useful of the problem space shared across different support tasks.  As an example, if our support tasks include question answering about celebrity gossip, news articles and tech blogs, we could hope that the model meta-learns about reading comprehension.  Then during test time, when our target task is question answering about financial updates, the model only needs to learn finance jargon since it already knows how to read.  For reinforcement learning, if our support MDPs include unique reward functions for a humanoid learning to run left, forward and backward, we could hope that the model has learned to balance and move.  Then, during test time, when our target MDP offers rewards for running to the right, the humanoid can learn this quickly.&lt;/p&gt;

&lt;h2 id=&quot;optimization-methods&quot;&gt;Optimization Methods&lt;/h2&gt;

&lt;p&gt;At a high-level, there are at least three methods for optimizing a meta-RL model (&lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa19/static/slides/lec-20.pdf&quot;&gt;Rakelly 2019&lt;/a&gt;).  These forms are all semantically equivalent, but can be seen as different ways of solving the same problem.&lt;/p&gt;

&lt;h4 id=&quot;1-optimization&quot;&gt;1. Optimization&lt;/h4&gt;
&lt;p&gt;We can train our reinforcement learning method using the MAML style gradients as we have already encountered.  If we optimize our model using policy gradients algorithm, then we are pushing up the likelihood of states that lead to higher rewards and pushing down states that lead to lower reward, which is similar to how we react to losses in supervised learning.  So the transition from regular meta-learning to RL-based meta-learning is to learn from reward signals rather than loss signals.  Just like regular meta-learning needed to develop first-order MAML (&lt;a href=&quot;https://arxiv.org/abs/1703.03400&quot;&gt;Finn et al., 2017&lt;/a&gt;) to get around the double-gradient issue, RL-based meta-learning also needs to make some simplifications along the way.  The major benefit of viewing meta-RL as optimization is that it works just like regular deep learning and will converge to the optimal solution given enough data.  We say that this method is ‚Äúconsistent‚Äù.  The downside is that if we face sparse rewards, the model will suffer even worse than in regular RL, which is already not so great.&lt;/p&gt;

&lt;h4 id=&quot;2-recurrence&quot;&gt;2. Recurrence&lt;/h4&gt;
&lt;p&gt;We would like a model to meta-learn something about the environment even if we didn‚Äôt get a reward for that episode, such as how to move around in the simulated world and the fact that the current trajectory was bad (thus leading to no reward).  Once way to keep track of such data is to feed each trajectory through an RNN instead.  Then the hidden states &lt;em&gt;h&lt;/em&gt; of the RNN can keep track of such meta-data.  A single RNN will run multiple episodes to produce multiple inner-gradient updates, which are then used to produce a single outer-gradient updates.  The hidden state of the RNN is reset to its initial \(h_0\) after each outer-loop gradient update, and the process can repeat again.  While recurrence is powerful at capturing more detail in the hidden state, this information is latent and may not end up capturing the details we care about (aka. ‚Äúnot expressive‚Äù).&lt;/p&gt;

&lt;h4 id=&quot;3-contextual&quot;&gt;3. Contextual&lt;/h4&gt;
&lt;p&gt;As an improvement on the recurrence method, we can bias the details we want to capture.  Suppose the model knows the MDP is it operating based on the context (ie. the rewards it has recieved so far). This will help the model produce temporally coherent trajectories when accomplishing the task, as opposed to taking random actions as it starts to explore the state space.  Consequently, the contextual method aims to predict the latent MDP &lt;em&gt;z&lt;/em&gt; before predicting the state, which produces more structured exploration during the inner loop  (&lt;a href=&quot;https://arxiv.org/abs/1802.07245&quot;&gt;Gupta et al., 2018&lt;/a&gt;) .  Concretely, we would aim to learn a model that first learns \(q(z|c)\), where &lt;em&gt;c&lt;/em&gt; is the context and then updates the \(\pi_{\theta}(a|s,z)\).  We learn &lt;em&gt;z&lt;/em&gt; with variational inference where we optimize the ELBO with \(D_{KL} [q_{\phi}(z|c_i) || p(z)]\) where \(p(z)\) is a normal Gaussian.  During inference, the model will choose actions conditioned on its best estimate of the environment parameters based on the context.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Meta-RL is a powerful tool for training reinforcement learning algorithms with limited data.  When applied to dialogue policy management, we can imagine learning a policy that learns to react to its environment with limited experience.  Given that marketing teams will often segment customers into different user types (eg. power users, core users, casual users), we can perhaps train agents that can adapt to different user segments in short time frames.&lt;/p&gt;</content><author><name></name></author><category term="rl" /><category term="explainer" /><summary type="html">This post will explore meta reinforcement learning with only minimal math. We attempt to dive into the core concepts without making it too complicated.</summary></entry><entry><title type="html">Embracing the AI-First Paradigm</title><link href="https://derekchen14.github.io/2021/05/23/embracing-ai-first.html" rel="alternate" type="text/html" title="Embracing the AI-First Paradigm" /><published>2021-05-23T00:00:00-04:00</published><updated>2021-05-23T00:00:00-04:00</updated><id>https://derekchen14.github.io/2021/05/23/embracing-ai-first</id><content type="html" xml:base="https://derekchen14.github.io/2021/05/23/embracing-ai-first.html">&lt;p&gt;When the web first appeared, many folks adopted the new technology by porting over existing physical assets (ie. magazines and newspapers) to appear on a website with almost no changes.  When smartphones began to take hold, those same people ported over their existing websites onto a smaller screen, again with no discernable changes.  Rather than embracing the new medium, people simply applied the content of the old onto the new.  In hindsight, it seems obvious that the builders and makers should have been developing for the new medium, but perhaps paradigm shifts aren‚Äôt so obvious when they are happening.  What lessons does that teach us about the shift into building AI-first companies?&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;To figure out what we can learn from this, we start by really understanding what went wrong.  Then, we can identify opportunities to amend a possibly misguided mindset.  Next, we apply this framework onto exisiting projects which exhibit spurious objectives.  Finally, we conclude by highlighting the takeaway lessons for building an AI-first company.&lt;/p&gt;

&lt;h2 id=&quot;1-what-went-wrong&quot;&gt;1. What Went Wrong?&lt;/h2&gt;

&lt;p&gt;Early webmasters recognized that the Internet was going to be a big thing, but often failed to take advantage of the new medium.  Rather than creating new experiences, they created online versions of offline artifacts.  With today‚Äôs view, we know that rather than static pages of content, a website can be interactive with links and buttons.  We can make spreadsheets, dashboards, and scrolling menus.  Rather than focusing on what the web couldn‚Äôt do, we should have been focusing on what it &lt;em&gt;could&lt;/em&gt; do.&lt;/p&gt;

&lt;p&gt;Mobile developers faced the same hurdles.  Rather than opting for pinch and zoom, most took content on the web and simplified it for smaller screens.  There was a lack of imagination in using the GPS tracking, the real-time updates and built-in camera to build mobile first experiences.  Looking back, we see that the most successful companies of that era (AirBnB, Uber, Pinterest, Instagra, etc.) are precisely those that learned to take advantage of these new features.  It seems then that embracing the new is the clear solution.&lt;/p&gt;

&lt;h2 id=&quot;2-lessons-from-the-past&quot;&gt;2. Lessons from the Past&lt;/h2&gt;

&lt;p&gt;What makes it so hard to see things ‚Äúinevitable‚Äù changes coming?  How could a developer be so right in betting on the web, yet be so mistaken in what they built?  A reasonable solution shouldn‚Äôt just recommend building with the advantages of the new medium, but should also acknowledge why the transition might be turbulent.  For many new technologies, there are people who jump onto the bandwagon simply because it is new and rising.  These people wrote blogs and put up largely static websites because it was the cool, new thing to do.  Does that explain the majority of the population though?  What about the brick and mortar stores moving their retail interfaces online?  What about the educators, evangelists and entertainment moguls building their empires?&lt;/p&gt;

&lt;p&gt;Most people building an online presence regarded the web as simply a new medium rather than a paradigm shift because they weren‚Äôt interested in paradigm shifts.  They were interested in selling whatever it was they already provided and so the web or mobile web was just another way to do what they had already been doing.  Even after it became clear that the new tech needed new ways of thinking, many people simply learned the best practices and left it at that.  These folks are not concerned with ushering in new wave of technology.  And who can blame them, they‚Äôve got their day jobs to worry about!&lt;/p&gt;

&lt;p&gt;The early majority and late majority need to be convinced of the new wave and also taught the appropriate methods of using the new technology.  Google had to build SEO, PPC, and web analytics platforms for people to learn how to build in their garden.  Apple and Android had to offer SDKs and other online training for mobile developers to build in their walled garden.  Both had to showcase examples of good websites and good mobile apps, respectively.&lt;/p&gt;

&lt;p&gt;Additionally, we‚Äôve also noted that the innovators will use new technology for the sake of new tech.  These folks are not so much ahead of the curve as they are just interested in playing with the latest shiny object.  This is why looking at their behaviors will lead to many false prophets (AR/VR, early tablets, autonomous driving, etc.)  The shift must occur then with the early adopters, who must have demanded that the builders create something that takes advantage of the technology or risk becoming another over-hyped flop.&lt;/p&gt;

&lt;h2 id=&quot;3-applications-to-the-present&quot;&gt;3. Applications to the Present&lt;/h2&gt;

&lt;p&gt;Chatbots today are still in the early stages where there are certainly some commercial successes (Alexa, Siri, Google Now), but certainly not the world-changing revolution that was promised around 2016.  The innovators have participated, but it will take a change in how we build these dialogue systems to attract the early adopter crowd.  Luckily, the errors in applying Conversationl AI seem to follow same patterns of omission, so the issue should be straightforward to diagnose.&lt;/p&gt;

&lt;p&gt;The first versions of task-oriented chatbots will likely operate with existing user interaction paradigms until we learn how to take advantage of the unique aspects of chatbot technology.  Consequently, what we see today are agent-assisted systems that help users navigate websites.   We see dialogue systems that take a mobile experience and modify it for chat by changing each mobile API call into a voice-based API call.  Everything is coded up as a bunch of if/then statements because that‚Äôs how we build our applications today.&lt;/p&gt;

&lt;p&gt;It doesn‚Äôt have to be this way of course.&lt;/p&gt;

&lt;p&gt;Intelligent virtual assistants should be a wholly &lt;em&gt;new&lt;/em&gt; method for interacting with technology and information.  We should be able to speak with mostly natural language and get back a reasonable response rather than spitting out directions one line at a time.  We should be able to ask for what we actually want (ie. book a hotel, shop for a pair of dress shoes) rather than using the virtual agent as a voice-activated navigation tool for a mobile app that offers travel reservations or sells shoes.  We should be able to recover from misunderstandings by clarifying what we want in a way that pushes the conversation forward, rather than screaming at a bot that can‚Äôt seem to comprehend that it just made a mistake.  In short, we should be engaging in conversations over commands.&lt;/p&gt;

&lt;h2 id=&quot;4-ai-first-companies&quot;&gt;4. AI-First Companies&lt;/h2&gt;

&lt;p&gt;An AI-first company (and really we‚Äôre referring to converstional AI companies) should build dialogue agents that understand the new medium that they represent.  It should be reliable enough to handle the variations found in real-life conversation, which means the agent should be trained in a way that can adapt to changing circumstances.  Training a flexible agent means being more innovative about where the training data comes from, how the training data is collected and what kind of data is annotated.  How the data aspect should be managed though is a subject for another day ‚Ä¶&lt;/p&gt;</content><author><name></name></author><category term="ai" /><category term="startups" /><category term="trends" /><summary type="html">When the web first appeared, many folks adopted the new technology by porting over existing physical assets (ie. magazines and newspapers) to appear on a website with almost no changes. When smartphones began to take hold, those same people ported over their existing websites onto a smaller screen, again with no discernable changes. Rather than embracing the new medium, people simply applied the content of the old onto the new. In hindsight, it seems obvious that the builders and makers should have been developing for the new medium, but perhaps paradigm shifts aren‚Äôt so obvious when they are happening. What lessons does that teach us about the shift into building AI-first companies?</summary></entry></feed>